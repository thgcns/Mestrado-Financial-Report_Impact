{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "# =====================\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import enum\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import ta\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77ea562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta 'dataset' já existe.\n",
      "Pasta 'dataset/prices' já existe.\n",
      "Pasta 'dataset/fundamental' já existe.\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURAÇÕES GERAIS\n",
    "# =====================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "userName =\"aluno.thiago.nunes\" # os.getenv(\"USERNAME\")\n",
    "password = \"NLPfinance2@23\" #os.getenv(\"PASSWORD\")\n",
    "\n",
    "CD_USERNAME = os.getenv(\"COMD_USER\", \"aluno.thiago.nunes\")\n",
    "CD_PASSWORD = os.getenv(\"COMD_PASS\", \"NLPfinance2@23\")\n",
    "\n",
    "\n",
    "# Proxy de mercado e risk-free\n",
    "#MARKET_PROXY = \"^BVSP\"  # alternativas: \"BOVA11.SA\" (se preferir ETF)\n",
    "#RISK_FREE_SERIES = \"CDI\"      # CDI diário como proxy (pode trocar para SELIC diária se preferir)\n",
    "\n",
    "# Janela de estimação e holding\n",
    "#ESTIMATION_WINDOW = 504  # ~2 anos de pregões\n",
    "#HOLDING_DAYS = 30         # janela pós-earnings (CAR)\n",
    "\n",
    "# Universo de teste (exemplos; substitua pelos seus)\n",
    "#B3_TICKERS = [\n",
    "#    \"ABEV3.SA\", \"ITUB4.SA\", \"PETR4.SA\", \"VALE3.SA\", \"BBDC4.SA\",\n",
    "#]\n",
    "\n",
    "startDate = \"01012010\"\n",
    "endDate   = \"31122019\"\n",
    "\n",
    "\n",
    "# PASTAS\n",
    "# =====================\n",
    "folders = [\"dataset\", \"dataset/prices\", \"dataset/fundamental\"]\n",
    "\n",
    "# Verifica se as pastas existem, se não, cria-as\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Pasta '{folder}' foi criada.\")\n",
    "    else:\n",
    "        print(f\"Pasta '{folder}' já existe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf37593",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = ['IBOV', 'CDI', 'ABEV3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426870c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['IBOV', 'CDI']\n",
    "empresas = [\n",
    "    #indices\n",
    "    \"IBOV\", \"CDI\",\n",
    "    # Empresas com papéis mais líquidos (ON, PN ou Unit)\n",
    "    \"AEDU3\", \"ABEV3\", \"ALLL11\", \"ALLL3\",\n",
    "    \"ALPA4\", \"ALSC3\", \"ALUP11\", \"AMBV3\",\n",
    "    \"AMBV4\", \"AMIL3\", \"ANIM3\", \"ARTR3\",\n",
    "    \"ARZZ3\", \"AZUL4\", \"B3SA3\", \"BBAS3\",\n",
    "    \"BBDC3\", \"BBDC4\", \"BBRK3\", \"BBSE3\",\n",
    "    \"BEEF3\", \"BIDI11\", \"BISA3\", \"BPAC11\",\n",
    "    \"BPNM4\", \"BRAP4\", \"BRDT3\", \"BRFS3\",\n",
    "    \"BRKM5\", \"BRML3\", \"BRPR3\", \"BRSR6\",\n",
    "    \"BRTO4\", \"BTOW3\", \"BVMF3\", \"CCRO3\",\n",
    "    \"CCXC3\", \"CESP6\", \"CIEL3\", \"CMIG3\",\n",
    "    \"CMIG4\", \"CNFB4\", \"CPFE3\", \"CPLE6\",\n",
    "    \"CRFB3\", \"CRUZ3\", \"CSAN3\", \"CSMG3\",\n",
    "    \"CSNA3\", \"CTIP3\", \"CVCB3\", \"CYRE3\",\n",
    "    \"DASA3\", \"DTEX3\", \"ECOD3\", \"ECOR3\",\n",
    "    \"EGIE3\", \"ELET3\", \"ELET6\", \"ELPL4\",\n",
    "    \"ELPL6\", \"EMBR3\", \"ENAT3\", \"ENBR3\",\n",
    "    \"ENEV3\", \"ENGI11\", \"EQTL3\", \"ESTC3\",\n",
    "    \"EVEN3\", \"EZTC3\", \"FFTL4\", \"FIBR3\",\n",
    "    \"FLRY3\", \"GETI4\", \"GFSA3\", \"GGBR3\",\n",
    "    \"GGBR4\", \"GNDI3\", \"GOAU4\", \"GOLL4\",\n",
    "    \"GRND3\", \"HAPV3\", \"HGTX3\", \"HRTP3\",\n",
    "    \"HYPE3\", \"IGTA3\", \"INPR3\", \"IRBR3\",\n",
    "    \"ITSA4\", \"ITUB3\", \"ITUB4\", \"JBSS3\",\n",
    "    \"JHSF3\", \"KEPL3\", \"KLBN11\", \"KLBN4\",\n",
    "    \"KROT3\", \"LAME3\", \"LAME4\", \"LCAM3\",\n",
    "    \"LEVE3\", \"LIGT3\", \"LINX3\", \"LLXL3\",\n",
    "    \"LREN3\", \"LUPA3\", \"MAGG3\", \"MDIA3\",\n",
    "    \"MGLU3\", \"MILS3\", \"MMXM3\", \"MPLU3\",\n",
    "    \"MPXE3\", \"MRFG3\", \"MRVE3\", \"MULT3\",\n",
    "    \"MYPK3\", \"NATU3\", \"NETC4\", \"ODPV3\",\n",
    "    \"OGXP3\", \"OIBR3\", \"OIBR4\", \"OSXB3\",\n",
    "    \"PCAR4\", \"PCAR5\", \"PDGR3\", \"PETR3\",\n",
    "    \"PETR4\", \"PLAS3\", \"PMAM3\", \"POMO4\",\n",
    "    \"POSI3\", \"PRML3\", \"PSSA3\", \"QGEP3\",\n",
    "    \"QUAL3\", \"RADL3\", \"RAIL3\", \"RAPT4\",\n",
    "    \"RDCD3\", \"RENT3\", \"RLOG3\", \"RPMG3\",\n",
    "    \"RSID3\", \"RUMO3\", \"SANB11\", \"SAPR11\",\n",
    "    \"SAPR4\", \"SBSP3\", \"SEER3\", \"SLCE3\",\n",
    "    \"SMLE3\", \"SMLS3\", \"SMTO3\", \"SULA11\",\n",
    "    \"SUZB3\", \"SUZB5\", \"TAEE11\", \"TAMM4\",\n",
    "    \"TBLE3\", \"TCSA3\", \"TCSL3\", \"TCSL4\",\n",
    "    \"TEND3\", \"TERI3\", \"TIET11\", \"TIMP3\",\n",
    "    \"TLPP4\", \"TMAR5\", \"TNLP3\", \"TNLP4\",\n",
    "    \"TOTS3\", \"TRPL4\", \"TUPY3\", \"UGPA3\",\n",
    "    \"UGPA4\", \"USIM3\", \"USIM5\", \"VAGR3\",\n",
    "    \"VALE3\", \"VALE5\", \"VIVO4\", \"VIVT4\",\n",
    "    \"VLID3\", \"VVAR11\", \"VVAR3\", \"WEGE3\",\n",
    "    \"WIZS3\", \"YDUQ3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1c9e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES CAPTURA VALORES\n",
    "def GetHistoricalPriceComdinheiro(ticker, startDate, endDate, userName, password, path2save):\n",
    "    url = \"https://www.comdinheiro.com.br/Clientes/API/EndPoint001.php\"\n",
    "    querystring = {\"code\": \"import_data\"}\n",
    "\n",
    "    internal_url = f\"HistoricoCotacaoAcao001-{ticker}-{startDate}-{endDate}-1-1\"\n",
    "    payload = f\"username={userName}&password={password}&URL={internal_url}&format=json3\"\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "\n",
    "    response = requests.post(url, data=payload, headers=headers, params=querystring)\n",
    "    data = response.json()\n",
    "\n",
    "    # --- Validação: se não houver dados ---\n",
    "    \n",
    "    if not isinstance(data, dict) or \"tables\" not in data or \"tab0\" not in data[\"tables\"]:\n",
    "        print(f\"⚠️ Sem dados disponíveis para {ticker} entre {startDate} e {endDate}\")\n",
    "        return pd.DataFrame()    \n",
    " \n",
    "    df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "    df = df.drop(\"lin0\", errors='ignore')\n",
    "\n",
    "    df.columns = [\n",
    "        \"Data\", \"FechAjust\", \"Var\", \"FechHist\", \"AbertAjust\",\n",
    "        \"MinAjust\", \"MedAjust\", \"MaxAjust\", \"Vol\", \"Neg\", \"Fator\", \"Tipo\", \"COL_A\", \"COL_B\"\n",
    "    ]\n",
    "\n",
    "    # 1) Converte Data → datetime (aceita 'DD/MM/AAAA' e 'AAAA-MM-DD')\n",
    "    #    dayfirst=True garante que '10/11/2012' seja 10 de novembro.\n",
    "    #df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\", dayfirst=True)\n",
    "    df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\", format='%d/%m/%Y', dayfirst=True)\n",
    "    df[\"Data\"] = df[\"Data\"].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Substitui 'nd' por NaN\n",
    "    df.replace(\"nd\", pd.NA, inplace=True)\n",
    "\n",
    "    # Converte colunas numéricas\n",
    "    colunas_numericas = [\n",
    "        \"FechAjust\", \"Var\", \"FechHist\", \"AbertAjust\",\n",
    "        \"MinAjust\", \"MedAjust\", \"MaxAjust\", \"Vol\", \"Neg\", \"Fator\"\n",
    "    ]\n",
    "\n",
    "    for col in colunas_numericas:\n",
    "        df[col] = df[col].str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df = df.sort_values(\"Data\").reset_index(drop=True)\n",
    "    df = df.drop(columns=[ \"COL_A\", \"COL_B\"])\n",
    "    \n",
    "    if path2save != '':\n",
    "     df.to_csv(path2save, index=False, date_format=\"%d/%m/%Y\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Captura de Eventos + Indicadores Fundamentalistas\n",
    "def EventsDate(ticker, userName, password, startDate, endDate, path2save):\n",
    "    url = \"https://www.comdinheiro.com.br/Clientes/API/EndPoint001.php\"\n",
    "    querystring = {\"code\": \"import_data\"}\n",
    "    \n",
    "    # Payload atualizado com todos os indicadores\n",
    "    payload = (\n",
    "        f\"username={userName}&password={password}\"\n",
    "        f\"&URL=HistoricoIndicadoresFundamentalistas001.php%3F\"\n",
    "        f\"%26data_ini%3D{startDate}\"\n",
    "        f\"%26data_fim%3D{endDate}\"\n",
    "        f\"%26trailing%3D12\"\n",
    "        f\"%26conv%3DMIXED\"\n",
    "        f\"%26moeda%3DMOEDA_ORIGINAL\"\n",
    "        f\"%26c_c%3Dconsolidado\"\n",
    "        f\"%26m_m%3D1000000\"\n",
    "        f\"%26n_c%3D2\"\n",
    "        f\"%26f_v%3D1\"\n",
    "        f\"%26papel%3D{ticker}\"\n",
    "        f\"%26indic%3DNOME_EMPRESA%2BRL%2BLL%2BEBITDA%2BDATA_PUBLICACAO\"\n",
    "        f\"%2BPRECO_ABERTURA%2BPRECO_FECHAMENTO%2BLPA%2BROA%2BROE%2BMEB\"\n",
    "        f\"%2BRL%2BCRESC_RL_12M%2BCRESC_LL_12M%2BCRESC_EBITDA_12M%2BCAPEX\"\n",
    "        f\"%2BRL%2BFCO%2BEBITDA%2BFCF%2BDIVIDA_LIQUIDA%2BPL%2BDIVIDA_BRUTA\"\n",
    "        f\"%2BAT%2BDVA_DESPESAS_FIN%2BPC%2BPNC%2BOUTROS_PC%2BLUB\"\n",
    "        f\"%26periodicidade%3Dtri\"\n",
    "        f\"%26graf_tab%3Dtabela\"\n",
    "        f\"%26desloc_data_analise%3D1\"\n",
    "        f\"%26flag_transpor%3D0\"\n",
    "        f\"%26c_d%3Dd\"\n",
    "        f\"%26enviar_email%3D0\"\n",
    "        f\"%26enviar_email_log%3D0\"\n",
    "        f\"%26cabecalho_excel%3Dmodo1\"\n",
    "        f\"%26relat_alias_automatico%3Dcmd_alias_01\"\n",
    "        \"&format=json3\"\n",
    "    )\n",
    "    \n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    response = requests.post(url, data=payload, headers=headers, params=querystring)\n",
    "    data = json.loads(response.text)\n",
    "    \n",
    "    # --- Validação: se não houver dados ---\n",
    "    if not isinstance(data, dict) or \"tables\" not in data or \"tab0\" not in data[\"tables\"]:\n",
    "        print(f\"⚠️ Sem dados disponíveis para {ticker} entre {startDate} e {endDate}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Constrói DataFrame\n",
    "    df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "    \n",
    "    # Colunas de acordo com a ordem dos indicadores\n",
    "    novas_colunas = [\n",
    "        \"Data\", \"Empresa\", \"RL\", \"LL\", \"EBITDA\", \"Data_Publicacao\",\n",
    "        \"Preco_Abertura\", \"Preco_Fechamento\", \"LPA\", \"ROA\", \"ROE\", \"MEB\",\n",
    "        \"RL_dup1\", \"CRESC_RL_12M\", \"CRESC_LL_12M\", \"CRESC_EBITDA_12M\",\n",
    "        \"CAPEX\", \"RL_dup2\", \"FCO\", \"EBITDA_dup\", \"FCF\",\n",
    "        \"Divida_Liquida\", \"PL\", \"Divida_Bruta\", \"AT\", \"DVA_Despesas_Fin\",\n",
    "        \"PC\", \"PNC\", \"Outros_PC\", \"LUB\",\n",
    "        \"Consolidado\", \"Convencao\", \"Moeda\", \"Data_Demonstracao\",\n",
    "        \"Meses\", \"Data_Analise\"\n",
    "    ]\n",
    "    df.columns = novas_colunas\n",
    "    \n",
    "    # Remove linha de cabeçalho interno\n",
    "    df = df.drop(\"lin0\", errors=\"ignore\")\n",
    "    \n",
    "    # Normaliza datas\n",
    "    df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\", format='%d/%m/%Y', dayfirst=True)\n",
    "    df[\"Data_Publicacao\"] = pd.to_datetime(df[\"Data_Publicacao\"], errors=\"coerce\", format='%d/%m/%Y', dayfirst=True)\n",
    "    df[\"Data_Demonstracao\"] = pd.to_datetime(df[\"Data_Demonstracao\"], errors=\"coerce\", format='%d/%m/%Y', dayfirst=True)\n",
    "    df[\"Data_Analise\"] = pd.to_datetime(df[\"Data_Analise\"], errors=\"coerce\", format='%d/%m/%Y', dayfirst=True)\n",
    "    #df['Data_Publicacao'] = pd.to_datetime(df['Data_Publicacao'], errors='coerce', format='%d/%m/%Y')\n",
    "    #df['Data'] = pd.to_datetime(df['Data'], errors='coerce', format='%d/%m/%Y')\n",
    "    #df['Data_Demonstracao'] = pd.to_datetime(df['Data_Demonstracao'], errors='coerce', format='%d/%m/%Y')    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Consolida duplicados (mantém o primeiro válido)\n",
    "    if {\"RL\", \"RL_dup1\", \"RL_dup2\"}.issubset(df.columns):\n",
    "        df[\"RL\"] = df[[\"RL\", \"RL_dup1\", \"RL_dup2\"]].bfill(axis=1).iloc[:, 0]\n",
    "        df = df.drop(columns=[\"RL_dup1\", \"RL_dup2\"])\n",
    "    if {\"EBITDA\", \"EBITDA_dup\"}.issubset(df.columns):\n",
    "        df[\"EBITDA\"] = df[[\"EBITDA\", \"EBITDA_dup\"]].bfill(axis=1).iloc[:, 0]\n",
    "        df = df.drop(columns=[\"EBITDA_dup\"])\n",
    "    \n",
    "    if path2save != '':\n",
    "        df.to_csv(path2save, index=False)        \n",
    "    \n",
    "    return df\n",
    "\n",
    "##### COLETA DE TICKERS DO IBOV\n",
    "\n",
    "def GetIbovComposition(userName, password, startYear=2010, endYear=2019):\n",
    "    url = \"https://api.comdinheiro.com.br/v1/ep1/import-data\"\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    \n",
    "    all_dfs = []\n",
    "    \n",
    "    for year in range(startYear, endYear + 1):\n",
    "        data_analise = f\"2209{year}\"  # 31/12/AAAA\n",
    "        \n",
    "        inner_url = (\n",
    "            f\"ComposicaoIndices001.php?\"\n",
    "            f\"data_analise={data_analise}\"\n",
    "            f\"&indice=IBOV\"\n",
    "            f\"&nome_portfolio=\"\n",
    "            f\"&tipo_portfolio=\"\n",
    "            f\"&overwrite=0\"\n",
    "            f\"&design=2\"\n",
    "            f\"&obs_portfolio=0\"\n",
    "            f\"&num_casas=0\"\n",
    "            f\"&salvar_dados=nenhum\"\n",
    "            f\"&sufixo=\"\n",
    "            f\"&nome_serie=\"\n",
    "            f\"&filtro_avancado=\"\n",
    "        )\n",
    "        \n",
    "        payload = f\"username={userName}&password={password}&URL={inner_url}&format=json3\"\n",
    "        \n",
    "        response = requests.post(url, data=payload, headers=headers)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not isinstance(data, dict) or \"tables\" not in data or \"tab0\" not in data[\"tables\"]:\n",
    "            print(f\"⚠️ Sem dados para {data_analise}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "        df = df.drop(\"lin0\", errors=\"ignore\").reset_index(drop=True)\n",
    "        #df[\"Data_Analise\"] = pd.to_datetime(data_analise, format=\"%d%m%Y\")\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Exemplo de chamada\n",
    "#df_ibov = GetIbovComposition(\n",
    "#    userName=userName,\n",
    "#    password=password,\n",
    "#    startYear=2010,\n",
    "#    endYear=2019\n",
    "#)\n",
    "#\n",
    "#print(df_ibov.head())\n",
    "#df_ibov.to_csv(\"dataset/IBOV_Composicao_2010_2019.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f4eb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNNLOAD PREÇOS\n",
    "for emp in empresas:\n",
    "  GetHistoricalPriceComdinheiro(ticker=emp, startDate=startDate, endDate=endDate, userName=userName, password=password, path2save=f\"dataset/prices/{emp}.SA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83249b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD FUNDAMENTALISTAS\n",
    "files = os.listdir('dataset/prices')\n",
    "for emp in files:  \n",
    "  EventsDate(ticker= emp[0:-7], userName=userName, password=password, startDate=startDate, endDate=endDate, path2save=f\"dataset/fundamental/{emp}\")\n",
    "  os.listdir('dataset/fundamental')\n",
    "#  price_p.insert(1, 'event', price_p['Data'].apply(lambda date: 1 if date in date_df['Data_Publicacao'].values else 0))\n",
    "#  price_p.to_csv(f\"dataset/prices_processed/{emp}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-ml)",
   "language": "python",
   "name": "fin-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
