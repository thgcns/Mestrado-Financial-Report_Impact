{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "# =====================\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setores\n",
    "\n",
    "CVM_SECTORS = OrderedDict([\n",
    "    (\"Bens Industriais\", 1),\n",
    "    (\"Consumo C√≠clico\", 2),\n",
    "    (\"Consumo N√£o C√≠clico\", 3),\n",
    "    (\"Financeiro e Outros\", 4),\n",
    "    (\"Materiais B√°sicos\", 5),\n",
    "    (\"Petr√≥leo, G√°s e Biocombust√≠veis\", 6),\n",
    "    (\"Sa√∫de\", 7),\n",
    "    (\"Tecnologia da Informa√ß√£o\", 8),\n",
    "    (\"Telecomunica√ß√µes\", 9),\n",
    "    (\"Utilidade P√∫blica\", 10),\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Mapeamento Ticker -> Setor (CVM, janela 2010-2019)\n",
    "#    Principais ajustes hist√≥ricos inclu√≠dos\n",
    "# -------------------------------\n",
    "TICKER_TO_SECTOR = {\n",
    "    # Financeiro e Outros\n",
    "    \"BBAS3\":\"Financeiro e Outros\",\"BBDC3\":\"Financeiro e Outros\",\"BBDC4\":\"Financeiro e Outros\",\n",
    "    \"BRSR6\":\"Financeiro e Outros\",\"SANB11\":\"Financeiro e Outros\",\"ITUB3\":\"Financeiro e Outros\",\n",
    "    \"ITUB4\":\"Financeiro e Outros\",\"ITSA4\":\"Financeiro e Outros\",\"BPAC11\":\"Financeiro e Outros\",\n",
    "    \"BIDI11\":\"Financeiro e Outros\",\"IRBR3\":\"Financeiro e Outros\",\"BBSE3\":\"Financeiro e Outros\",\n",
    "    \"PSSA3\":\"Financeiro e Outros\",\"WIZS3\":\"Financeiro e Outros\",\"BPNM4\":\"Financeiro e Outros\",\n",
    "    \"B3SA3\":\"Financeiro e Outros\",\"BVMF3\":\"Financeiro e Outros\",\n",
    "    \"BRPR3\":\"Financeiro e Outros\",\"IGTA3\":\"Financeiro e Outros\",\"BRML3\":\"Financeiro e Outros\",\n",
    "    \"MULT3\":\"Financeiro e Outros\",\"ALSC3\":\"Financeiro e Outros\",\"JHSF3\":\"Financeiro e Outros\",\n",
    "    \"CTIP3\":\"Financeiro e Outros\",\"SULA11\":\"Financeiro e Outros\",\"CIEL3\":\"Financeiro e Outros\",\n",
    "    \"BRAP4\":\"Financeiro e Outros\",\n",
    "\n",
    "    # Bens Industriais (inclui transporte/concess√µes/a√©reas/log√≠stica e bens de capital)\n",
    "    \"WEGE3\":\"Bens Industriais\",\"RAPT4\":\"Bens Industriais\",\"KEPL3\":\"Bens Industriais\",\n",
    "    \"TUPY3\":\"Bens Industriais\",\"MYPK3\":\"Bens Industriais\",\"LEVE3\":\"Bens Industriais\",\n",
    "    \"PLAS3\":\"Bens Industriais\",\"INPR3\":\"Bens Industriais\",\"MILS3\":\"Bens Industriais\",\n",
    "    \"MAGG3\":\"Bens Industriais\",\"EMBR3\":\"Bens Industriais\",\n",
    "    \"CCRO3\":\"Bens Industriais\",\"ECOR3\":\"Bens Industriais\",\"ARTR3\":\"Bens Industriais\",\n",
    "    \"RAIL3\":\"Bens Industriais\",\"RUMO3\":\"Bens Industriais\",\"ALLL11\":\"Bens Industriais\",\"ALLL3\":\"Bens Industriais\",\n",
    "    \"RLOG3\":\"Bens Industriais\",\"GOLL4\":\"Bens Industriais\",\"AZUL4\":\"Bens Industriais\",\n",
    "    \"LLXL3\":\"Bens Industriais\",\"PRML3\":\"Bens Industriais\",\"POMO4\":\"Bens Industriais\",\"ECOD3\":\"Bens Industriais\",\n",
    "    \"TAMM4\":\"Bens Industriais\",\n",
    "\n",
    "    # Consumo C√≠clico (varejo, moda, loca√ß√£o, educa√ß√£o, viagens/lazer, intermedia√ß√£o imobili√°ria)\n",
    "    \"LREN3\":\"Consumo C√≠clico\",\"MGLU3\":\"Consumo C√≠clico\",\"VVAR11\":\"Consumo C√≠clico\",\"VVAR3\":\"Consumo C√≠clico\",\n",
    "    \"BTOW3\":\"Consumo C√≠clico\",\"LAME3\":\"Consumo C√≠clico\",\"LAME4\":\"Consumo C√≠clico\",\n",
    "    \"ARZZ3\":\"Consumo C√≠clico\",\"HGTX3\":\"Consumo C√≠clico\",\"GRND3\":\"Consumo C√≠clico\",\n",
    "    \"RENT3\":\"Consumo C√≠clico\",\"LCAM3\":\"Consumo C√≠clico\",\"CVCB3\":\"Consumo C√≠clico\",\n",
    "    \"MPLU3\":\"Consumo C√≠clico\",\"SMLS3\":\"Consumo C√≠clico\",\n",
    "    \"AEDU3\":\"Consumo C√≠clico\",\"ANIM3\":\"Consumo C√≠clico\",\"ESTC3\":\"Consumo C√≠clico\",\n",
    "    \"KROT3\":\"Consumo C√≠clico\",\"SEER3\":\"Consumo C√≠clico\",\"YDUQ3\":\"Consumo C√≠clico\",\n",
    "    \"ALPA4\":\"Consumo C√≠clico\",\"BBRK3\":\"Consumo C√≠clico\", \"BISA3\":\"Consumo C√≠clico\", \n",
    "    \"CYRE3\":\"Consumo C√≠clico\", \"EVEN3\":\"Consumo C√≠clico\", \"EZTC3\":\"Consumo C√≠clico\", \n",
    "    \"GFSA3\":\"Consumo C√≠clico\", \"MRVE3\":\"Consumo C√≠clico\", \"PDGR3\":\"Consumo C√≠clico\", \n",
    "    \"RSID3\":\"Consumo C√≠clico\", \"TCSA3\":\"Consumo C√≠clico\", \"TEND3\":\"Consumo C√≠clico\",\n",
    "\n",
    "    # Consumo N√£o C√≠clico (alimentos/bebidas, higiene/med., agro, varejo alimentar)\n",
    "    \"ABEV3\":\"Consumo N√£o C√≠clico\",\"AMBV3\":\"Consumo N√£o C√≠clico\",\"AMBV4\":\"Consumo N√£o C√≠clico\",\n",
    "    \"CRFB3\":\"Consumo N√£o C√≠clico\",\"PCAR4\":\"Consumo N√£o C√≠clico\",\"PCAR5\":\"Consumo N√£o C√≠clico\",\n",
    "    \"BRFS3\":\"Consumo N√£o C√≠clico\",\"JBSS3\":\"Consumo N√£o C√≠clico\",\"BEEF3\":\"Consumo N√£o C√≠clico\",\n",
    "    \"MDIA3\":\"Consumo N√£o C√≠clico\",\"NATU3\":\"Consumo N√£o C√≠clico\",\"RADL3\":\"Consumo N√£o C√≠clico\",\n",
    "    \"SLCE3\":\"Consumo N√£o C√≠clico\",\"SMTO3\":\"Consumo N√£o C√≠clico\",\"VAGR3\":\"Consumo N√£o C√≠clico\",\n",
    "    \"CRUZ3\":\"Consumo N√£o C√≠clico\",\"SMLE3\":\"Consumo N√£o C√≠clico\",\"HYPE3\":\"Consumo N√£o C√≠clico\",\n",
    "    \"MRFG3\":\"Consumo N√£o C√≠clico\",\n",
    "\n",
    "    # Materiais B√°sicos\n",
    "    \"VALE3\":\"Materiais B√°sicos\",\"VALE5\":\"Materiais B√°sicos\",\n",
    "    \"GGBR3\":\"Materiais B√°sicos\",\"GGBR4\":\"Materiais B√°sicos\",\"GOAU4\":\"Materiais B√°sicos\",\n",
    "    \"CSNA3\":\"Materiais B√°sicos\",\"USIM3\":\"Materiais B√°sicos\",\"USIM5\":\"Materiais B√°sicos\",\n",
    "    \"KLBN11\":\"Materiais B√°sicos\",\"KLBN4\":\"Materiais B√°sicos\",\"FIBR3\":\"Materiais B√°sicos\",\n",
    "    \"SUZB3\":\"Materiais B√°sicos\",\"SUZB5\":\"Materiais B√°sicos\",\"BRKM5\":\"Materiais B√°sicos\",\n",
    "    \"PMAM3\":\"Materiais B√°sicos\",\"MMXM3\":\"Materiais B√°sicos\",\"CNFB4\":\"Materiais B√°sicos\",\n",
    "    \"FFTL4\":\"Materiais B√°sicos\",\"CCXC3\":\"Materiais B√°sicos\",\"DTEX3\":\"Materiais B√°sicos\",\n",
    "\n",
    "    # Petr√≥leo, G√°s e Biocombust√≠veis\n",
    "    \"PETR3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"PETR4\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\n",
    "    \"ENAT3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"QGEP3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\n",
    "    \"OGXP3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"OSXB3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\n",
    "    \"UGPA3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"UGPA4\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\n",
    "    \"CSAN3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"RPMG3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\n",
    "    \"MPXE3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"LUPA3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\n",
    "    \"ENEV3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"HRTP3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\"BRDT3\":\"Petr√≥leo, G√°s e Biocombust√≠veis\",\n",
    "\n",
    "    # Sa√∫de\n",
    "    \"FLRY3\":\"Sa√∫de\",\"HAPV3\":\"Sa√∫de\",\"GNDI3\":\"Sa√∫de\",\"DASA3\":\"Sa√∫de\",\"QUAL3\":\"Sa√∫de\",\"AMIL3\":\"Sa√∫de\",\n",
    "    \"ODPV3\":\"Sa√∫de\",\n",
    "\n",
    "    # Tecnologia da Informa√ß√£o\n",
    "    \"TOTS3\":\"Tecnologia da Informa√ß√£o\",\"LINX3\":\"Tecnologia da Informa√ß√£o\",\n",
    "    \"POSI3\":\"Tecnologia da Informa√ß√£o\",\"VLID3\":\"Tecnologia da Informa√ß√£o\",\n",
    "\n",
    "    # Telecomunica√ß√µes\n",
    "    \"VIVT4\":\"Telecomunica√ß√µes\",\"VIVO4\":\"Telecomunica√ß√µes\",\"TIMP3\":\"Telecomunica√ß√µes\",\n",
    "    \"OIBR3\":\"Telecomunica√ß√µes\",\"OIBR4\":\"Telecomunica√ß√µes\",\n",
    "    \"TCSL3\":\"Telecomunica√ß√µes\",\"TCSL4\":\"Telecomunica√ß√µes\",\n",
    "    \"TNLP3\":\"Telecomunica√ß√µes\",\"TNLP4\":\"Telecomunica√ß√µes\",\n",
    "    \"TLPP4\":\"Telecomunica√ß√µes\",\"TMAR5\":\"Telecomunica√ß√µes\",\n",
    "    \"NETC4\":\"Telecomunica√ß√µes\",\"BRTO4\":\"Telecomunica√ß√µes\",\n",
    "\n",
    "    # Utilidade P√∫blica\n",
    "    \"ELET3\":\"Utilidade P√∫blica\",\"ELET6\":\"Utilidade P√∫blica\",\"CMIG3\":\"Utilidade P√∫blica\",\n",
    "    \"CMIG4\":\"Utilidade P√∫blica\",\"CPFE3\":\"Utilidade P√∫blica\",\"CPLE6\":\"Utilidade P√∫blica\",\n",
    "    \"EGIE3\":\"Utilidade P√∫blica\",\"ENBR3\":\"Utilidade P√∫blica\",\"ENGI11\":\"Utilidade P√∫blica\",\n",
    "    \"EQTL3\":\"Utilidade P√∫blica\",\"TAEE11\":\"Utilidade P√∫blica\",\"TRPL4\":\"Utilidade P√∫blica\",\n",
    "    \"TIET11\":\"Utilidade P√∫blica\",\"CESP6\":\"Utilidade P√∫blica\",\"LIGT3\":\"Utilidade P√∫blica\",\n",
    "    \"TBLE3\":\"Utilidade P√∫blica\",\"SAPR11\":\"Utilidade P√∫blica\",\"SAPR4\":\"Utilidade P√∫blica\",\n",
    "    \"SBSP3\":\"Utilidade P√∫blica\",\"GETI4\":\"Utilidade P√∫blica\",\"TERI3\":\"Utilidade P√∫blica\",\n",
    "    \"ELPL4\":\"Utilidade P√∫blica\",\"ELPL6\":\"Utilidade P√∫blica\",\"ALUP11\":\"Utilidade P√∫blica\",\"CSMG3\":\"Utilidade P√∫blica\",\n",
    "}\n",
    "\n",
    "def adicionar_setor_cvm(input_csv: str, output_csv: str = None) -> str:\n",
    "    \"\"\"\n",
    "    L√™ um CSV com coluna 'Ticker', adiciona 'SectorID' (CVM) e 'SectorName' e salva.\n",
    "    - input_csv: caminho de entrada (deve conter 'Ticker')\n",
    "    - output_csv: caminho de sa√≠da; se None, sobrescreve o input\n",
    "\n",
    "    Tamb√©m imprime um relat√≥rio com mapeados/n√£o mapeados.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    if \"Ticker\" not in df.columns:\n",
    "        raise ValueError(\"O arquivo precisa conter a coluna 'Ticker'.\")\n",
    "\n",
    "    # Normaliza ticker\n",
    "    tickers = df[\"Ticker\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Aplica mapeamento\n",
    "    sector_name = tickers.map(TICKER_TO_SECTOR).fillna(\"Desconhecido\")\n",
    "    sector_id = sector_name.map(lambda s: CVM_SECTORS.get(s, -1))\n",
    "\n",
    "    # Anexa\n",
    "    df[\"SectorID\"] = sector_id.values\n",
    "    df[\"SectorName\"] = sector_name.values\n",
    "\n",
    "    # Salva\n",
    "    if output_csv is None:\n",
    "        output_csv = input_csv\n",
    "    Path(output_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Relat√≥rio\n",
    "    unmapped = df.loc[df[\"SectorID\"] == -1, \"Ticker\"].unique().tolist()\n",
    "    print(f\"[CVM Setores] Total: {len(df)} | Mapeados: {(df['SectorID']!=-1).sum()} | N√£o mapeados: {(df['SectorID']==-1).sum()}\")\n",
    "    if unmapped:\n",
    "        print(\"[Aten√ß√£o] Tickers a revisar (legados/raros):\", \", \".join(sorted(unmapped)))\n",
    "\n",
    "    return str(output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils\n",
    "# -----------------------\n",
    "\n",
    "def to_float_smart(x):\n",
    "    \"\"\"\n",
    "    Converte strings tipo '1.234,56' (BR) ou '1,234.56' (US) e variantes em float.\n",
    "    Trata negativos e milhares. Retorna NaN se n√£o der.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if s in {\"\", \"-\", \"--\", \"nan\", \"NaN\", \"None\", \"NULL\"}:\n",
    "        return np.nan\n",
    "\n",
    "    # mant√©m apenas d√≠gitos, sinais e separadores\n",
    "    s = re.sub(r\"[^0-9\\-\\.,]\", \"\", s)\n",
    "\n",
    "    has_dot   = \".\" in s\n",
    "    has_comma = \",\" in s\n",
    "\n",
    "    try:\n",
    "        if has_dot and has_comma:\n",
    "            # decide pelo separador mais √† direita\n",
    "            if s.rfind(\",\") > s.rfind(\".\"):\n",
    "                # BR: 1.234,56 -> 1234.56\n",
    "                s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            else:\n",
    "                # US: 1,234.56 -> 1234.56\n",
    "                s = s.replace(\",\", \"\")\n",
    "            return float(s)\n",
    "\n",
    "        if has_comma and not has_dot:\n",
    "            # BR decimal: 1234,56 -> 1234.56\n",
    "            return float(s.replace(\",\", \".\"))\n",
    "\n",
    "        if has_dot and not has_comma:\n",
    "            # Pode ser decimal (um ponto) ou milhares (v√°rios pontos)\n",
    "            if s.count(\".\") == 1:\n",
    "                return float(s)  # 1234.56\n",
    "            else:\n",
    "                # 109.641.290.194 -> 109641290194\n",
    "                return float(s.replace(\".\", \"\"))\n",
    "\n",
    "        # S√≥ d√≠gitos e talvez sinal\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    \n",
    "def parse_date_br_any(sr: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converte a coluna de datas de publica√ß√£o para datetime,\n",
    "    assumindo SEMPRE padr√£o brasileiro (DD/MM/AAAA) quando houver ambiguidade.\n",
    "\n",
    "    Regras:\n",
    "    - Se estiver no padr√£o ISO 'YYYY-MM-DD', usamos isso direto (n√£o √© amb√≠guo).\n",
    "    - Se estiver no padr√£o brasileiro 'DD/MM/YYYY', interpretamos como dia/m√™s/ano.\n",
    "    - Se vier em qualquer outro formato, tentamos parse com dayfirst=True.\n",
    "    - No final, retornamos datetime normalizado (sem hora).\n",
    "    \"\"\"\n",
    "\n",
    "    s = sr.astype(str).str.strip()\n",
    "\n",
    "    # 1) tenta ISO claro: 2024-03-31\n",
    "    iso_mask = s.str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "    out_iso = pd.to_datetime(\n",
    "        s.where(iso_mask),\n",
    "        format=\"%Y-%m-%d\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 2) tenta BR claro: 31/03/2024\n",
    "    br_mask = s.str.match(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n",
    "    out_br = pd.to_datetime(\n",
    "        s.where(br_mask),\n",
    "        format=\"%d/%m/%Y\",\n",
    "        dayfirst=True,\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 3) come√ßa com ISO e preenche lacunas com BR\n",
    "    out = out_iso.fillna(out_br)\n",
    "\n",
    "    # 4) fallback gen√©rico:\n",
    "    #    qualquer coisa que sobrou a gente interpreta assumindo padr√£o brasileiro (dayfirst=True)\n",
    "    still_nat = out.isna()\n",
    "    if still_nat.any():\n",
    "        out_fallback = pd.to_datetime(\n",
    "            s[still_nat],\n",
    "            errors=\"coerce\",\n",
    "            dayfirst=True   # <- for√ßa sem√¢ntica brasileira\n",
    "        )\n",
    "        out.loc[still_nat] = out_fallback\n",
    "\n",
    "    # 5) normaliza para \"apenas a data\" (zera hora)\n",
    "    out = out.dt.normalize()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def clean_events(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpeza b√°sica do dataset bruto, antes de padronizar:\n",
    "    1) Converte AnnounceDate/EventTradeDate para datetime.\n",
    "    2) Remove linhas sem AnnounceDate.\n",
    "    3) Dedup por (Ticker, AnnounceDate), mantendo o menor EventTradeDate.\n",
    "    4) Ordena.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # normaliza datas vindas em string DD/MM/AAAA\n",
    "    out[\"AnnounceDate\"] = pd.to_datetime(out[\"AnnounceDate\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "    if \"EventTradeDate\" in out.columns:\n",
    "        out[\"EventTradeDate\"] = pd.to_datetime(out[\"EventTradeDate\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "    # remove eventos sem data de an√∫ncio v√°lida\n",
    "    out = out.dropna(subset=[\"AnnounceDate\"]).copy()\n",
    "    \n",
    "    if \"Year\" not in out.columns:\n",
    "        out[\"Year\"] = out[\"EventTradeDate\"].dt.year\n",
    "\n",
    "    # ordena pra poder resolver duplicatas determin√≠sticamente\n",
    "    sort_cols = [\"Ticker\", \"AnnounceDate\"]\n",
    "    if \"EventTradeDate\" in out.columns:\n",
    "        sort_cols.append(\"EventTradeDate\")\n",
    "\n",
    "    out = (\n",
    "        out.sort_values(sort_cols)\n",
    "           .drop_duplicates(subset=[\"Ticker\",\"AnnounceDate\"], keep=\"first\")\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "def merge_price_features_into_events(\n",
    "    events_df: pd.DataFrame,\n",
    "    prices_dir: str = \"dataset/prices_processed\",\n",
    "    features_cols: Optional[List[str]] = None,\n",
    "    outfile: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Faz merge (left) das features vindas de dataset/prices_processed/<TICKER>.SA.csv\n",
    "    em (Ticker, AnnounceDate). AnnounceDate ‚â° Data (preg√£o T0) no prices.\n",
    "    Se 'ret_t0_t1' n√£o existir, cria a partir de Daily_Return.shift(-1).\n",
    "    \"\"\"\n",
    "    if features_cols is None:\n",
    "        features_cols = [\n",
    "            \"ret_t0_t1\",\n",
    "            \"MA5\",\"MA50\",\"MA200\",\n",
    "            \"RSI9\",\"RSI14\",\"RSI30\",\n",
    "            \"MA5_50\",\"MA5_200\",\"MA50_200\",\n",
    "            \"MOM_1M\",\"MOM_3M\",\"MOM_6M\",\"MOM_12M\",\n",
    "        ]\n",
    "\n",
    "    prices_dir = Path(prices_dir)\n",
    "\n",
    "    ev = events_df.copy()\n",
    "    # üîß Garanta datetime na chave do lado dos eventos\n",
    "    ev[\"AnnounceDate\"] = parse_date_br_any(ev[\"AnnounceDate\"])\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for tkr in ev[\"Ticker\"].dropna().unique():\n",
    "        price_file = prices_dir / f\"{tkr}.SA.csv\"\n",
    "        if not price_file.exists():\n",
    "            alts = list(prices_dir.glob(f\"{tkr}*.csv\"))\n",
    "            if not alts:\n",
    "                continue\n",
    "            price_file = alts[0]\n",
    "\n",
    "        px = pd.read_csv(price_file, dtype=str)\n",
    "        if \"Data\" not in px.columns:\n",
    "            continue\n",
    "\n",
    "        px[\"Data\"] = parse_date_br_any(px[\"Data\"])\n",
    "\n",
    "        # Cria ret_t0_t1 se necess√°rio\n",
    "        if \"ret_t0_t1\" not in px.columns:\n",
    "            if \"Daily_Return\" in px.columns:\n",
    "                dr = pd.to_numeric(px[\"Daily_Return\"].apply(to_float_smart), errors=\"coerce\")\n",
    "                px[\"ret_t0_t1\"] = dr.shift(-1)\n",
    "            else:\n",
    "                px[\"ret_t0_t1\"] = np.nan\n",
    "\n",
    "        # Garante as colunas pedidas\n",
    "        for c in features_cols:\n",
    "            if c not in px.columns:\n",
    "                px[c] = np.nan\n",
    "\n",
    "        # Datas de an√∫ncio desse ticker (datetime)\n",
    "        adates = ev.loc[ev[\"Ticker\"] == tkr, \"AnnounceDate\"].dropna().unique()\n",
    "        if adates.size == 0:\n",
    "            continue\n",
    "\n",
    "        sub = px.loc[px[\"Data\"].isin(adates), [\"Data\", *features_cols]].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        sub[\"Ticker\"] = tkr\n",
    "        sub = sub.rename(columns={\"Data\": \"AnnounceDate\"})\n",
    "\n",
    "        sub = (\n",
    "            sub.sort_values([\"Ticker\",\"AnnounceDate\"])\n",
    "               .drop_duplicates([\"Ticker\",\"AnnounceDate\"], keep=\"first\")\n",
    "        )\n",
    "        chunks.append(sub)\n",
    "\n",
    "    if not chunks:\n",
    "        if outfile:\n",
    "            Path(os.path.dirname(outfile) or \".\").mkdir(parents=True, exist_ok=True)\n",
    "            ev.to_csv(outfile, index=False)\n",
    "        return ev\n",
    "\n",
    "    feat_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    merged = ev.merge(\n",
    "        feat_df,\n",
    "        on=[\"Ticker\",\"AnnounceDate\"],\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\"\n",
    "    )\n",
    "\n",
    "    # (opcional) voltar AnnounceDate para DD/MM/AAAA\n",
    "    merged[\"AnnounceDate\"] = merged[\"AnnounceDate\"].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    if outfile:\n",
    "        Path(os.path.dirname(outfile) or \".\").mkdir(parents=True, exist_ok=True)\n",
    "        merged.to_csv(outfile, index=False)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d144148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos\n",
    "# -----------------------\n",
    "RAW_PATH        = \"dataset/final/pead_event_dataset_2010_2019.csv\"\n",
    "FINAL_CLEAN_OUT = \"dataset/final/final_pead_event_dataset_2010_2019.csv\"\n",
    "os.makedirs(os.path.dirname(FINAL_CLEAN_OUT), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b753715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo final salvo em dataset/final/final_pead_event_dataset_2010_2019.csv\n",
      "Head do final padronizado:\n",
      "  Ticker AnnounceDate EventTradeDate   CAR_30D CAR_Sign      Beta  \\\n",
      "0  ABEV3   2014-03-24     2014-03-25 -0.098386        0  0.694085   \n",
      "1  ABEV3   2014-05-07     2014-05-08 -0.029948        0  0.588614   \n",
      "2  ABEV3   2014-07-31     2014-08-01 -0.023337        0  0.610779   \n",
      "3  ABEV3   2014-10-31     2014-11-03  0.021248        1  0.583170   \n",
      "4  ABEV3   2015-02-26     2015-02-27  0.035410        1  0.588007   \n",
      "\n",
      "   EstimationLen    FundSource        Data    Empresa  ...  Outros_PC_MET  \\\n",
      "0           88.0  ABEV3.SA.csv  31/12/2013  AMBEV S/A  ...       4224.941   \n",
      "1          117.0  ABEV3.SA.csv  31/03/2014  AMBEV S/A  ...       4753.495   \n",
      "2          175.0  ABEV3.SA.csv  30/06/2014  AMBEV S/A  ...       3437.598   \n",
      "3          241.0  ABEV3.SA.csv  30/09/2014  AMBEV S/A  ...       3905.387   \n",
      "4          318.0  ABEV3.SA.csv  31/12/2014  AMBEV S/A  ...       7846.705   \n",
      "\n",
      "   Outros_PC_Q_Change  Outros_PC_Y_Change    LUB_MET  LUB_Q_Change  \\\n",
      "0            3027.282                 NaN  23393.590           NaN   \n",
      "1             528.554                 NaN        NaN           NaN   \n",
      "2           -1315.897            2057.461        NaN           NaN   \n",
      "3             467.789            2707.728        NaN           NaN   \n",
      "4            3941.318            3621.764  25265.198           NaN   \n",
      "\n",
      "   LUB_Y_Change  EPS_EarningsSurprise  EPS_Earnings_Surprise_Backward_Diff  \\\n",
      "0           NaN                   NaN                                  NaN   \n",
      "1           NaN                   NaN                                  NaN   \n",
      "2           NaN                   NaN                                  NaN   \n",
      "3           NaN                   NaN                                  NaN   \n",
      "4      1871.608                   NaN                                  NaN   \n",
      "\n",
      "   EPS_Earnings_Surprise_Backward_Ave_Diff  Year  \n",
      "0                                      NaN  2014  \n",
      "1                                      NaN  2014  \n",
      "2                                      NaN  2014  \n",
      "3                                      NaN  2014  \n",
      "4                                      NaN  2015  \n",
      "\n",
      "[5 rows x 117 columns]\n",
      "\n",
      "Descri√ß√£o CAR_30D ap√≥s limpeza (sem padronizar):\n",
      "count    4915.000000\n",
      "mean       -0.006176\n",
      "std         0.135902\n",
      "min        -1.418663\n",
      "1%         -0.417688\n",
      "5%         -0.213018\n",
      "50%         0.001703\n",
      "95%         0.184176\n",
      "99%         0.337382\n",
      "max         0.861991\n",
      "Name: CAR_30D, dtype: float64\n",
      "OK: (4919, 130) linhas/colunas\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE FINAL\n",
    "# -----------------------\n",
    "\n",
    "# 1. l√™ dataset bruto consolidado (sa√≠da do EventDatasetBuilder)\n",
    "raw_df = pd.read_csv(RAW_PATH, dtype=str)\n",
    "\n",
    "# 2. convers√£o de tipos b√°sicos antes de limpar\n",
    "#    - AnnounceDate / EventTradeDate ‚Üí datetime\n",
    "#    - CAR_30D, CAR_Sign, Beta etc ‚Üí num√©rico\n",
    "tmp = raw_df.copy()\n",
    "\n",
    "# converte datas (n√£o vou sobrescrever agora; clean_events faz isso tamb√©m)\n",
    "# mas j√° convertemos num√©ricos cr√≠ticos ANTES do winsor\n",
    "num_like_cols = [\n",
    "    c for c in tmp.columns\n",
    "    if c not in [\"Ticker\",\"CAR_Sign\",\"AnnounceDate\",\"EventTradeDate\",\"FundSource\", \"Empresa\", \"Consolidado\", \"Convencao\", \"Moeda\", \"Data_Demonstracao\", \"Data_Analise\", \"QuarterEnd\", \"Data\" ]\n",
    "]\n",
    "\n",
    "for c in num_like_cols:\n",
    "    # tenta converter pra float usando nosso parser robusto\n",
    "    tmp[c] = tmp[c].apply(to_float_smart)\n",
    "\n",
    "# 3. limpeza estrutural (remove duplicata, garante coer√™ncia temporal)\n",
    "tmp_clean = clean_events(tmp)\n",
    "\n",
    "# 4. winsor + z-score\n",
    "#final_std = winsorize_and_standardize(tmp_clean)\n",
    "\n",
    "# 5. salva resultado final\n",
    "#final_std.to_csv(FINAL_CLEAN_OUT, index=False)\n",
    "tmp_clean.to_csv(FINAL_CLEAN_OUT, index=False)\n",
    "print(f\"Arquivo final salvo em {FINAL_CLEAN_OUT}\")\n",
    "\n",
    "# (Opcional) sanity check r√°pido\n",
    "print(\"Head do final padronizado:\")\n",
    "print(tmp_clean.head())\n",
    "\n",
    "print(\"\\nDescri√ß√£o CAR_30D ap√≥s limpeza (sem padronizar):\")\n",
    "if \"CAR_30D\" in tmp_clean.columns:\n",
    "    print(tmp_clean[\"CAR_30D\"].describe(percentiles=[.01,.05,.5,.95,.99]))\n",
    "else:\n",
    "    print(\"Coluna CAR_30D n√£o encontrada em tmp_clean.\")\n",
    "\n",
    "events_path = \"dataset/final/pead_event_dataset_2010_2019.csv\"\n",
    "events = pd.read_csv(events_path, dtype=str)\n",
    "\n",
    "merged = merge_price_features_into_events(\n",
    "    events_df=events,\n",
    "    prices_dir=\"dataset/prices_processed\",\n",
    "    outfile=\"dataset/final/pead_event_dataset_2010_2019_with_prices.csv\"\n",
    ")\n",
    "print(\"OK:\", merged.shape, \"linhas/colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88a76935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CVM Setores] Total: 4919 | Mapeados: 4919 | N√£o mapeados: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dataset/final/dataprep_pead_event_2010_2019.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adicionar_setor_cvm(\"dataset/final/pead_event_dataset_2010_2019_with_prices.csv\", \"dataset/final/dataprep_pead_event_2010_2019.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-ml)",
   "language": "python",
   "name": "fin-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
