{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6002f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils\n",
    "\n",
    "def ensure_dirs(paths):\n",
    "    \"\"\"\n",
    "    Garante que todas as pastas em 'paths' existem.\n",
    "    paths pode ser lista de strings ou uma única string.\n",
    "    \"\"\"\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def to_float_smart(x):\n",
    "    \"\"\"\n",
    "    Converte strings tipo '1.234,56' (BR) ou '1,234.56' (US) e variantes em float.\n",
    "    Trata negativos e milhares. Retorna NaN se não der.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if s in {\"\", \"-\", \"--\", \"nan\", \"NaN\", \"None\", \"NULL\"}:\n",
    "        return np.nan\n",
    "\n",
    "    # mantém apenas dígitos, sinais e separadores\n",
    "    s = re.sub(r\"[^0-9\\-\\.,]\", \"\", s)\n",
    "\n",
    "    has_dot   = \".\" in s\n",
    "    has_comma = \",\" in s\n",
    "\n",
    "    try:\n",
    "        if has_dot and has_comma:\n",
    "            # decide pelo separador mais à direita\n",
    "            if s.rfind(\",\") > s.rfind(\".\"):\n",
    "                # BR: 1.234,56 -> 1234.56\n",
    "                s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            else:\n",
    "                # US: 1,234.56 -> 1234.56\n",
    "                s = s.replace(\",\", \"\")\n",
    "            return float(s)\n",
    "\n",
    "        if has_comma and not has_dot:\n",
    "            # BR decimal: 1234,56 -> 1234.56\n",
    "            return float(s.replace(\",\", \".\"))\n",
    "\n",
    "        if has_dot and not has_comma:\n",
    "            # Pode ser decimal (um ponto) ou milhares (vários pontos)\n",
    "            if s.count(\".\") == 1:\n",
    "                return float(s)  # 1234.56\n",
    "            else:\n",
    "                # 109.641.290.194 -> 109641290194\n",
    "                return float(s.replace(\".\", \"\"))\n",
    "\n",
    "        # Só dígitos e talvez sinal\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "def parse_date_br_any(sr: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converte a coluna de datas de publicação para datetime,\n",
    "    assumindo SEMPRE padrão brasileiro (DD/MM/AAAA) quando houver ambiguidade.\n",
    "\n",
    "    Regras:\n",
    "    - Se estiver no padrão ISO 'YYYY-MM-DD', usamos isso direto (não é ambíguo).\n",
    "    - Se estiver no padrão brasileiro 'DD/MM/YYYY', interpretamos como dia/mês/ano.\n",
    "    - Se vier em qualquer outro formato, tentamos parse com dayfirst=True.\n",
    "    - No final, retornamos datetime normalizado (sem hora).\n",
    "    \"\"\"\n",
    "\n",
    "    s = sr.astype(str).str.strip()\n",
    "\n",
    "    # 1) tenta ISO claro: 2024-03-31\n",
    "    iso_mask = s.str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "    out_iso = pd.to_datetime(\n",
    "        s.where(iso_mask),\n",
    "        format=\"%Y-%m-%d\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 2) tenta BR claro: 31/03/2024\n",
    "    br_mask = s.str.match(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n",
    "    out_br = pd.to_datetime(\n",
    "        s.where(br_mask),\n",
    "        format=\"%d/%m/%Y\",\n",
    "        dayfirst=True,\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 3) começa com ISO e preenche lacunas com BR\n",
    "    out = out_iso.fillna(out_br)\n",
    "\n",
    "    # 4) fallback genérico:\n",
    "    #    qualquer coisa que sobrou a gente interpreta assumindo padrão brasileiro (dayfirst=True)\n",
    "    still_nat = out.isna()\n",
    "    if still_nat.any():\n",
    "        out_fallback = pd.to_datetime(\n",
    "            s[still_nat],\n",
    "            errors=\"coerce\",\n",
    "            dayfirst=True   # <- força semântica brasileira\n",
    "        )\n",
    "        out.loc[still_nat] = out_fallback\n",
    "\n",
    "    # 5) normaliza para \"apenas a data\" (zera hora)\n",
    "    out = out.dt.normalize()\n",
    "\n",
    "    return out\n",
    "        \n",
    "# =========================================\n",
    "# Core helpers: índice do evento, beta, CAR\n",
    "# =========================================\n",
    "\n",
    "   \n",
    "def detect_start_index(prices: pd.DataFrame,\n",
    "                       event_date: pd.Timestamp) -> int:\n",
    "    \"\"\"\n",
    "    Retorna o índice T1 = primeiro pregão APÓS o pregão marcado como evento.\n",
    "\n",
    "    event_date: data do pregão em que 'event == 1' (earnings divulgado).\n",
    "    T1 = índice do próximo pregão (reação), não o próprio dia do evento.\n",
    "    \"\"\"\n",
    "\n",
    "    if prices is None or prices.empty or \"Date\" not in prices:\n",
    "        return 0\n",
    "\n",
    "    pr_sorted = prices.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # achar o índice da data do evento em si\n",
    "    hit = pr_sorted.index[pr_sorted[\"Date\"] == event_date]\n",
    "\n",
    "    if len(hit) > 0:\n",
    "        base_idx = int(hit[0])\n",
    "        # queremos o pregão seguinte\n",
    "        t1_idx = base_idx + 1\n",
    "        # clamp para não sair do range\n",
    "        if t1_idx >= len(pr_sorted):\n",
    "            t1_idx = len(pr_sorted) - 1\n",
    "        return t1_idx\n",
    "\n",
    "    # fallback: se a data exata não está no df (não deveria acontecer se veio de event==1),\n",
    "    # achar posição >= event_date e pular pro seguinte\n",
    "    ds = pr_sorted[\"Date\"].values\n",
    "    idx_ge = np.searchsorted(ds, np.array(event_date, dtype=\"datetime64[ns]\"))\n",
    "    t1_idx = idx_ge + 1\n",
    "    if t1_idx >= len(pr_sorted):\n",
    "        t1_idx = len(pr_sorted) - 1\n",
    "    return int(t1_idx)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "def estimate_beta(stock_df: pd.DataFrame,\n",
    "                  mkt_df: pd.DataFrame,\n",
    "                  rf_df: pd.DataFrame,\n",
    "                  event_idx: int,\n",
    "                  estimation_window: int = 504) -> float:\n",
    "\n",
    "    stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
    "    mkt_df   = mkt_df.sort_values('Date').reset_index(drop=True)\n",
    "    rf_df    = rf_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    m = stock_df[['Date','Close']].merge(\n",
    "            mkt_df[['Date','Close']], on='Date', suffixes=('_i','_m')\n",
    "        )\n",
    "    m = m.merge(\n",
    "            rf_df[['Date','rf_daily']],\n",
    "            on='Date',\n",
    "            how='left'\n",
    "        ).ffill().sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    m[\"Close_i\"]  = m[\"Close_i\"].apply(to_float_smart).astype(float)\n",
    "    m[\"Close_m\"]  = m[\"Close_m\"].apply(to_float_smart).astype(float)\n",
    "    m[\"rf_daily\"] = m[\"rf_daily\"].apply(to_float_smart).astype(float)\n",
    "\n",
    "    m['ri'] = np.log(m['Close_i'] / m['Close_i'].shift(1))\n",
    "    m['rm'] = np.log(m['Close_m'] / m['Close_m'].shift(1))\n",
    "\n",
    "    if event_idx < 2:\n",
    "        return np.nan\n",
    "    if event_idx >= len(stock_df):\n",
    "        return np.nan\n",
    "\n",
    "    event_date = stock_df.iloc[event_idx]['Date']\n",
    "\n",
    "    eidx_arr = m.index[m['Date'] == event_date]\n",
    "    if len(eidx_arr) == 0:\n",
    "        return np.nan\n",
    "    eidx = int(eidx_arr[0])\n",
    "\n",
    "    start = max(m.index.min(), eidx - estimation_window)\n",
    "    end   = eidx - 1\n",
    "\n",
    "    # precisa de histórico mínimo pra regressão\n",
    "    if end - start < 30:\n",
    "        return np.nan\n",
    "\n",
    "    win = m.loc[start:end].dropna()\n",
    "    if win.empty:\n",
    "        return np.nan\n",
    "\n",
    "    x = (win['rm'] - win['rf_daily']).values.reshape(-1, 1)\n",
    "    y = (win['ri'] - win['rf_daily']).values.reshape(-1, 1)\n",
    "\n",
    "    if x.shape[0] < 5:\n",
    "        return np.nan\n",
    "\n",
    "    beta = np.linalg.lstsq(x, y, rcond=None)[0].ravel()[0]\n",
    "    return float(beta)\n",
    "\n",
    "\n",
    "def compute_car(stock_df: pd.DataFrame,\n",
    "                mkt_df: pd.DataFrame,\n",
    "                rf_df: pd.DataFrame,\n",
    "                event_idx: int,\n",
    "                beta: float,\n",
    "                holding_days: int = 30) -> float:\n",
    "\n",
    "    stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
    "    mkt_df   = mkt_df.sort_values('Date').reset_index(drop=True)\n",
    "    rf_df    = rf_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    m = stock_df[['Date','Close']].merge(\n",
    "            mkt_df[['Date','Close']], on='Date', suffixes=('_i','_m')\n",
    "        )\n",
    "    m = m.merge(\n",
    "            rf_df[['Date','rf_daily']],\n",
    "            on='Date',\n",
    "            how='left'\n",
    "        ).ffill().sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    m[\"Close_i\"]  = m[\"Close_i\"].apply(to_float_smart).astype(float)\n",
    "    m[\"Close_m\"]  = m[\"Close_m\"].apply(to_float_smart).astype(float)\n",
    "    m[\"rf_daily\"] = m[\"rf_daily\"].apply(to_float_smart).astype(float)\n",
    "\n",
    "    m['ri'] = np.log(m['Close_i'] / m['Close_i'].shift(1))\n",
    "    m['rm'] = np.log(m['Close_m'] / m['Close_m'].shift(1))\n",
    "\n",
    "    start = event_idx\n",
    "    end   = min(start + holding_days - 1, len(m) - 1)\n",
    "\n",
    "    seg = m.iloc[start:end+1].dropna()\n",
    "    if seg.empty:\n",
    "        return np.nan\n",
    "\n",
    "    seg['E_ri'] = seg['rf_daily'] + beta * (seg['rm'] - seg['rf_daily'])\n",
    "    seg['AR']   = seg['ri'] - seg['E_ri']\n",
    "\n",
    "    return float(seg['AR'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d81600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregadores de mercado\n",
    "# =========================================\n",
    "class MarketAndRiskLoader:\n",
    "    @staticmethod\n",
    "    def load_ibov_csv(path_ibov: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path_ibov, dtype=str)\n",
    "        df[\"Data\"] = pd.to_datetime(df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "        for c in [\"FechAjust\",\"FechHist\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = df[c].apply(to_float_smart)\n",
    "\n",
    "        close_col = \"FechAjust\" if \"FechAjust\" in df.columns and df[\"FechAjust\"].notna().any() else \"FechHist\"\n",
    "\n",
    "        out = (df[[\"Data\", close_col]]\n",
    "               .rename(columns={\"Data\":\"Date\", close_col:\"Close\"})\n",
    "               .dropna(subset=[\"Date\",\"Close\"])\n",
    "               .sort_values(\"Date\")\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "        out[\"Close\"] = out[\"Close\"].astype(float)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def load_cdi_csv(path_cdi: str) -> pd.DataFrame:\n",
    "        df = pd.read_csv(path_cdi, dtype=str)\n",
    "        df[\"Data\"] = pd.to_datetime(df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "        if \"Var\" in df.columns:\n",
    "            df[\"Var\"] = df[\"Var\"].apply(to_float_smart)\n",
    "            if df[\"Var\"].notna().sum() > 3:\n",
    "                out = df[[\"Data\"]].copy()\n",
    "                out[\"rf_daily\"] = df[\"Var\"] / 100.0\n",
    "                out = (out.rename(columns={\"Data\":\"Date\"})\n",
    "                          .dropna()\n",
    "                          .sort_values(\"Date\")\n",
    "                          .reset_index(drop=True))\n",
    "                out[\"rf_daily\"] = out[\"rf_daily\"].astype(float)\n",
    "                return out\n",
    "\n",
    "        for c in [\"FechAjust\",\"FechHist\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = df[c].apply(to_float_smart)\n",
    "                if df[c].notna().sum() > 3:\n",
    "                    r = df[c].pct_change()\n",
    "                    out = df[[\"Data\"]].copy()\n",
    "                    out[\"rf_daily\"] = r\n",
    "                    out = (out.rename(columns={\"Data\":\"Date\"})\n",
    "                              .dropna()\n",
    "                              .sort_values(\"Date\")\n",
    "                              .reset_index(drop=True))\n",
    "                    out[\"rf_daily\"] = out[\"rf_daily\"].astype(float)\n",
    "                    return out\n",
    "\n",
    "        return pd.DataFrame(columns=[\"Date\",\"rf_daily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb22998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builder de eventos PEAD / CAR\n",
    "# =========================================\n",
    "class EventDatasetBuilder:\n",
    "    def __init__(self, \n",
    "                 mkt_df: pd.DataFrame, \n",
    "                 rf_df: pd.DataFrame,\n",
    "                 estimation_window: int = 504, \n",
    "                 holding_days: int = 30, \n",
    "                 min_estimation: int = 60):\n",
    "        self.mkt = mkt_df.sort_values('Date').reset_index(drop=True)\n",
    "        self.rf  = rf_df.sort_values('Date').reset_index(drop=True)\n",
    "        self.estimation_window = estimation_window\n",
    "        self.holding_days      = holding_days\n",
    "        self.min_estimation    = min_estimation\n",
    "\n",
    "    def build_for_ticker(self, \n",
    "                         tkr: str,\n",
    "                         price_csv: str,\n",
    "                         fund_csv: str) -> pd.DataFrame:\n",
    "        # ====== 1. Carrega preços processados ======\n",
    "        px = pd.read_csv(price_csv, dtype=str)\n",
    "\n",
    "#        # Data em datetime BR\n",
    "        px[\"Data\"] = parse_date_br_any(px[\"Data\"])\n",
    "\n",
    "#        # Close em float\n",
    "#        if \"Close\" in px.columns:\n",
    "#            px[\"Close\"] = px[\"Close\"].apply(to_float_smart)\n",
    "#        elif \"FechAjust\" in px.columns:\n",
    "#            px[\"Close\"] = px[\"FechAjust\"].apply(to_float_smart)\n",
    "#        elif \"FechHist\" in px.columns:\n",
    "#            px[\"Close\"] = px[\"FechHist\"].apply(to_float_smart)\n",
    "#        else:\n",
    "#            return pd.DataFrame()\n",
    "\n",
    "        # event como inteiro (0/1)\n",
    "        if \"event\" not in px.columns:\n",
    "            return pd.DataFrame()\n",
    "        px[\"event\"] = pd.to_numeric(px[\"event\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        # limpa linhas inválidas\n",
    "        px = (px.dropna(subset=[\"Data\",\"Close\"])\n",
    "                .sort_values(\"Data\")\n",
    "                .reset_index(drop=True))\n",
    "        #px[\"Close\"] = px[\"Close\"].astype(float)\n",
    "\n",
    "        # dataframes auxiliares usados em beta e CAR\n",
    "        px_idx   = px[[\"Data\"]].rename(columns={\"Data\":\"Date\"})         # só datas\n",
    "        px_close = px[[\"Data\",\"Close\"]].rename(columns={\"Data\":\"Date\"}) # datas + preço\n",
    "\n",
    "        # ====== 2. Definir quais são os eventos ======\n",
    "        # AnnounceDate = cada pregão onde event == 1\n",
    "        event_rows = px[px[\"event\"] == 1].copy().sort_values(\"Data\")\n",
    "        announce_dates = event_rows[\"Data\"].tolist()  # list de Timestamp\n",
    "\n",
    "        if len(announce_dates) == 0:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # ====== 3. Carrega fundamentals só para anexar features ======\n",
    "        fund = pd.read_csv(fund_csv, dtype=str)\n",
    "\n",
    "        if \"AnnounceDate\" in fund.columns:\n",
    "            fund[\"AnnounceDate\"] = parse_date_br_any(fund[\"AnnounceDate\"])\n",
    "        elif \"Data_Publicacao\" in fund.columns:\n",
    "            fund[\"AnnounceDate\"] = parse_date_br_any(fund[\"Data_Publicacao\"])\n",
    "        else:\n",
    "            fund[\"AnnounceDate\"] = pd.NaT\n",
    "\n",
    "        fund = (fund.dropna(subset=[\"AnnounceDate\"])\n",
    "                    .sort_values(\"AnnounceDate\")\n",
    "                    .reset_index(drop=True))\n",
    "        fund_feats = fund.copy()\n",
    "\n",
    "        # ====== 4. Loop de cada evento ======\n",
    "        recs = []\n",
    "\n",
    "        for announced_at in announce_dates:\n",
    "            # announced_at: Timestamp do pregão onde event == 1\n",
    "            # t1_idx: índice do PRIMEIRO pregão DEPOIS do evento\n",
    "            t1_idx = detect_start_index(px_idx, announced_at)\n",
    "\n",
    "            # histórico suficiente p/ estimar beta?\n",
    "            est_len = min(self.estimation_window, t1_idx)\n",
    "            if est_len < self.min_estimation:\n",
    "                continue\n",
    "\n",
    "            beta = estimate_beta(px_close, self.mkt, self.rf, t1_idx, est_len)\n",
    "            if pd.isna(beta):\n",
    "                continue\n",
    "\n",
    "            car  = compute_car(px_close, self.mkt, self.rf, t1_idx, beta, self.holding_days)\n",
    "            if pd.isna(car):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "\n",
    "            trade_date = px_idx.iloc[t1_idx][\"Date\"]  # pregão usado como início do CAR (T+1)\n",
    "\n",
    "            row = {\n",
    "                \"Ticker\"        : tkr,\n",
    "                # AnnounceDate agora é o pregão marcado com event == 1 (T0)\n",
    "                \"AnnounceDate\"  : announced_at.strftime(\"%d/%m/%Y\") if not pd.isna(announced_at) else \"\",\n",
    "                # EventTradeDate é T+1 = início da janela CAR\n",
    "                \"EventTradeDate\": trade_date.strftime(\"%d/%m/%Y\") if not pd.isna(trade_date) else \"\",\n",
    "                \"CAR_30D\"       : float(car),\n",
    "                \"CAR_Sign\"      : int(car > 0),\n",
    "                \"Beta\"          : float(beta),\n",
    "                \"EstimationLen\" : int(est_len),\n",
    "                \"FundSource\"    : os.path.basename(fund_csv),\n",
    "            }\n",
    "\n",
    "            # opcional: anexar features fundamentalistas \"válidas até aquele evento\"\n",
    "            if not fund_feats.empty:\n",
    "                # pega a última linha de fund com AnnounceDate <= announced_at\n",
    "                mask = fund_feats[\"AnnounceDate\"] <= announced_at\n",
    "                if mask.any():\n",
    "                    fmatch = fund_feats.loc[mask].iloc[-1]\n",
    "                    extras = fmatch.drop(labels=[\"AnnounceDate\"], errors=\"ignore\").to_dict()\n",
    "                    row.update(extras)\n",
    "\n",
    "            recs.append(row)\n",
    "\n",
    "        out_df = pd.DataFrame(recs)\n",
    "\n",
    "        if \"AnnounceDate\" in out_df.columns:\n",
    "            # como agora AnnounceDate é string BR \"DD/MM/AAAA\",\n",
    "            # vamos ordenar por data convertendo temporariamente\n",
    "            order_helper = parse_date_br_any(out_df[\"AnnounceDate\"])\n",
    "            out_df = out_df.assign(_ord=order_helper).sort_values([\"_ord\",\"Ticker\"]).drop(columns=\"_ord\").reset_index(drop=True)\n",
    "\n",
    "        return out_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f811a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK — dataset consolidado salvo em dataset/final/pead_event_dataset_2010_2019.csv\n",
      "   Ticker AnnounceDate EventTradeDate   CAR_30D  CAR_Sign      Beta  \\\n",
      "0   FIBR3   01/02/2012     02/02/2012 -0.009841         0  1.255711   \n",
      "1   CIEL3   01/02/2016     02/02/2016 -0.095911         0  0.636058   \n",
      "2  KLBN11   01/02/2018     02/02/2018  0.071078         1  0.332900   \n",
      "3   KLBN4   01/02/2018     02/02/2018  0.008948         1  0.360474   \n",
      "4   MDIA3   01/03/2011     02/03/2011  0.112207         1  0.390996   \n",
      "\n",
      "   EstimationLen     FundSource        Data       Empresa  ...  \\\n",
      "0            504   FIBR3.SA.csv  31/12/2011        FIBRIA  ...   \n",
      "1            504   CIEL3.SA.csv  31/12/2015         CIELO  ...   \n",
      "2            504  KLBN11.SA.csv  31/12/2017    KLABIN S/A  ...   \n",
      "3            504   KLBN4.SA.csv  31/12/2017    KLABIN S/A  ...   \n",
      "4            287   MDIA3.SA.csv  30/09/2010  M.DIASBRANCO  ...   \n",
      "\n",
      "          PNC_Y_Change Outros_PC_MET   Outros_PC_Q_Change  \\\n",
      "0  -178.14300000000003       272.407  -128.07500000000005   \n",
      "1              7662.15      1060.937    546.0779999999999   \n",
      "2   1498.3280000000013       226.682   11.492999999999995   \n",
      "3   1498.3280000000013       226.682   11.492999999999995   \n",
      "4                  NaN           NaN                  NaN   \n",
      "\n",
      "    Outros_PC_Y_Change   LUB_MET         LUB_Q_Change        LUB_Y_Change  \\\n",
      "0  -1590.7040000000002   730.031  -125.42400000000009  -858.6970000000001   \n",
      "1   350.07299999999987   5811.63    264.1940000000004  1136.6720000000005   \n",
      "2             -132.662  2735.547    256.7930000000001   338.8609999999999   \n",
      "3             -132.662  2735.547    256.7930000000001   338.8609999999999   \n",
      "4                  NaN   998.504   22.577999999999975                 NaN   \n",
      "\n",
      "    EPS_EarningsSurprise EPS_Earnings_Surprise_Backward_Diff  \\\n",
      "0        -1.115097084791                  1.9158970830680002   \n",
      "1    0.04957619537000002             -0.00027594294999988556   \n",
      "2   -0.21599389196399998                     -0.608146466232   \n",
      "3   -0.04319877839299999                -0.12162929324669998   \n",
      "4  0.0019875616100000215                 0.20514133764999976   \n",
      "\n",
      "  EPS_Earnings_Surprise_Backward_Ave_Diff  \n",
      "0                     -1.6963506494473335  \n",
      "1                    -0.02077505966999997  \n",
      "2                    -0.39398098214733335  \n",
      "3                    -0.07879619642966668  \n",
      "4                                     NaN  \n",
      "\n",
      "[5 rows x 116 columns]\n"
     ]
    }
   ],
   "source": [
    "# Execução principal (gera arquivos finais)\n",
    "# =========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    # 0) Pastas base\n",
    "    ensure_dirs([\n",
    "        \"dataset/final\",\n",
    "        \"dataset/prices_processed\",\n",
    "        \"dataset/fund_processed\",\n",
    "        \"dataset/prices\",\n",
    "    ])\n",
    "\n",
    "    # 1) Carrega IBOV e CDI\n",
    "    mkt = MarketAndRiskLoader.load_ibov_csv(\"dataset/prices/IBOV.SA.csv\")  # -> Date, Close\n",
    "    rf  = MarketAndRiskLoader.load_cdi_csv(\"dataset/prices/CDI.SA.csv\")    # -> Date, rf_daily\n",
    "\n",
    "    # 2) Builder de eventos CAR/Beta\n",
    "    builder = EventDatasetBuilder(\n",
    "        mkt_df=mkt,\n",
    "        rf_df=rf,\n",
    "        estimation_window=504,  # ~2 anos úteis\n",
    "        holding_days=30,        # CAR de 30 dias úteis\n",
    "        min_estimation=60       # exige pelo menos ~3 meses de histórico antes do evento\n",
    "    )\n",
    "\n",
    "    all_events = []\n",
    "\n",
    "    # 3) Loop pelos tickers já processados\n",
    "    prices_dir = Path(\"dataset/prices_processed\")\n",
    "    fund_dir   = Path(\"dataset/fund_processed\")\n",
    "    final_dir  = Path(\"dataset/final\")\n",
    "\n",
    "    for price_file in prices_dir.glob(\"*SA.csv\"):\n",
    "        # Inferir ticker base: \"PETR4.SA.csv\" -> \"PETR4\"\n",
    "        base_name = price_file.stem      # \"PETR4.SA\"\n",
    "        tkr = base_name.split(\".\")[0]    # \"PETR4\"\n",
    "\n",
    "        fund_file = fund_dir / f\"{tkr}.SA.csv\"\n",
    "        if not fund_file.exists():\n",
    "            # se não tem fundamentals pra esse ticker, pula\n",
    "            continue\n",
    "\n",
    "        ev_df = builder.build_for_ticker(\n",
    "            tkr=tkr,\n",
    "            price_csv=str(price_file),\n",
    "            fund_csv=str(fund_file),\n",
    "           \n",
    "        )\n",
    "\n",
    "        if ev_df is None or ev_df.empty:\n",
    "            continue\n",
    "\n",
    "        # salva arquivo final por ticker:\n",
    "        # dataset/final/{tkr}_pead_event.csv\n",
    "        out_tkr_path = final_dir / f\"{tkr}_pead_event.csv\"\n",
    "        ev_df.to_csv(out_tkr_path, index=False)\n",
    "\n",
    "        all_events.append(ev_df)\n",
    "\n",
    "    # 4) Consolidado geral\n",
    "    if all_events:\n",
    "        events_df = pd.concat(all_events, ignore_index=True)\n",
    "\n",
    "        # Ordenar também o consolidado por AnnounceDate\n",
    "        if \"AnnounceDate\" in events_df.columns:\n",
    "            events_df = (\n",
    "                events_df.sort_values([\"AnnounceDate\",\"Ticker\"])\n",
    "                         .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "        # salva dataset consolidado\n",
    "        out_all_path = final_dir / \"pead_event_dataset_2010_2019.csv\"\n",
    "        events_df.to_csv(out_all_path, index=False)\n",
    "\n",
    "        print(\"OK — dataset consolidado salvo em dataset/final/pead_event_dataset_2010_2019.csv\")\n",
    "        print(events_df.head())\n",
    "    else:\n",
    "        print(\"Nenhum evento encontrado. Verifique se 'AnnounceDate' está presente em fund_processed/*.csv e se há preços válidos em prices_processed/*.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-ml)",
   "language": "python",
   "name": "fin-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
