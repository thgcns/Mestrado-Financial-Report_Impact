{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "038fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "# =====================\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import enum\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import ta\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a77ea562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta 'dataset' foi criada.\n",
      "Pasta 'dataset/prices' foi criada.\n",
      "Pasta 'dataset/prices_processed' foi criada.\n",
      "Pasta 'dataset/fundamental' foi criada.\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURAÃ‡Ã•ES GERAIS\n",
    "# =====================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "userName =\"aluno.thiago.nunes\" # os.getenv(\"USERNAME\")\n",
    "password = \"NLPfinance2@23\" #os.getenv(\"PASSWORD\")\n",
    "\n",
    "CD_USERNAME = os.getenv(\"COMD_USER\", \"aluno.thiago.nunes\")\n",
    "CD_PASSWORD = os.getenv(\"COMD_PASS\", \"NLPfinance2@23\")\n",
    "\n",
    "\n",
    "# Proxy de mercado e risk-free\n",
    "MARKET_PROXY = \"^BVSP\"  # alternativas: \"BOVA11.SA\" (se preferir ETF)\n",
    "RISK_FREE_SERIES = \"CDI\"      # CDI diÃ¡rio como proxy (pode trocar para SELIC diÃ¡ria se preferir)\n",
    "\n",
    "# Janela de estimaÃ§Ã£o e holding\n",
    "ESTIMATION_WINDOW = 252  # ~1 ano de pregÃµes\n",
    "HOLDING_DAYS = 30         # janela pÃ³s-earnings (CAR)\n",
    "\n",
    "# Universo de teste (exemplos; substitua pelos seus)\n",
    "#B3_TICKERS = [\n",
    "#    \"ABEV3.SA\", \"ITUB4.SA\", \"PETR4.SA\", \"VALE3.SA\", \"BBDC4.SA\",\n",
    "#]\n",
    "\n",
    "startDate = \"01012010\"\n",
    "endDate   = \"31122019\"\n",
    "\n",
    "\n",
    "# PASTAS\n",
    "# =====================\n",
    "folders = [\"dataset\", \"dataset/prices\", \"dataset/prices_processed\", \"dataset/fundamental\"]\n",
    "\n",
    "# Verifica se as pastas existem, se nÃ£o, cria-as\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Pasta '{folder}' foi criada.\")\n",
    "    else:\n",
    "        print(f\"Pasta '{folder}' jÃ¡ existe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "426870c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['IBOV', 'CDI']\n",
    "empresas = [\n",
    "    # Empresas com papÃ©is mais lÃ­quidos (ON, PN ou Unit)\n",
    "    \"AEDU3\", \"ABEV3\", \"ALLL11\", \"ALLL3\",\n",
    "    \"ALPA4\", \"ALSC3\", \"ALUP11\", \"AMBV3\",\n",
    "    \"AMBV4\", \"AMIL3\", \"ANIM3\", \"ARTR3\",\n",
    "    \"ARZZ3\", \"AZUL4\", \"B3SA3\", \"BBAS3\",\n",
    "    \"BBDC3\", \"BBDC4\", \"BBRK3\", \"BBSE3\",\n",
    "    \"BEEF3\", \"BIDI11\", \"BISA3\", \"BPAC11\",\n",
    "    \"BPNM4\", \"BRAP4\", \"BRDT3\", \"BRFS3\",\n",
    "    \"BRKM5\", \"BRML3\", \"BRPR3\", \"BRSR6\",\n",
    "    \"BRTO4\", \"BTOW3\", \"BVMF3\", \"CCRO3\",\n",
    "    \"CCXC3\", \"CESP6\", \"CIEL3\", \"CMIG3\",\n",
    "    \"CMIG4\", \"CNFB4\", \"CPFE3\", \"CPLE6\",\n",
    "    \"CRFB3\", \"CRUZ3\", \"CSAN3\", \"CSMG3\",\n",
    "    \"CSNA3\", \"CTIP3\", \"CVCB3\", \"CYRE3\",\n",
    "    \"DASA3\", \"DTEX3\", \"ECOD3\", \"ECOR3\",\n",
    "    \"EGIE3\", \"ELET3\", \"ELET6\", \"ELPL4\",\n",
    "    \"ELPL6\", \"EMBR3\", \"ENAT3\", \"ENBR3\",\n",
    "    \"ENEV3\", \"ENGI11\", \"EQTL3\", \"ESTC3\",\n",
    "    \"EVEN3\", \"EZTC3\", \"FFTL4\", \"FIBR3\",\n",
    "    \"FLRY3\", \"GETI4\", \"GFSA3\", \"GGBR3\",\n",
    "    \"GGBR4\", \"GNDI3\", \"GOAU4\", \"GOLL4\",\n",
    "    \"GRND3\", \"HAPV3\", \"HGTX3\", \"HRTP3\",\n",
    "    \"HYPE3\", \"IGTA3\", \"INPR3\", \"IRBR3\",\n",
    "    \"ITSA4\", \"ITUB3\", \"ITUB4\", \"JBSS3\",\n",
    "    \"JHSF3\", \"KEPL3\", \"KLBN11\", \"KLBN4\",\n",
    "    \"KROT3\", \"LAME3\", \"LAME4\", \"LCAM3\",\n",
    "    \"LEVE3\", \"LIGT3\", \"LINX3\", \"LLXL3\",\n",
    "    \"LREN3\", \"LUPA3\", \"MAGG3\", \"MDIA3\",\n",
    "    \"MGLU3\", \"MILS3\", \"MMXM3\", \"MPLU3\",\n",
    "    \"MPXE3\", \"MRFG3\", \"MRVE3\", \"MULT3\",\n",
    "    \"MYPK3\", \"NATU3\", \"NETC4\", \"ODPV3\",\n",
    "    \"OGXP3\", \"OIBR3\", \"OIBR4\", \"OSXB3\",\n",
    "    \"PCAR4\", \"PCAR5\", \"PDGR3\", \"PETR3\",\n",
    "    \"PETR4\", \"PLAS3\", \"PMAM3\", \"POMO4\",\n",
    "    \"POSI3\", \"PRML3\", \"PSSA3\", \"QGEP3\",\n",
    "    \"QUAL3\", \"RADL3\", \"RAIL3\", \"RAPT4\",\n",
    "    \"RDCD3\", \"RENT3\", \"RLOG3\", \"RPMG3\",\n",
    "    \"RSID3\", \"RUMO3\", \"SANB11\", \"SAPR11\",\n",
    "    \"SAPR4\", \"SBSP3\", \"SEER3\", \"SLCE3\",\n",
    "    \"SMLE3\", \"SMLS3\", \"SMTO3\", \"SULA11\",\n",
    "    \"SUZB3\", \"SUZB5\", \"TAEE11\", \"TAMM4\",\n",
    "    \"TBLE3\", \"TCSA3\", \"TCSL3\", \"TCSL4\",\n",
    "    \"TEND3\", \"TERI3\", \"TIET11\", \"TIMP3\",\n",
    "    \"TLPP4\", \"TMAR5\", \"TNLP3\", \"TNLP4\",\n",
    "    \"TOTS3\", \"TRPL4\", \"TUPY3\", \"UGPA3\",\n",
    "    \"UGPA4\", \"USIM3\", \"USIM5\", \"VAGR3\",\n",
    "    \"VALE3\", \"VALE5\", \"VIVO4\", \"VIVT4\",\n",
    "    \"VLID3\", \"VVAR11\", \"VVAR3\", \"WEGE3\",\n",
    "    \"WIZS3\", \"YDUQ3\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c1c9e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÃ‡Ã•ES CAPTURA VALORES\n",
    "def GetHistoricalPriceComdinheiro(ticker, startDate, endDate, userName, password, path2save):\n",
    "    url = \"https://www.comdinheiro.com.br/Clientes/API/EndPoint001.php\"\n",
    "    querystring = {\"code\": \"import_data\"}\n",
    "\n",
    "    internal_url = f\"HistoricoCotacaoAcao001-{ticker}-{startDate}-{endDate}-1-1\"\n",
    "    payload = f\"username={userName}&password={password}&URL={internal_url}&format=json3\"\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "\n",
    "    response = requests.post(url, data=payload, headers=headers, params=querystring)\n",
    "    data = response.json()\n",
    "\n",
    "    # --- ValidaÃ§Ã£o: se nÃ£o houver dados ---\n",
    "    \n",
    "    if not isinstance(data, dict) or \"tables\" not in data or \"tab0\" not in data[\"tables\"]:\n",
    "        print(f\"âš ï¸ Sem dados disponÃ­veis para {ticker} entre {startDate} e {endDate}\")\n",
    "        return pd.DataFrame()    \n",
    " \n",
    "    df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "    df = df.drop(\"lin0\", errors='ignore')\n",
    "\n",
    "    df.columns = [\n",
    "        \"Data\", \"FechAjust\", \"Var\", \"FechHist\", \"AbertAjust\",\n",
    "        \"MinAjust\", \"MedAjust\", \"MaxAjust\", \"Vol\", \"Neg\", \"Fator\", \"Tipo\", \"COL_A\", \"COL_B\"\n",
    "    ]\n",
    "\n",
    "    # Converte data\n",
    "    df[\"Data\"] = pd.to_datetime(df[\"Data\"], format=\"%d/%m/%Y\", errors='coerce')\n",
    "\n",
    "    # Substitui 'nd' por NaN\n",
    "    df.replace(\"nd\", pd.NA, inplace=True)\n",
    "\n",
    "    # Converte colunas numÃ©ricas\n",
    "    colunas_numericas = [\n",
    "        \"FechAjust\", \"Var\", \"FechHist\", \"AbertAjust\",\n",
    "        \"MinAjust\", \"MedAjust\", \"MaxAjust\", \"Vol\", \"Neg\", \"Fator\"\n",
    "    ]\n",
    "\n",
    "    for col in colunas_numericas:\n",
    "        df[col] = df[col].str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df = df.sort_values(\"Data\").reset_index(drop=True)\n",
    "    df = df.drop(columns=[ \"COL_A\", \"COL_B\"])\n",
    "    \n",
    "    if path2save != '':\n",
    "     df.to_csv(path2save, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Captura de Eventos + Indicadores Fundamentalistas\n",
    "def EventsDate(ticker, userName, password, startDate, endDate, path2save):\n",
    "    url = \"https://www.comdinheiro.com.br/Clientes/API/EndPoint001.php\"\n",
    "    querystring = {\"code\": \"import_data\"}\n",
    "    \n",
    "    # Payload atualizado com todos os indicadores\n",
    "    payload = (\n",
    "        f\"username={userName}&password={password}\"\n",
    "        f\"&URL=HistoricoIndicadoresFundamentalistas001.php%3F\"\n",
    "        f\"%26data_ini%3D{startDate}\"\n",
    "        f\"%26data_fim%3D{endDate}\"\n",
    "        f\"%26trailing%3D12\"\n",
    "        f\"%26conv%3DMIXED\"\n",
    "        f\"%26moeda%3DMOEDA_ORIGINAL\"\n",
    "        f\"%26c_c%3Dconsolidado\"\n",
    "        f\"%26m_m%3D1000000\"\n",
    "        f\"%26n_c%3D2\"\n",
    "        f\"%26f_v%3D1\"\n",
    "        f\"%26papel%3D{ticker}\"\n",
    "        f\"%26indic%3DNOME_EMPRESA%2BRL%2BLL%2BEBITDA%2BDATA_PUBLICACAO\"\n",
    "        f\"%2BPRECO_ABERTURA%2BPRECO_FECHAMENTO%2BLPA%2BROA%2BROE%2BMEB\"\n",
    "        f\"%2BRL%2BCRESC_RL_12M%2BCRESC_LL_12M%2BCRESC_EBITDA_12M%2BCAPEX\"\n",
    "        f\"%2BRL%2BFCO%2BEBITDA%2BFCF%2BDIVIDA_LIQUIDA%2BPL%2BDIVIDA_BRUTA\"\n",
    "        f\"%2BAT%2BDVA_DESPESAS_FIN%2BPC%2BPNC%2BOUTROS_PC%2BLUB\"\n",
    "        f\"%26periodicidade%3Dtri\"\n",
    "        f\"%26graf_tab%3Dtabela\"\n",
    "        f\"%26desloc_data_analise%3D1\"\n",
    "        f\"%26flag_transpor%3D0\"\n",
    "        f\"%26c_d%3Dd\"\n",
    "        f\"%26enviar_email%3D0\"\n",
    "        f\"%26enviar_email_log%3D0\"\n",
    "        f\"%26cabecalho_excel%3Dmodo1\"\n",
    "        f\"%26relat_alias_automatico%3Dcmd_alias_01\"\n",
    "        \"&format=json3\"\n",
    "    )\n",
    "    \n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    response = requests.post(url, data=payload, headers=headers, params=querystring)\n",
    "    data = json.loads(response.text)\n",
    "    \n",
    "    # --- ValidaÃ§Ã£o: se nÃ£o houver dados ---\n",
    "    if not isinstance(data, dict) or \"tables\" not in data or \"tab0\" not in data[\"tables\"]:\n",
    "        print(f\"âš ï¸ Sem dados disponÃ­veis para {ticker} entre {startDate} e {endDate}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # ConstrÃ³i DataFrame\n",
    "    df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "    \n",
    "    # Colunas de acordo com a ordem dos indicadores\n",
    "    novas_colunas = [\n",
    "        \"Data\", \"Empresa\", \"RL\", \"LL\", \"EBITDA\", \"Data_Publicacao\",\n",
    "        \"Preco_Abertura\", \"Preco_Fechamento\", \"LPA\", \"ROA\", \"ROE\", \"MEB\",\n",
    "        \"RL_dup1\", \"CRESC_RL_12M\", \"CRESC_LL_12M\", \"CRESC_EBITDA_12M\",\n",
    "        \"CAPEX\", \"RL_dup2\", \"FCO\", \"EBITDA_dup\", \"FCF\",\n",
    "        \"Divida_Liquida\", \"PL\", \"Divida_Bruta\", \"AT\", \"DVA_Despesas_Fin\",\n",
    "        \"PC\", \"PNC\", \"Outros_PC\", \"LUB\",\n",
    "        \"Consolidado\", \"Convencao\", \"Moeda\", \"Data_Demonstracao\",\n",
    "        \"Meses\", \"Data_Analise\"\n",
    "    ]\n",
    "    df.columns = novas_colunas\n",
    "    \n",
    "    # Remove linha de cabeÃ§alho interno\n",
    "    df = df.drop(\"lin0\", errors=\"ignore\")\n",
    "    \n",
    "    # Normaliza datas\n",
    "    df['Data_Publicacao'] = pd.to_datetime(df['Data_Publicacao'], errors='coerce', format='%d/%m/%Y')\n",
    "    df['Data_Publicacao'] = df['Data_Publicacao'].dt.strftime('%Y-%m-%d')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Consolida duplicados (mantÃ©m o primeiro vÃ¡lido)\n",
    "    if {\"RL\", \"RL_dup1\", \"RL_dup2\"}.issubset(df.columns):\n",
    "        df[\"RL\"] = df[[\"RL\", \"RL_dup1\", \"RL_dup2\"]].bfill(axis=1).iloc[:, 0]\n",
    "        df = df.drop(columns=[\"RL_dup1\", \"RL_dup2\"])\n",
    "    if {\"EBITDA\", \"EBITDA_dup\"}.issubset(df.columns):\n",
    "        df[\"EBITDA\"] = df[[\"EBITDA\", \"EBITDA_dup\"]].bfill(axis=1).iloc[:, 0]\n",
    "        df = df.drop(columns=[\"EBITDA_dup\"])\n",
    "    \n",
    "    if path2save != '':\n",
    "        df.to_csv(path2save, index=False)        \n",
    "    \n",
    "    return df\n",
    "\n",
    "##### COLETA DE TICKERS DO IBOV\n",
    "\n",
    "def GetIbovComposition(userName, password, startYear=2010, endYear=2019):\n",
    "    url = \"https://api.comdinheiro.com.br/v1/ep1/import-data\"\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    \n",
    "    all_dfs = []\n",
    "    \n",
    "    for year in range(startYear, endYear + 1):\n",
    "        data_analise = f\"2209{year}\"  # 31/12/AAAA\n",
    "        \n",
    "        inner_url = (\n",
    "            f\"ComposicaoIndices001.php?\"\n",
    "            f\"data_analise={data_analise}\"\n",
    "            f\"&indice=IBOV\"\n",
    "            f\"&nome_portfolio=\"\n",
    "            f\"&tipo_portfolio=\"\n",
    "            f\"&overwrite=0\"\n",
    "            f\"&design=2\"\n",
    "            f\"&obs_portfolio=0\"\n",
    "            f\"&num_casas=0\"\n",
    "            f\"&salvar_dados=nenhum\"\n",
    "            f\"&sufixo=\"\n",
    "            f\"&nome_serie=\"\n",
    "            f\"&filtro_avancado=\"\n",
    "        )\n",
    "        \n",
    "        payload = f\"username={userName}&password={password}&URL={inner_url}&format=json3\"\n",
    "        \n",
    "        response = requests.post(url, data=payload, headers=headers)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not isinstance(data, dict) or \"tables\" not in data or \"tab0\" not in data[\"tables\"]:\n",
    "            print(f\"âš ï¸ Sem dados para {data_analise}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "        df = df.drop(\"lin0\", errors=\"ignore\").reset_index(drop=True)\n",
    "        df[\"Data_Analise\"] = pd.to_datetime(data_analise, format=\"%d%m%Y\")\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Exemplo de chamada\n",
    "#df_ibov = GetIbovComposition(\n",
    "#    userName=userName,\n",
    "#    password=password,\n",
    "#    startYear=2010,\n",
    "#    endYear=2019\n",
    "#)\n",
    "#\n",
    "#print(df_ibov.head())\n",
    "#df_ibov.to_csv(\"dataset/IBOV_Composicao_2010_2019.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f4eb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWN\n",
    "for emp in empresas:\n",
    "  GetHistoricalPriceComdinheiro(ticker=emp, startDate=startDate, endDate=endDate, userName=userName, password=password, path2save=f\"dataset/prices/{emp}.SA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "83249b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('dataset/prices')\n",
    "for emp in files:  \n",
    "  EventsDate(ticker= emp[0:-7], userName=userName, password=password, startDate=startDate, endDate=endDate, path2save=f\"dataset/fundamental/{emp}\")\n",
    "  os.listdir('dataset/fundamental')\n",
    "#  price_p.insert(1, 'event', price_p['Data'].apply(lambda date: 1 if date in date_df['Data_Publicacao'].values else 0))\n",
    "#  price_p.to_csv(f\"dataset/prices_processed/{emp}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "dataprep_pipeline.py\n",
    "\n",
    "PrÃ©-processamento de:\n",
    "  - PreÃ§os: dataset/prices/TICKER.SA.csv\n",
    "  - Fundamentos (tri): dataset/fundamental/TICKER_fUND.csv\n",
    "\n",
    "SaÃ­das:\n",
    "  - dataset/prices_processed/TICKER.SA.csv (preÃ§o com retornos/indicadores)\n",
    "  - dataset/final/TICKER.final.csv (preÃ§o + flag de evento + fundamentos asof)\n",
    "  - dataset/final/final_dataprep.csv (consolidado de todos os tickers)\n",
    "\n",
    "Requisitos: pandas, numpy\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "# -------------------------\n",
    "# Utils de I/O e diretÃ³rios\n",
    "# -------------------------\n",
    "DEFAULT_DIRS = [\"dataset\", \"dataset/prices\", \"dataset/prices_processed\",\n",
    "                \"dataset/fundamental\", \"dataset/final\"]\n",
    "\n",
    "def ensure_dirs(paths: List[str] = DEFAULT_DIRS) -> None:\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Utils de parsing numÃ©rico\n",
    "# -------------------------\n",
    "def to_float_smart(x) -> float:\n",
    "    \"\"\"\n",
    "    Converte string numÃ©rica brasileira/heterogÃªnea para float.\n",
    "    Regras:\n",
    "    - 'nd'/'': NaN\n",
    "    - se tem vÃ­rgula -> assume vÃ­rgula decimal: remove pontos (milhar) e troca vÃ­rgula por ponto\n",
    "    - senÃ£o, se tem >1 ponto -> mantÃ©m apenas o Ãºltimo ponto como decimal\n",
    "    - senÃ£o, tenta float direto\n",
    "    \"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nd\", \"nan\", \"none\"}:\n",
    "        return np.nan\n",
    "    if \",\" in s:\n",
    "        s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try:\n",
    "            return float(s)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    # nÃºmeros com muitos pontos (milhar com ponto e decimal com ponto)\n",
    "    if s.count(\".\") > 1:\n",
    "        parts = s.split(\".\")\n",
    "        s = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "    # remove qualquer lixo exceto dÃ­gitos, sinal e ponto\n",
    "    s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def to_int_smart(x) -> float:\n",
    "    \"\"\"Converte para inteiro removendo tudo que nÃ£o for dÃ­gito/sinal.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    s = re.sub(r\"[^0-9\\-]\", \"\", str(x))\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# -------------------------\n",
    "# Indicador discreto (âˆ’1/0/+1)\n",
    "# -------------------------\n",
    "def catalog_return(row, x, name_return):\n",
    "    try:\n",
    "        val = float(row[name_return])\n",
    "        std = float(row.get(f\"Rolling_std_{name_return}\", np.nan))\n",
    "    except Exception:\n",
    "        return 0\n",
    "    if np.isnan(val) or np.isnan(std) or std == 0:\n",
    "        return 0\n",
    "    if val > x * std:   return 1\n",
    "    if val < -x * std:  return -1\n",
    "    return 0\n",
    "\n",
    "# -------------------------\n",
    "# Classe: PreÃ§os (expansÃ£o da sua DataProcessing)\n",
    "# -------------------------\n",
    "class PriceProcessing:\n",
    "    def __init__(self, df_prices: pd.DataFrame, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.df = df_prices.copy()\n",
    "        # detecÃ§Ã£o de separador automÃ¡tica + leitura como string recomendÃ¡vel\n",
    "        # (se vier pronto, apenas normaliza)\n",
    "        # NormalizaÃ§Ã£o de colunas esperadas\n",
    "        # Esperado: Data, FechAjust, Var, FechHist, AbertAjust, MinAjust, MedAjust, MaxAjust, Vol, Neg, Fator, Tipo\n",
    "        # 1) Data\n",
    "        self.df[\"Data\"] = pd.to_datetime(self.df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "        # 2) PreÃ§os (float)\n",
    "        float_cols = [\"FechAjust\", \"FechHist\", \"AbertAjust\", \"MinAjust\", \"MedAjust\", \"MaxAjust\", \"Var\", \"Fator\", \"Vol\"]\n",
    "        for c in float_cols:\n",
    "            if c in self.df.columns:\n",
    "                self.df[c] = self.df[c].apply(to_float_smart)\n",
    "        # 3) Volume/NegÃ³cios (int)\n",
    "        for c in [\"Neg\"]:\n",
    "            if c in self.df.columns:\n",
    "                self.df[c] = self.df[c].apply(to_int_smart)\n",
    "        # 4) Ordena\n",
    "        self.df = self.df.sort_values(\"Data\").drop_duplicates(\"Data\").reset_index(drop=True)\n",
    "        # 5) Fechamento preferido (ajustado se existir)\n",
    "        if \"FechAjust\" in self.df.columns and self.df[\"FechAjust\"].notna().any():\n",
    "            self.df[\"Close\"] = self.df[\"FechAjust\"]\n",
    "        else:\n",
    "            if \"AbertAjust\" in self.df.columns:\n",
    "                self.df[\"Close\"] = self.df[\"AbertAjust\"].shift(-1)\n",
    "            else:\n",
    "                self.df[\"Close\"] = np.nan            \n",
    "\n",
    "        # 6) Campo auxiliar ponderado\n",
    "        if \"Vol\" in self.df.columns and self.df[\"Vol\"].notna().any():\n",
    "            vol_mean = self.df[\"Vol\"].replace(0, np.nan).mean()\n",
    "            self.df[\"FechPonderado\"] = self.df[\"Close\"] * self.df[\"Vol\"] / (vol_mean if vol_mean else self.df[\"Vol\"])\n",
    "        else:\n",
    "            self.df[\"FechPonderado\"] = np.nan\n",
    "\n",
    "    def create_return_by_period(self, name_return: str, period: int, column_name: str = \"Close\", remove_nan=False):\n",
    "        self.df[name_return] = np.log(self.df[column_name] / self.df[column_name].shift(period))\n",
    "        if remove_nan:\n",
    "            self.df.dropna(subset=[name_return], inplace=True)\n",
    "\n",
    "    def create_cumulative_std(self, name_return: str):\n",
    "        self.df[f\"Cumulative_std_{name_return}\"] = self.df[name_return].expanding().std()\n",
    "        \n",
    "    def create_rolling_std(self, name_return: str, window: int = 20):\n",
    "        \"\"\"\n",
    "        Calcula o desvio padrÃ£o em uma janela mÃ³vel.\n",
    "        Ex.: window=20 â†’ volatilidade de 20 perÃ­odos.\n",
    "        \"\"\"\n",
    "        self.df[f\"Rolling_std_{name_return}_{window}\"] = (\n",
    "            self.df[name_return].rolling(window=window).std()\n",
    "    )\n",
    "\n",
    "    def create_indicator(self, name_return: str, factor: float = 0.1):\n",
    "        self.df[f\"Indicator_{name_return}\"] = self.df.apply(lambda row: catalog_return(row, factor, name_return), axis=1)\n",
    "\n",
    "    def finalize(self) -> pd.DataFrame:\n",
    "        # garante colunas essenciais\n",
    "        cols = [\"Data\", \"Close\", \"FechPonderado\", \"Vol\", \"Neg\", \"Var\"]\n",
    "        cols += [c for c in self.df.columns if c.startswith(\"Daily_\") or c.startswith(\"Week_\") or c.startswith(\"Month_\")]\n",
    "        cols += [c for c in self.df.columns if c.startswith(\"Rolling_std_\") or c.startswith(\"Indicator_\")]\n",
    "        cols = [c for c in cols if c in self.df.columns]\n",
    "        out = self.df[cols].copy()\n",
    "        out.insert(0, \"Ticker\", self.ticker)\n",
    "        return out\n",
    "\n",
    "# -------------------------\n",
    "# Classe: Fundamentos (tri)\n",
    "# -------------------------\n",
    "class FundamentalProcessing:\n",
    "    def __init__(self, df_fund: pd.DataFrame, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.df = df_fund.copy()\n",
    "\n",
    "        # Normaliza datas\n",
    "        # Data_Publicacao (evento)\n",
    "        if \"Data_Publicacao\" in self.df.columns:\n",
    "            self.df[\"Data_Publicacao\"] = pd.to_datetime(self.df[\"Data_Publicacao\"], dayfirst=True, errors=\"coerce\")\n",
    "        # Data_Demonstracao / Data_Analise\n",
    "        if \"Data_Demonstracao\" in self.df.columns:\n",
    "            self.df[\"Data_Demonstracao\"] = pd.to_datetime(self.df[\"Data_Demonstracao\"], dayfirst=True, errors=\"coerce\")\n",
    "        if \"Data_Analise\" in self.df.columns:\n",
    "            self.df[\"Data_Analise\"] = pd.to_datetime(self.df[\"Data_Analise\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "        # Converte colunas numÃ©ricas principais\n",
    "        num_cols = [\n",
    "            \"RL\",\"LL\",\"EBITDA\",\"Preco_Abertura\",\"Preco_Fechamento\",\"LPA\",\"ROA\",\"ROE\",\"MEB\",\n",
    "            \"CRESC_RL_12M\",\"CRESC_LL_12M\",\"CRESC_EBITDA_12M\",\"CAPEX\",\"FCO\",\"FCF\",\"Divida_Liquida\",\n",
    "            \"PL\",\"Divida_Bruta\",\"AT\",\"DVA_Despesas_Fin\",\"PC\",\"PNC\",\"Outros_PC\",\"LUB\"\n",
    "        ]\n",
    "        for c in num_cols:\n",
    "            if c in self.df.columns:\n",
    "                self.df[c] = self.df[c].apply(to_float_smart)\n",
    "\n",
    "        # Duplicatas (quando existirem)\n",
    "        for dup_base in [(\"RL\",\"RL_dup1\",\"RL_dup2\"), (\"EBITDA\",\"EBITDA_dup\")]:\n",
    "            keep = dup_base[0]\n",
    "            alts = [c for c in dup_base[1:] if c in self.df.columns]\n",
    "            if keep in self.df.columns and alts:\n",
    "                self.df[keep] = self.df[[keep] + alts].bfill(axis=1).iloc[:,0]\n",
    "                self.df.drop(columns=[c for c in alts if c in self.df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        # Define QuarterEnd (prioridade Data_Demonstracao > Data_Analise)\n",
    "        self.df[\"QuarterEnd\"] = self.df.get(\"Data_Demonstracao\", pd.NaT)\n",
    "        if \"QuarterEnd\" in self.df.columns and self.df[\"QuarterEnd\"].isna().all() and \"Data_Analise\" in self.df.columns:\n",
    "            self.df[\"QuarterEnd\"] = self.df[\"Data_Analise\"]\n",
    "\n",
    "        # Ordena e dedup\n",
    "        self.df = self.df.sort_values([\"QuarterEnd\", \"Data_Publicacao\"]).drop_duplicates([\"QuarterEnd\"]).reset_index(drop=True)\n",
    "\n",
    "    def get_publication_dates(self) -> pd.Series:\n",
    "        \"\"\"Retorna as datas de publicaÃ§Ã£o (eventos).\"\"\"\n",
    "        return self.df[\"Data_Publicacao\"].dropna().astype(\"datetime64[ns]\")\n",
    "\n",
    "    def features_for_asof_merge(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Seleciona colunas numÃ©ricas + QuarterEnd + Data_Publicacao para merge_asof no preÃ§o.\n",
    "        \"\"\"\n",
    "        keep = [\"QuarterEnd\", \"Data_Publicacao\"]\n",
    "        num_cols = [c for c in self.df.columns if c not in keep and pd.api.types.is_numeric_dtype(self.df[c])]\n",
    "        out = self.df[keep + num_cols].dropna(subset=[\"Data_Publicacao\"]).sort_values(\"Data_Publicacao\").reset_index(drop=True)\n",
    "        # renomeia colunas para evitar colisÃ£o de nomes\n",
    "        rename_map = {c: f\"F_{c}\" for c in num_cols}\n",
    "        out = out.rename(columns=rename_map)\n",
    "        return out\n",
    "\n",
    "# -------------------------\n",
    "# Classe: Merge & OrquestraÃ§Ã£o\n",
    "# -------------------------\n",
    "class DataPrepPipeline:\n",
    "    def __init__(self,\n",
    "                 prices_dir: str = \"dataset/prices\",\n",
    "                 fund_dir: str = \"dataset/fundamental\",\n",
    "                 out_prices_dir: str = \"dataset/prices_processed\",\n",
    "                 out_final_dir: str = \"dataset/final\"):\n",
    "        self.prices_dir = prices_dir\n",
    "        self.fund_dir = fund_dir\n",
    "        self.out_prices_dir = out_prices_dir\n",
    "        self.out_final_dir = out_final_dir\n",
    "        ensure_dirs([prices_dir, fund_dir, out_prices_dir, out_final_dir])\n",
    "\n",
    "    @staticmethod\n",
    "    def _ticker_from_price_filename(fname: str) -> str:\n",
    "        # \"AZUL4.SA.csv\" -> \"AZUL4\"\n",
    "        base = os.path.basename(fname)\n",
    "        if base.endswith(\".csv\"):\n",
    "            base = base[:-4]\n",
    "        return base.replace(\".SA\", \"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _fund_path_for_ticker(fund_dir: str, ticker: str) -> Optional[str]:\n",
    "        # procura \"<TICKER>_fUND.csv\" (case-insensitive)\n",
    "        for fn in os.listdir(fund_dir):\n",
    "            if fn.lower() == f\"{ticker.lower()}_fund.csv\" or fn.lower() == f\"{ticker.lower()}_fundamental.csv\" or fn.lower() == f\"{ticker.lower()}_fund.csv\":\n",
    "                return os.path.join(fund_dir, fn)\n",
    "            if fn.lower().startswith(ticker.lower()) and \"fund\" in fn.lower():\n",
    "                return os.path.join(fund_dir, fn)\n",
    "        return None\n",
    "\n",
    "    def process_one(self, price_csv_path: str,\n",
    "                    indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,\n",
    "                    attach_fundamentals_asof: bool = True,\n",
    "                    only_events: bool = False) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Processa um ticker:\n",
    "          - preÃ§o -> retornos/STD/indicadores\n",
    "          - evento -> flag 0/1 (Data âˆˆ Data_Publicacao)\n",
    "          - (opcional) merge_asof com fundamentos publicados\n",
    "          - salva CSV final\n",
    "        \"\"\"\n",
    "        # --- preÃ§os\n",
    "        dfp = pd.read_csv(price_csv_path, sep=None, engine=\"python\", dtype=str)\n",
    "        tkr = self._ticker_from_price_filename(price_csv_path)\n",
    "        price = PriceProcessing(dfp, tkr)\n",
    "        # retornos\n",
    "        price.create_return_by_period(\"Daily_Return\", 1, column_name=\"Close\", remove_nan=False)\n",
    "        price.create_return_by_period(\"Week_Return\", 5, column_name=\"Close\", remove_nan=False)\n",
    "        price.create_return_by_period(\"Month_Return\", 22, column_name=\"Close\", remove_nan=False)\n",
    "        # stds\n",
    "        price.create_rolling_std(\"Daily_Return\", window=21)\n",
    "        price.create_rolling_std(\"Week_Return\", window=65)\n",
    "        price.create_rolling_std(\"Month_Return\", window=252)\n",
    "        # indicadores discretos\n",
    "        price.create_indicator(\"Daily_Return\", indicator_factor)\n",
    "        price.create_indicator(\"Week_Return\", indicator_factor)\n",
    "        price.create_indicator(\"Month_Return\", indicator_factor)\n",
    "        df_price_feat = price.finalize()\n",
    "\n",
    "        # salva intermediÃ¡rio (compatÃ­vel com seu script atual)\n",
    "        if save_intermediate_prices:\n",
    "            out_p_path = os.path.join(self.out_prices_dir, os.path.basename(price_csv_path))\n",
    "            df_price_feat.to_csv(out_p_path, index=False)\n",
    "\n",
    "        # --- fundamentos (se houver)\n",
    "        fund_path = self._fund_path_for_ticker(self.fund_dir, tkr)\n",
    "        if fund_path is None:\n",
    "            # sem fundamentos -> apenas marca evento=0 e finaliza\n",
    "            df_final = df_price_feat.copy()\n",
    "            df_final[\"event\"] = 0\n",
    "        else:\n",
    "            dff_raw = pd.read_csv(fund_path, sep=None, engine=\"python\", dtype=str)\n",
    "            fund = FundamentalProcessing(dff_raw, tkr)\n",
    "            pub_dates = set(pd.to_datetime(fund.get_publication_dates(), errors=\"coerce\").dropna().values)\n",
    "            df_final = df_price_feat.copy()\n",
    "            # flag de evento\n",
    "            df_final[\"event\"] = df_final[\"Data\"].isin(pub_dates).astype(int)\n",
    "\n",
    "            # (opcional) merge_asof colando fundamentos atÃ© a prÃ³xima publicaÃ§Ã£o\n",
    "            if attach_fundamentals_asof:\n",
    "                f_asof = fund.features_for_asof_merge()\n",
    "                if not f_asof.empty:\n",
    "                    df_final = df_final.sort_values(\"Data\")\n",
    "                    f_asof = f_asof.sort_values(\"Data_Publicacao\")\n",
    "                    df_final = pd.merge_asof(\n",
    "                        df_final,\n",
    "                        f_asof,\n",
    "                        left_on=\"Data\",\n",
    "                        right_on=\"Data_Publicacao\",\n",
    "                        direction=\"backward\"\n",
    "                    )\n",
    "                    # remove coluna-Ã¢ncora para nÃ£o poluir\n",
    "                    if \"Data_Publicacao\" in df_final.columns:\n",
    "                        df_final.drop(columns=[\"Data_Publicacao\"], inplace=True)\n",
    "\n",
    "            if only_events:\n",
    "                df_final = df_final[df_final[\"event\"] == 1].copy()\n",
    "\n",
    "        # salva final por ticker\n",
    "        out_final_path = os.path.join(self.out_final_dir, f\"{tkr}.final.csv\")\n",
    "        df_final.to_csv(out_final_path, index=False)\n",
    "        return df_final\n",
    "\n",
    "    def process_all(self,\n",
    "                    indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,\n",
    "                    attach_fundamentals_asof: bool = True,\n",
    "                    only_events: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Processa todos os arquivos em dataset/prices e devolve consolidado.\"\"\"\n",
    "        all_final = []\n",
    "        for fn in os.listdir(self.prices_dir):\n",
    "            if not fn.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            try:\n",
    "                path = os.path.join(self.prices_dir, fn)\n",
    "                df_final = self.process_one(\n",
    "                    path,\n",
    "                    indicator_factor=indicator_factor,\n",
    "                    save_intermediate_prices=save_intermediate_prices,\n",
    "                    attach_fundamentals_asof=attach_fundamentals_asof,\n",
    "                    only_events=only_events\n",
    "                )\n",
    "                if df_final is not None and not df_final.empty:\n",
    "                    all_final.append(df_final.assign(Ticker=self._ticker_from_price_filename(fn)))\n",
    "            except Exception as ex:\n",
    "                print(f\"Erro no ticker de {fn}: {ex}\")\n",
    "                continue\n",
    "\n",
    "        if not all_final:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df_all = pd.concat(all_final, ignore_index=True)\n",
    "        df_all = df_all.sort_values([\"Ticker\", \"Data\"]).reset_index(drop=True)\n",
    "        # salva consolidado\n",
    "        df_all.to_csv(os.path.join(self.out_final_dir, \"final_dataprep.csv\"), index=False)\n",
    "        return df_all\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Exemplo de uso (script)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    ensure_dirs()\n",
    "\n",
    "    pipeline = DataPrepPipeline(\n",
    "        prices_dir=\"dataset/prices\",\n",
    "        fund_dir=\"dataset/fundamental\",\n",
    "        out_prices_dir=\"dataset/prices_processed\",\n",
    "        out_final_dir=\"dataset/final\"\n",
    "    )\n",
    "\n",
    "    # Processa todos os tickers:\n",
    "    # - inclui fundamentos por merge_asof (attach_fundamentals_asof=True)\n",
    "    # - filtra somente linhas de evento? -> only_events=True (opcional)\n",
    "    df_consolidado = pipeline.process_all(\n",
    "        indicator_factor=0.1,\n",
    "        save_intermediate_prices=True,\n",
    "        attach_fundamentals_asof=True,\n",
    "        only_events=False\n",
    "    )\n",
    "\n",
    "    print(\"OK! Arquivos salvos em dataset/prices_processed/ e dataset/final/\")\n",
    "    print(df_consolidado.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86153337",
   "metadata": {},
   "source": [
    "### Pipeline quebrado em modulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f0e4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1) Utilidades (pastas, parsing numÃ©rico)\n",
    "# ==========================\n",
    "# SeÃ§Ã£o 1 â€” Utils e Parsing\n",
    "# ==========================\n",
    "import os, re\n",
    "from typing import List, Optional, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DEFAULT_DIRS = [\"dataset\", \"dataset/prices\", \"dataset/prices_processed\",\n",
    "                \"dataset/fundamental\", \"dataset/final\"]\n",
    "\n",
    "def ensure_dirs(paths: List[str] = DEFAULT_DIRS) -> None:\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def to_float_smart(x) -> float:\n",
    "    \"\"\"Converte string BR (milhar com ponto, decimal com vÃ­rgula) e outros formatos para float.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nd\", \"nan\", \"none\"}: return np.nan\n",
    "    if \",\" in s:\n",
    "        s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    else:\n",
    "        s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n",
    "        if s.count(\".\") > 1:\n",
    "            parts = s.split(\".\")\n",
    "            s = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def to_int_smart(x) -> float:\n",
    "    \"\"\"Converte texto para inteiro removendo nÃ£o-dÃ­gitos.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan\n",
    "    s = re.sub(r\"[^0-9\\-]\", \"\", str(x))\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def winsorize_series(s: pd.Series, p: float = 0.01) -> pd.Series:\n",
    "    \"\"\"Winsoriza 1%/1% (default) â€” por sÃ©rie.\"\"\"\n",
    "    if s.notna().sum() < 5:\n",
    "        return s\n",
    "    lo, hi = np.nanpercentile(s, [p*100, (1-p)*100])\n",
    "    return s.clip(lo, hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366bfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Loader do Mercado (IBOV) e Risco-zero (CDI) a partir de CSV\n",
    "# ================================================\n",
    "# SeÃ§Ã£o 2 â€” Market/Risk Loader (IBOV e CDI locais)\n",
    "# ================================================\n",
    "class MarketAndRiskLoader:\n",
    "    @staticmethod\n",
    "    def load_ibov_csv(path_ibov: str) -> pd.DataFrame:\n",
    "        \"\"\"LÃª dataset/prices/IBOV.SA.csv â†’ DataFrame(Date, Close). Prioriza FechAjust.\"\"\"\n",
    "        df = pd.read_csv(path_ibov, dtype=str)\n",
    "        df[\"Data\"] = pd.to_datetime(df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "        for c in [\"FechAjust\",\"FechHist\"]:\n",
    "            if c in df.columns: df[c] = df[c].apply(to_float_smart)\n",
    "        close = \"FechAjust\" if \"FechAjust\" in df.columns and df[\"FechAjust\"].notna().any() else \"FechHist\"\n",
    "        out = df[[\"Data\", close]].rename(columns={\"Data\":\"Date\", close:\"Close\"})\n",
    "        return out.dropna(subset=[\"Date\",\"Close\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_cdi_csv(path_cdi: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        LÃª dataset/prices/CDI.SA.csv â†’ DataFrame(Date, rf_daily).\n",
    "        HeurÃ­stica principal: coluna Var como taxa diÃ¡ria em % (ex.: ~0,03% â†’ 0.0003).\n",
    "        Fallback: pct_change de FechAjust/FechHist.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path_cdi, dtype=str)\n",
    "        df[\"Data\"] = pd.to_datetime(df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "        if \"Var\" in df.columns:\n",
    "            df[\"Var\"] = df[\"Var\"].apply(to_float_smart)\n",
    "            if df[\"Var\"].notna().sum() > 3:\n",
    "                out = df[[\"Data\"]].copy()\n",
    "                out[\"rf_daily\"] = df[\"Var\"] / 100.0\n",
    "                return out.dropna().rename(columns={\"Data\":\"Date\"}).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "        for c in [\"FechAjust\",\"FechHist\"]:\n",
    "            if c in df.columns:\n",
    "                df[c] = df[c].apply(to_float_smart)\n",
    "                if df[c].notna().sum() > 3:\n",
    "                    r = df[c].pct_change()\n",
    "                    out = df[[\"Data\"]].copy()\n",
    "                    out[\"rf_daily\"] = r\n",
    "                    return out.dropna().rename(columns={\"Data\":\"Date\"}).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "        return pd.DataFrame(columns=[\"Date\",\"rf_daily\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SeÃ§Ã£o 3 â€” PriceProcessing (filtro de datas nulas adicionado)\n",
    "# =========================================\n",
    "# SeÃ§Ã£o 3 â€” Classe de preÃ§os (PriceProcessing) [ATUALIZADA]\n",
    "# =========================================\n",
    "def catalog_return(row, x, name_return):\n",
    "    val = row.get(name_return, np.nan)\n",
    "    std = row.get(f\"Rolling_std_{name_return}\", np.nan)\n",
    "    if pd.isna(val) or pd.isna(std) or std == 0: return 0\n",
    "    if val > x * std:   return 1\n",
    "    if val < -x * std:  return -1\n",
    "    return 0\n",
    "\n",
    "class PriceProcessing:\n",
    "    def __init__(self, df_prices: pd.DataFrame, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.df = df_prices.copy()\n",
    "\n",
    "        # Datas\n",
    "        self.df[\"Data\"] = pd.to_datetime(self.df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "        # ðŸ”§ NOVO: remove linhas com Data = NaT (evita erro no merge_asof)\n",
    "        self.df = self.df[~self.df[\"Data\"].isna()].copy()\n",
    "\n",
    "        # NÃºmeros\n",
    "        float_cols = [\"FechAjust\",\"FechHist\",\"AbertAjust\",\"MinAjust\",\"MedAjust\",\"MaxAjust\",\"Var\",\"Fator\"]\n",
    "        for c in float_cols:\n",
    "            if c in self.df.columns: self.df[c] = self.df[c].apply(to_float_smart)\n",
    "        for c in [\"Vol\",\"Neg\"]:\n",
    "            if c in self.df.columns: self.df[c] = self.df[c].apply(to_int_smart)\n",
    "\n",
    "        # Ordena/dedup\n",
    "        self.df = self.df.sort_values(\"Data\").drop_duplicates(\"Data\").reset_index(drop=True)\n",
    "\n",
    "        # Close\n",
    "        #self.df[\"Close\"] = self.df[\"FechAjust\"] if (\"FechAjust\" in self.df and self.df[\"FechAjust\"].notna().any()) else self.df.get(\"FechHist\", np.nan)\n",
    "        if \"FechAjust\" in self.df.columns and self.df[\"FechAjust\"].notna().any():\n",
    "            self.df[\"Close\"] = self.df[\"FechAjust\"]\n",
    "        else:\n",
    "            if \"AbertAjust\" in self.df.columns:\n",
    "                self.df[\"Close\"] = self.df[\"AbertAjust\"].shift(-1)\n",
    "            else:\n",
    "                self.df[\"Close\"] = np.nan        \n",
    "\n",
    "        # Fechamento ponderado por volume\n",
    "        if \"Vol\" in self.df.columns and self.df[\"Vol\"].notna().any():\n",
    "            vol_mean = self.df[\"Vol\"].replace(0, np.nan).mean()\n",
    "            self.df[\"FechPonderado\"] = self.df[\"Close\"] * self.df[\"Vol\"] / (vol_mean if vol_mean else self.df[\"Vol\"])\n",
    "        else:\n",
    "            self.df[\"FechPonderado\"] = np.nan\n",
    "\n",
    "    def create_return_by_period(self, name_return: str, period: int, column_name: str = \"Close\", remove_nan=False):\n",
    "        self.df[name_return] = np.log(self.df[column_name] / self.df[column_name].shift(period))\n",
    "        if remove_nan:\n",
    "            self.df.dropna(subset=[name_return], inplace=True)\n",
    "\n",
    "   \n",
    "    def create_rolling_std(self, name_return: str, window: int = 20):\n",
    "        \"\"\"\n",
    "        Calcula o desvio padrÃ£o em uma janela mÃ³vel.\n",
    "        Ex.: window=20 â†’ volatilidade de 20 perÃ­odos.\n",
    "        \"\"\"\n",
    "        self.df[f\"Rolling_std_{name_return}\"] = (\n",
    "            self.df[name_return].rolling(window=window).std()\n",
    "    )    \n",
    "\n",
    "    def create_indicator(self, name_return: str, factor: float = 0.1):\n",
    "        self.df[f\"Indicator_{name_return}\"] = self.df.apply(lambda r: catalog_return(r, factor, name_return), axis=1)\n",
    "\n",
    "    def finalize(self) -> pd.DataFrame:\n",
    "        cols = [\"Data\",\"Close\",\"FechPonderado\",\"Vol\",\"Neg\",\"Var\"]\n",
    "        cols += [c for c in self.df.columns if c.startswith((\"Daily_\",\"Week_\",\"Month_\"))]\n",
    "        cols += [c for c in self.df.columns if c.startswith((\"Rolling_std_\",\"Indicator_\"))]\n",
    "        cols = [c for c in cols if c in self.df.columns]\n",
    "        out = self.df[cols].copy()\n",
    "        out.insert(0, \"Ticker\", self.ticker)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "54d7fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SeÃ§Ã£o 4 â€” Fundamentos (Î”QoQ/Î”YoY & EPS/Proxy) + ASOF\n",
    "# ====================================================\n",
    "# SeÃ§Ã£o 4 â€” Classe de fundamentos (FundamentalProcessing)\n",
    "# ====================================================\n",
    "class FundamentalProcessing:\n",
    "    def __init__(self, df_fund: pd.DataFrame, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.df = df_fund.copy()\n",
    "\n",
    "        # Datas\n",
    "        if \"Data_Publicacao\" in self.df.columns:\n",
    "            self.df[\"Data_Publicacao\"] = pd.to_datetime(self.df[\"Data_Publicacao\"], dayfirst=True, errors=\"coerce\")\n",
    "        if \"Data_Demonstracao\" in self.df.columns:\n",
    "            self.df[\"Data_Demonstracao\"] = pd.to_datetime(self.df[\"Data_Demonstracao\"], dayfirst=True, errors=\"coerce\")\n",
    "        if \"Data_Analise\" in self.df.columns:\n",
    "            self.df[\"Data_Analise\"] = pd.to_datetime(self.df[\"Data_Analise\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "        # NumÃ©ricos relevantes\n",
    "        num_cols = [\n",
    "            \"RL\",\"LL\",\"EBITDA\",\"Preco_Abertura\",\"Preco_Fechamento\",\"LPA\",\"ROA\",\"ROE\",\"MEB\",\n",
    "            \"CRESC_RL_12M\",\"CRESC_LL_12M\",\"CRESC_EBITDA_12M\",\"CAPEX\",\"FCO\",\"FCF\",\n",
    "            \"Divida_Liquida\",\"PL\",\"Divida_Bruta\",\"AT\",\"DVA_Despesas_Fin\",\"PC\",\"PNC\",\"Outros_PC\",\"LUB\"\n",
    "        ]\n",
    "        for c in num_cols:\n",
    "            if c in self.df.columns: self.df[c] = self.df[c].apply(to_float_smart)\n",
    "\n",
    "        # Duplicados comuns\n",
    "        if set([\"RL\",\"RL_dup1\",\"RL_dup2\"]).issubset(self.df.columns):\n",
    "            self.df[\"RL\"] = self.df[[\"RL\",\"RL_dup1\",\"RL_dup2\"]].bfill(axis=1).iloc[:,0]\n",
    "            self.df.drop(columns=[\"RL_dup1\",\"RL_dup2\"], inplace=True, errors=\"ignore\")\n",
    "        if set([\"EBITDA\",\"EBITDA_dup\"]).issubset(self.df.columns):\n",
    "            self.df[\"EBITDA\"] = self.df[[\"EBITDA\",\"EBITDA_dup\"]].bfill(axis=1).iloc[:,0]\n",
    "            self.df.drop(columns=[\"EBITDA_dup\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        # QuarterEnd\n",
    "        self.df[\"QuarterEnd\"] = self.df.get(\"Data_Demonstracao\", pd.NaT)\n",
    "        if self.df[\"QuarterEnd\"].isna().all() and \"Data_Analise\" in self.df.columns:\n",
    "            self.df[\"QuarterEnd\"] = self.df[\"Data_Analise\"]\n",
    "\n",
    "        # Ordena por Quarter/Publ.; 1 linha por quarter\n",
    "        self.df = (self.df.sort_values([\"QuarterEnd\",\"Data_Publicacao\"])\n",
    "                        .drop_duplicates([\"QuarterEnd\"])\n",
    "                        .reset_index(drop=True))\n",
    "\n",
    "    def get_publication_dates(self) -> pd.Series:\n",
    "        return self.df[\"Data_Publicacao\"].dropna()\n",
    "\n",
    "    def features_for_asof_merge(self) -> pd.DataFrame:\n",
    "        keep = [\"QuarterEnd\",\"Data_Publicacao\"]\n",
    "        num_cols = [c for c in self.df.columns if c not in keep and pd.api.types.is_numeric_dtype(self.df[c])]\n",
    "        out = self.df[keep + num_cols].dropna(subset=[\"Data_Publicacao\"]).sort_values(\"Data_Publicacao\").reset_index(drop=True)\n",
    "        out = out.rename(columns={c: f\"F_{c}\" for c in num_cols})\n",
    "        return out\n",
    "\n",
    "    def build_qoq_yoy_and_eps(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tabela por Data_Publicacao/QuarterEnd com mÃ©tricas + Î”QoQ/Î”YoY e EPS Surprise (proxy Î”LPA se nÃ£o houver consenso).\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        out = df[[\"Data_Publicacao\",\"QuarterEnd\"]].copy()\n",
    "        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "        for c in num_cols:\n",
    "            out[c] = df[c].values\n",
    "            out[f\"{c}_Q_Change\"] = df[c].diff(1).values\n",
    "            out[f\"{c}_Y_Change\"] = (df[c] - df[c].shift(4)).values\n",
    "\n",
    "        # EPS Surprise real se houver consenso; senÃ£o proxy via Î”LPA\n",
    "        if \"EPS_Consensus\" in df.columns and \"LPA\" in df.columns:\n",
    "            eps = df[\"LPA\"] - df[\"EPS_Consensus\"]\n",
    "        elif \"LPA\" in df.columns:\n",
    "            eps = df[\"LPA\"].diff(1)\n",
    "        else:\n",
    "            eps = pd.Series([np.nan]*len(df))\n",
    "\n",
    "        out[\"EPS_EarningsSurprise\"] = eps\n",
    "        out[\"EPS_Earnings_Surprise_Backward_Diff\"] = eps - eps.shift(1)\n",
    "        out[\"EPS_Earnings_Surprise_Backward_Ave_Diff\"] = eps - eps.shift(3).rolling(3).mean()\n",
    "\n",
    "        out = out.dropna(subset=[\"Data_Publicacao\"]).sort_values(\"Data_Publicacao\").reset_index(drop=True)\n",
    "        return out.rename(columns={\"Data_Publicacao\":\"AnnounceDate\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeÃ§Ã£o 5 â€” Helpers PEAD: Î² CAPM, CAR, T1  [ATUALIZADA]\n",
    "# ==========================================================\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def detect_start_index(prices: pd.DataFrame,\n",
    "                       announce_date: pd.Timestamp,\n",
    "                       announce_time: Optional[str] = None) -> int:\n",
    "    \"\"\"\n",
    "    Retorna o Ã­ndice T1 na sÃ©rie de preÃ§os (coluna 'Date'):\n",
    "      - Sem horÃ¡rio (None ou 'DUR'/'BMO'): 1Âº pregÃ£o >= announce_date\n",
    "      - 'AMC' (after market):               1Âº pregÃ£o  > announce_date\n",
    "    CompatÃ­vel com chamadas de 2 ou 3 argumentos.\n",
    "    \"\"\"\n",
    "    if prices is None or prices.empty or 'Date' not in prices:\n",
    "        return 0\n",
    "\n",
    "    # Garante dtype/ordem e vetor numpy ordenado\n",
    "    ds = pd.to_datetime(prices['Date'], errors='coerce').sort_values().values\n",
    "    if ds.size == 0 or pd.isna(announce_date):\n",
    "        return 0\n",
    "\n",
    "    idx_ge = np.searchsorted(ds, np.array(announce_date, dtype='datetime64[ns]'))\n",
    "\n",
    "    # Se horÃ¡rio conhecido e for after-market, pula para o prÃ³ximo pregÃ£o\n",
    "    if announce_time and str(announce_time).upper() == 'AMC':\n",
    "        idx = idx_ge + 1\n",
    "    else:\n",
    "        idx = idx_ge\n",
    "\n",
    "    # Limites seguros\n",
    "    idx = int(max(0, min(idx, ds.size - 1)))\n",
    "    return idx\n",
    "\n",
    "\n",
    "def estimate_beta(stock_df: pd.DataFrame,\n",
    "                  mkt_df: pd.DataFrame,\n",
    "                  rf_df: pd.DataFrame,\n",
    "                  event_idx: int,\n",
    "                  estimation_window: int = 252) -> float:\n",
    "    \"\"\"\n",
    "    Estima Î² via OLS na janela [event_idx - estimation_window, event_idx-1],\n",
    "    usando retornos em excesso (ri - rf) e (rm - rf).\n",
    "    A janela jÃ¡ deve chegar 'adaptada' pela lÃ³gica chamadora.\n",
    "    \"\"\"\n",
    "    m = stock_df[['Date','Close']].merge(\n",
    "            mkt_df[['Date','Close']], on='Date', suffixes=('_i','_m')\n",
    "        )\n",
    "    m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').ffill().sort_values('Date')\n",
    "\n",
    "    #m['ri'] = m['Close_i'].pct_change()\n",
    "    #m['rm'] = m['Close_m'].pct_change()\n",
    "    m['ri'] = np.log(m['Close_i'] / m['Close_i'].shift(1))\n",
    "    m['rm'] = np.log(m['Close_m'] / m['Close_m'].shift(1))\n",
    "\n",
    "    if event_idx < 2:\n",
    "        return np.nan\n",
    "\n",
    "    event_date = stock_df.iloc[event_idx]['Date']\n",
    "    eidx = m.index[m['Date'] == event_date]\n",
    "    if len(eidx) == 0:\n",
    "        return np.nan\n",
    "    eidx = eidx[0]\n",
    "\n",
    "    start = max(m.index.min(), eidx - estimation_window)\n",
    "    end   = eidx - 1\n",
    "    if end - start < 30:  # resguardo mÃ­nimo para OLS\n",
    "        return np.nan\n",
    "\n",
    "    win = m.loc[start:end].dropna()\n",
    "    if win.empty:\n",
    "        return np.nan\n",
    "\n",
    "    x = (win['rm'] - win['rf_daily']).values.reshape(-1, 1)\n",
    "    y = (win['ri'] - win['rf_daily']).values.reshape(-1, 1)\n",
    "    beta = np.linalg.lstsq(x, y, rcond=None)[0].ravel()[0]\n",
    "    return float(beta)\n",
    "\n",
    "\n",
    "def compute_car(stock_df: pd.DataFrame,\n",
    "                mkt_df: pd.DataFrame,\n",
    "                rf_df: pd.DataFrame,\n",
    "                event_idx: int,\n",
    "                beta: float,\n",
    "                holding_days: int = 30) -> float:\n",
    "    \"\"\"\n",
    "    CAR = soma dos retornos anormais no intervalo [T1, T1+holding_days-1].\n",
    "    Modelo de expectativa: CAPM com Î² estimado na janela de estimaÃ§Ã£o.\n",
    "    \"\"\"\n",
    "    m = stock_df[['Date','Close']].merge(\n",
    "            mkt_df[['Date','Close']], on='Date', suffixes=('_i','_m')\n",
    "        )\n",
    "    m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').ffill().sort_values('Date')\n",
    "\n",
    "    #m['ri'] = m['Close_i'].pct_change()\n",
    "    #m['rm'] = m['Close_m'].pct_change()\n",
    "    m['ri'] = np.log(m['Close_i'] / m['Close_i'].shift(1))\n",
    "    m['rm'] = np.log(m['Close_m'] / m['Close_m'].shift(1))\n",
    "\n",
    "    start = event_idx\n",
    "    end   = min(start + holding_days - 1, len(m) - 1)\n",
    "    seg   = m.iloc[start:end+1].dropna()\n",
    "    if seg.empty:\n",
    "        return np.nan\n",
    "\n",
    "    seg['E_ri'] = seg['rf_daily'] + beta * (seg['rm'] - seg['rf_daily'])\n",
    "    seg['AR']   = seg['ri'] - seg['E_ri']\n",
    "    return float(seg['AR'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeab05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SeÃ§Ã£o 6 â€” DataPrepPipeline.process_all\n",
    "def process_all(self,\n",
    "                indicator_factor: float = 0.1,\n",
    "                save_intermediate_prices: bool = True,\n",
    "                attach_fundamentals_asof: bool = True,\n",
    "                only_events: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Processa todos os arquivos em dataset/prices e devolve consolidado.\"\"\"\n",
    "    all_final = []\n",
    "    for fn in os.listdir(self.prices_dir):\n",
    "        if not fn.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        try:\n",
    "            path = os.path.join(self.prices_dir, fn)\n",
    "            df_final = self.process_one(\n",
    "                path,\n",
    "                indicator_factor=indicator_factor,\n",
    "                save_intermediate_prices=save_intermediate_prices,\n",
    "                attach_fundamentals_asof=attach_fundamentals_asof,\n",
    "                only_events=only_events\n",
    "            )\n",
    "            if df_final is not None and not df_final.empty:\n",
    "                all_final.append(df_final.assign(Ticker=self._ticker_from_price_filename(fn)))\n",
    "        except Exception as ex:\n",
    "            print(f\"Erro no ticker de {fn}: {ex}\")\n",
    "            continue\n",
    "    if not all_final:\n",
    "        return pd.DataFrame()\n",
    "    df_all = pd.concat(all_final, ignore_index=True)\n",
    "    df_all = df_all.sort_values([\"Ticker\", \"Data\"]).reset_index(drop=True)\n",
    "    # salva consolidado\n",
    "    df_all.to_csv(os.path.join(self.out_final_dir, \"final_dataprep.csv\"), index=False)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SeÃ§Ã£o 6 â€” DataPrepPipeline.process_one\n",
    "# ============================================================\n",
    "# SeÃ§Ã£o 6 â€” MÃ©todo process_one (DataPrepPipeline) [ATUALIZADO]\n",
    "# ============================================================\n",
    "\n",
    "def process_one(self, price_csv_path: str,\n",
    "                indicator_factor: float = 0.1,\n",
    "                save_intermediate_prices: bool = True,\n",
    "                attach_fundamentals_asof: bool = True,\n",
    "                only_events: bool = False) -> Optional[pd.DataFrame]:\n",
    "\n",
    "    base = os.path.basename(price_csv_path).upper()\n",
    "    if base in {\"IBOV.SA.CSV\", \"CDI.SA.CSV\"}:\n",
    "        return pd.DataFrame()  # proxies â€” nÃ£o sÃ£o papÃ©is\n",
    "\n",
    "    # PreÃ§o\n",
    "    dfp = pd.read_csv(price_csv_path, sep=None, engine=\"python\", dtype=str)\n",
    "    tkr = self._ticker_from_price_filename(price_csv_path)\n",
    "    price = PriceProcessing(dfp, tkr)\n",
    "\n",
    "    # Retornos e indicadores\n",
    "    price.create_return_by_period(\"Daily_Return\", 1)\n",
    "    price.create_return_by_period(\"Week_Return\", 5)\n",
    "    price.create_return_by_period(\"Month_Return\", 22)\n",
    "    price.create_rolling_std(\"Daily_Return\", window=21)\n",
    "    price.create_rolling_std(\"Week_Return\", window=65)\n",
    "    price.create_rolling_std(\"Month_Return\", window=252)\n",
    "    # indicadores discretos    \n",
    "    price.create_indicator(\"Daily_Return\", indicator_factor)\n",
    "    price.create_indicator(\"Week_Return\", indicator_factor)\n",
    "    price.create_indicator(\"Month_Return\", indicator_factor)\n",
    "\n",
    "    df_price_feat = price.finalize()\n",
    "    if save_intermediate_prices:\n",
    "        df_price_feat.to_csv(os.path.join(self.out_prices_dir, os.path.basename(price_csv_path)), index=False)\n",
    "\n",
    "    # Fundamentos\n",
    "    fund_path = self._fund_path_for_pricefile(self.fund_dir, price_csv_path)\n",
    "    if fund_path is None:\n",
    "        df_final = df_price_feat.copy()\n",
    "        df_final[\"event\"] = 0\n",
    "    else:\n",
    "        dff_raw = pd.read_csv(fund_path, sep=None, engine=\"python\", dtype=str)\n",
    "        fund = FundamentalProcessing(dff_raw, tkr)\n",
    "\n",
    "        # DiagnÃ³stico opcional\n",
    "        if fund.get_publication_dates().dropna().empty:\n",
    "            print(f\"[AVISO] Sem Data_Publicacao vÃ¡lida para {tkr} em {os.path.basename(fund_path)}\")\n",
    "\n",
    "        pub_dates = set(pd.to_datetime(fund.get_publication_dates(), errors=\"coerce\").dropna().values)\n",
    "        df_final = df_price_feat.copy()\n",
    "        df_final[\"event\"] = df_final[\"Data\"].isin(pub_dates).astype(int)\n",
    "\n",
    "        if attach_fundamentals_asof:\n",
    "            f_asof = fund.features_for_asof_merge()\n",
    "            if not f_asof.empty:\n",
    "                # ðŸ”§ Chaves sem NaT + ordenadas (requisito do merge_asof)\n",
    "                df_final = df_final.dropna(subset=[\"Data\"]).sort_values(\"Data\").reset_index(drop=True)\n",
    "                f_asof   = f_asof.dropna(subset=[\"Data_Publicacao\"]).sort_values(\"Data_Publicacao\").reset_index(drop=True)\n",
    "\n",
    "                df_final = pd.merge_asof(\n",
    "                    df_final, f_asof,\n",
    "                    left_on=\"Data\",\n",
    "                    right_on=\"Data_Publicacao\",\n",
    "                    direction=\"backward\"\n",
    "                )\n",
    "                if \"Data_Publicacao\" in df_final.columns:\n",
    "                    df_final.drop(columns=[\"Data_Publicacao\"], inplace=True)\n",
    "\n",
    "        if only_events:\n",
    "            df_final = df_final[df_final[\"event\"] == 1].copy()\n",
    "\n",
    "    # Garante Ticker correto (sem duplicar)\n",
    "    if \"Ticker\" in df_final.columns:\n",
    "        df_final[\"Ticker\"] = tkr\n",
    "    else:\n",
    "        df_final.insert(0, \"Ticker\", tkr)\n",
    "\n",
    "    out_final_path = os.path.join(self.out_final_dir, f\"{tkr}.final.csv\")\n",
    "    df_final.to_csv(out_final_path, index=False)\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db29f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeÃ§Ã£o 7 â€” Builder do dataset de eventos (PT-BR)  [CORRIGIDA]\n",
    "# ==========================================================\n",
    "from typing import Optional, Dict, List\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Helpers de data em PT-BR (dd/mm/aaaa) + YYYYMMDD ----------\n",
    "def _parse_date_br_str(s: str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Converte datas de 'dd/mm/aaaa' ou 'YYYYMMDD' para Timestamp normalizado (00:00).\n",
    "    Aceita espaÃ§os e caracteres soltos; retorna NaT se invÃ¡lida.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return pd.NaT\n",
    "    s = str(s).strip()\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return pd.NaT\n",
    "\n",
    "    # limpa tudo que nÃ£o seja dÃ­gito ou '/'\n",
    "    raw = re.sub(r\"[^0-9/]\", \"\", s)\n",
    "\n",
    "    # caso YYYYMMDD (8 dÃ­gitos e sem '/'): parse explÃ­cito sem dayfirst\n",
    "    if re.fullmatch(r\"\\d{8}\", raw) and \"/\" not in raw:\n",
    "        try:\n",
    "            return pd.to_datetime(raw, format=\"%Y%m%d\", errors=\"raise\").normalize()\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "\n",
    "    # caso dd/mm/aaaa (PT-BR) â€” parse explÃ­cito\n",
    "    try:\n",
    "        return pd.to_datetime(raw, format=\"%d/%m/%Y\", errors=\"raise\").normalize()\n",
    "    except Exception:\n",
    "        # fallback robusto com dayfirst\n",
    "        return pd.to_datetime(raw, dayfirst=True, errors=\"coerce\").normalize()\n",
    "\n",
    "def _parse_series_date_br(sr: pd.Series) -> pd.Series:\n",
    "    return sr.astype(str).map(_parse_date_br_str)\n",
    "\n",
    "# ---------- Wrapper compatÃ­vel para detect_start_index (2 ou 3 args) ----------\n",
    "def _safe_detect_start_index(prices: pd.DataFrame,\n",
    "                             announce_date: pd.Timestamp,\n",
    "                             announce_time: Optional[str] = None) -> int:\n",
    "    \"\"\"\n",
    "    Chama detect_start_index com 3 ou 2 argumentos conforme a versÃ£o carregada.\n",
    "    Evita TypeError no ambiente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect_start_index(prices, announce_date, announce_time)  # nova assinatura\n",
    "    except TypeError:\n",
    "        return detect_start_index(prices, announce_date)                 # assinatura antiga\n",
    "\n",
    "\n",
    "class EventDatasetBuilder:\n",
    "    def __init__(self, mkt_df: pd.DataFrame, rf_df: pd.DataFrame,\n",
    "                 estimation_window: int = 252, holding_days: int = 30, min_estimation: int = 60):\n",
    "        \"\"\"\n",
    "        estimation_window: janela alvo para Î² (mÃ¡ximo). SerÃ¡ reduzida se nÃ£o houver histÃ³rico.\n",
    "        min_estimation   : mÃ­nimo exigido para estimar Î² (evita IPOs muito recentes).\n",
    "        \"\"\"\n",
    "        self.mkt = mkt_df.sort_values('Date').reset_index(drop=True)\n",
    "        self.rf  = rf_df.sort_values('Date').reset_index(drop=True)\n",
    "        self.estimation_window = estimation_window\n",
    "        self.holding_days      = holding_days\n",
    "        self.min_estimation    = min_estimation\n",
    "\n",
    "    def build_for_ticker(self, tkr: str,\n",
    "                         price_final_csv: str,\n",
    "                         fund_raw_csv: str,\n",
    "                         announce_time_map: Optional[Dict[pd.Timestamp,str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        price_final_csv: dataset/final/TICKER.final.csv â€” precisa de [Data, Close]\n",
    "        fund_raw_csv   : dataset/fundamental/TICKER.SA.csv â€” fonte de Data_Publicacao (PT-BR)\n",
    "        announce_time_map: opcional {Timestamp: 'BMO'|'AMC'|'DUR'}; se None, assume DUR/UNK\n",
    "        \"\"\"\n",
    "        # ----- preÃ§os -----\n",
    "        px = pd.read_csv(price_final_csv, parse_dates=['Data'])\n",
    "        px = px.sort_values('Data').dropna(subset=['Close']).reset_index(drop=True)\n",
    "        px_idx   = px[['Data']].rename(columns={'Data':'Date'})\n",
    "        px_close = px[['Data','Close']].rename(columns={'Data':'Date'})\n",
    "\n",
    "        # ----- fundamentos (parse PT-BR/YYYYMMDD) -----\n",
    "        raw = pd.read_csv(fund_raw_csv, sep=None, engine='python', dtype=str)\n",
    "\n",
    "        if 'Data_Publicacao' not in raw.columns:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        pub_series = _parse_series_date_br(raw['Data_Publicacao'])\n",
    "        pub_dates  = pub_series.dropna().drop_duplicates().sort_values()\n",
    "        if pub_dates.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # features de fundamentos (QoQ/YoY/EPS etc.), alinhadas Ã  AnnounceDate\n",
    "        fund  = FundamentalProcessing(raw, tkr)\n",
    "        feats = fund.build_qoq_yoy_and_eps()  # deve gerar 'AnnounceDate'\n",
    "        if 'AnnounceDate' in feats.columns:\n",
    "            feats['363'] = _parse_series_date_br(feats['AnnounceDate'])\n",
    "            feats['AnnounceDate'] = feats['AnnounceDate'].dt.normalize()\n",
    "            feats = feats.dropna(subset=['AnnounceDate'])\\\n",
    "                         .drop_duplicates(subset=['AnnounceDate'])\\\n",
    "                         .reset_index(drop=True)\n",
    "        else:\n",
    "            # sem coluna â€” segue sÃ³ com CAR/Beta\n",
    "            feats = pd.DataFrame(columns=['AnnounceDate'])\n",
    "\n",
    "        # ----- loop de eventos (AnnounceDate = Data_Publicacao) -----\n",
    "        recs = []\n",
    "        for ad in pub_dates:\n",
    "            atime  = announce_time_map.get(ad) if announce_time_map else None  # sem horÃ¡rio â†’ DUR/UNK\n",
    "            t1_idx = _safe_detect_start_index(px_idx, ad, atime)\n",
    "\n",
    "            # janela adaptativa para Î²\n",
    "            est_len = min(self.estimation_window, t1_idx)\n",
    "            if est_len < self.min_estimation:\n",
    "                continue  # pula eventos sem histÃ³rico suficiente\n",
    "\n",
    "            beta = estimate_beta(px_close, self.mkt, self.rf, t1_idx, est_len)\n",
    "            if pd.isna(beta): \n",
    "                continue\n",
    "            car  = compute_car(px_close, self.mkt, self.rf, t1_idx, beta, self.holding_days)\n",
    "            if pd.isna(car):\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                'Ticker'        : tkr,\n",
    "                'AnnounceDate'  : ad,                             # = Data_Publicacao (PT-BR)\n",
    "                'EventTradeDate': px_idx.iloc[t1_idx]['Date'],    # 1Âº pregÃ£o â‰¥ anÃºncio\n",
    "                'CAR_30D'       : float(car),\n",
    "                'CAR_Sign'      : int(car > 0),\n",
    "                'Beta'          : float(beta),\n",
    "                'EstimationLen' : int(est_len),\n",
    "                'FundSource'    : os.path.basename(fund_raw_csv),\n",
    "            }\n",
    "\n",
    "            # anexa features exatamente na AnnounceDate (sem look-ahead)\n",
    "            if not feats.empty:\n",
    "                frow = feats.loc[feats['AnnounceDate'] == ad]\\\n",
    "                           .drop(columns=['AnnounceDate','QuarterEnd'], errors='ignore')\n",
    "                if not frow.empty:\n",
    "                    row.update(frow.iloc[0].to_dict())\n",
    "\n",
    "            recs.append(row)\n",
    "\n",
    "        return pd.DataFrame(recs)\n",
    "\n",
    "\n",
    "def winsorize_and_standardize(events_df: pd.DataFrame,\n",
    "                              by_col: str = 'Ticker',\n",
    "                              exclude_cols: List[str] = ['Ticker','AnnounceDate','EventTradeDate','CAR_30D','CAR_Sign','FundSource']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Winsoriza 1%/1% por ticker e aplica z-score global.\n",
    "    (EventTradeDate/FundSource sÃ£o nÃ£o-numÃ©ricas e ficam excluÃ­das.)\n",
    "    \"\"\"\n",
    "    df = events_df.copy()\n",
    "    num_cols = [c for c in df.columns if c not in exclude_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "    # winsor por ticker\n",
    "    def _w(g):\n",
    "        for c in num_cols:\n",
    "            g[c] = winsorize_series(g[c], p=0.01)\n",
    "        return g\n",
    "    df = df.groupby(by_col, group_keys=False).apply(_w)\n",
    "\n",
    "    # standardize global\n",
    "    if num_cols:\n",
    "        m = df[num_cols].fillna(0.0).mean()\n",
    "        s = df[num_cols].fillna(0.0).std(ddof=0).replace(0, 1.0)\n",
    "        X = (df[num_cols].fillna(0.0) - m) / s\n",
    "        X.columns = [f\"STD_{c}\" for c in X.columns]\n",
    "        out = pd.concat([df[exclude_cols], X], axis=1)\n",
    "    else:\n",
    "        out = df[exclude_cols].copy()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "32307433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeÃ§Ã£o 8 â€” Helper de fallback p/ fundamentos [NOVO]\n",
    "# ==================================\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def find_fund_path_for_tkr(fund_dir: str, tkr: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Procura dataset/fundamental/<MESMO BASENAME>.SA.csv e, se nÃ£o achar,\n",
    "    tenta o 'ticker base' (remove sufixos de classe: 11, 34, 3, 4, 5, 6).\n",
    "    Ex.: AZUL4 -> AZUL ; SANB11 -> SANB\n",
    "    \"\"\"\n",
    "    # 1) mesmo basename\n",
    "    p = Path(fund_dir) / f\"{tkr}.SA.csv\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    # 2) fallback por ticker base\n",
    "    t_base = re.sub(r\"(11|34|3|4|5|6)$\", \"\", tkr.upper())\n",
    "    cand = Path(fund_dir) / f\"{t_base}.SA.csv\"\n",
    "    if cand.exists():\n",
    "        return cand\n",
    "\n",
    "    matches = list(Path(fund_dir).glob(f\"{t_base}*.csv\"))\n",
    "    return matches[0] if matches else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "17acb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PÃ³s-processamento: limpeza + recomputo Î² & CAR (consistÃªncia) [ATUALIZADO]\n",
    "# ==========================================================\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Wrapper compatÃ­vel com detect_start_index de 2 OU 3 argumentos ---\n",
    "def safe_detect_start_index(prices: pd.DataFrame,\n",
    "                            announce_date: pd.Timestamp,\n",
    "                            announce_time=None) -> int:\n",
    "    \"\"\"\n",
    "    Tenta chamar detect_start_index(prices, announce_date, announce_time).\n",
    "    Se a assinatura antiga (2 args) estiver carregada no ambiente, faz fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect_start_index(prices, announce_date, announce_time)  # nova assinatura\n",
    "    except TypeError:\n",
    "        return detect_start_index(prices, announce_date)                 # assinatura antiga\n",
    "\n",
    "def _load_price_final_for(ticker: str, final_dir: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retorna (px_idx, px_close) a partir de dataset/final/<TICKER>.final.csv\n",
    "    \"\"\"\n",
    "    p = Path(final_dir) / f\"{ticker}.final.csv\"\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    px = pd.read_csv(p, parse_dates=['Data'])\n",
    "    px = px.dropna(subset=['Close']).sort_values('Data').reset_index(drop=True)\n",
    "    px_idx   = px[['Data']].rename(columns={'Data':'Date'})\n",
    "    px_close = px[['Data','Close']].rename(columns={'Data':'Date'})\n",
    "    return px_idx, px_close\n",
    "\n",
    "def clean_events(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Drop AnnounceDate nula\n",
    "    2) Normaliza dtype e ordena\n",
    "    3) Remove duplicatas (Ticker, AnnounceDate) mantendo o menor EventTradeDate\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # normaliza datas\n",
    "    out['AnnounceDate']   = pd.to_datetime(out['AnnounceDate'], errors='coerce')\n",
    "    if 'EventTradeDate' in out.columns:\n",
    "        out['EventTradeDate'] = pd.to_datetime(out['EventTradeDate'], errors='coerce')\n",
    "\n",
    "    # remove nulos de announce\n",
    "    out = out.dropna(subset=['AnnounceDate'])\n",
    "\n",
    "    # para desempate em duplicatas, manter menor EventTradeDate\n",
    "    sort_cols = ['Ticker','AnnounceDate'] + (['EventTradeDate'] if 'EventTradeDate' in out.columns else [])\n",
    "    out = out.sort_values(sort_cols).drop_duplicates(subset=['Ticker','AnnounceDate'], keep='first').reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def recompute_beta_car(ready_df: pd.DataFrame,\n",
    "                       final_dir: str,\n",
    "                       mkt_df: pd.DataFrame,\n",
    "                       rf_df: pd.DataFrame,\n",
    "                       estimation_window: int = 252,\n",
    "                       min_estimation: int = 60,\n",
    "                       holding_days: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recalcula Beta, CAR_30D, EstimationLen e EventTradeDate para TODO o dataset,\n",
    "    garantindo consistÃªncia com SeÃ§Ã£o 5 (detect_start_index/estimate_beta/compute_car).\n",
    "    \"\"\"\n",
    "    df = ready_df.copy()\n",
    "    df['AnnounceDate'] = pd.to_datetime(df['AnnounceDate'], errors='coerce')\n",
    "\n",
    "    for tkr, grp in df.groupby('Ticker', sort=False):\n",
    "        px_idx, px_close = _load_price_final_for(tkr, final_dir)\n",
    "        if px_idx.empty or px_close.empty:\n",
    "            df.loc[grp.index, ['Beta','CAR_30D','EstimationLen','EventTradeDate']] = np.nan\n",
    "            continue\n",
    "\n",
    "        for i in grp.index:\n",
    "            ad = df.at[i, 'AnnounceDate']\n",
    "            if pd.isna(ad):\n",
    "                df.at[i, 'Beta'] = np.nan\n",
    "                df.at[i, 'CAR_30D'] = np.nan\n",
    "                df.at[i, 'EstimationLen'] = np.nan\n",
    "                df.at[i, 'EventTradeDate'] = pd.NaT\n",
    "                continue\n",
    "\n",
    "            # T1 de acordo com DUR/UNK (sem horÃ¡rio) â€” usa wrapper compatÃ­vel\n",
    "            t1_idx = safe_detect_start_index(px_idx, ad, None)\n",
    "\n",
    "            # janela adaptativa\n",
    "            est_len = min(estimation_window, t1_idx)\n",
    "            if est_len < min_estimation:\n",
    "                df.at[i, 'Beta'] = np.nan\n",
    "                df.at[i, 'CAR_30D'] = np.nan\n",
    "                df.at[i, 'EstimationLen'] = est_len\n",
    "                df.at[i, 'EventTradeDate'] = px_idx.iloc[t1_idx]['Date'] if len(px_idx) else pd.NaT\n",
    "                continue\n",
    "\n",
    "            beta = estimate_beta(px_close, mkt_df, rf_df, t1_idx, est_len)\n",
    "            car  = compute_car(px_close, mkt_df, rf_df, t1_idx, beta, holding_days)\n",
    "\n",
    "            df.at[i, 'Beta'] = beta\n",
    "            df.at[i, 'CAR_30D'] = car\n",
    "            df.at[i, 'EstimationLen'] = est_len\n",
    "            df.at[i, 'EventTradeDate'] = px_idx.iloc[t1_idx]['Date']\n",
    "\n",
    "    return df.sort_values(['Ticker','AnnounceDate']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f804de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso\n",
    "# ==================================\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "\n",
    "    # 0) Pastas\n",
    "    ensure_dirs()\n",
    "\n",
    "    # 1) PrÃ©-processa todos os papÃ©is (gera dataset/prices_processed/ e dataset/final/TICKER.final.csv)\n",
    "    pipeline = DataPrepPipeline(\n",
    "        prices_dir=\"dataset/prices\",\n",
    "        fund_dir=\"dataset/fundamental\",\n",
    "        out_prices_dir=\"dataset/prices_processed\",\n",
    "        out_final_dir=\"dataset/final\"\n",
    "    )\n",
    "    df_all_final = pipeline.process_all(\n",
    "        indicator_factor=0.1,\n",
    "        save_intermediate_prices=True,\n",
    "        attach_fundamentals_asof=True,  # fundamentos â€œvigentesâ€ atÃ© a prÃ³xima publicaÃ§Ã£o\n",
    "        only_events=False               # se quiser sÃ³ linhas em datas de evento, mude para True\n",
    "    )\n",
    "    print(\"Step 1 OK â€” final_dataprep.csv salvo em dataset/final/\")\n",
    "\n",
    "    # 2) Carrega proxies de mercado e CDI dos CSVs locais\n",
    "    mkt = MarketAndRiskLoader.load_ibov_csv(\"dataset/prices/IBOV.SA.csv\")   # Date, Close\n",
    "    rf  = MarketAndRiskLoader.load_cdi_csv(\"dataset/prices/CDI.SA.csv\")     # Date, rf_daily\n",
    "\n",
    "    # 3) ConstrÃ³i dataset de eventos (CAR_30D, Beta, Î”QoQ/Î”YoY, EPS/proxy)\n",
    "    builder = EventDatasetBuilder(mkt_df=mkt, rf_df=rf, estimation_window=252, holding_days=30)\n",
    "    all_events = []\n",
    "    for f in Path(\"dataset/final\").glob(\"*.final.csv\"):\n",
    "        tkr = f.stem.split(\".\")[0]\n",
    "        # Fundamental com o MESMO basename do preÃ§o\n",
    "        fund_path = Path(\"dataset/fundamental\") / f\"{tkr}.SA.csv\"\n",
    "        if not fund_path.exists():\n",
    "            cand = next(Path(\"dataset/fundamental\").glob(f\"{tkr}*.csv\"), None)\n",
    "            fund_path = cand if cand else None\n",
    "        if fund_path is None or not Path(fund_path).exists():\n",
    "            continue\n",
    "\n",
    "        # Sem horÃ¡rio â†’ DUR/UNK (T1 = 1Âº pregÃ£o â‰¥ Data de PublicaÃ§Ã£o)\n",
    "        ev_tkr = builder.build_for_ticker(tkr, str(f), str(fund_path), announce_time_map=None)\n",
    "        if not ev_tkr.empty:\n",
    "            all_events.append(ev_tkr)\n",
    "\n",
    "    if all_events:\n",
    "        events_df = pd.concat(all_events, ignore_index=True)\n",
    "        ready_df  = winsorize_and_standardize(events_df)\n",
    "        ready_df.to_csv(\"dataset/final/pead_event_dataset_2010_2019.csv\", index=False)\n",
    "        print(\"OK â€” dataset de eventos salvo em dataset/final/pead_event_dataset_2010_2019.csv\")\n",
    "        print(ready_df.head())\n",
    "    else:\n",
    "        print(\"Nenhum evento encontrado. Verifique se 'Data_Publicacao' existe e estÃ¡ no formato dd/mm/aaaa nos CSVs de fundamentos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bef6f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK â€” dataset de eventos (limpo e consistente) salvo em dataset/final/pead_event_dataset_2010_2019.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Limpeza (nulos + duplicatas)\n",
    "ready_df = clean_events(ready_df)\n",
    "\n",
    "# 2) Recalcula Î², CAR e ajusta EventTradeDate para todo o dataset (consistÃªncia)\n",
    "ready_df = recompute_beta_car(\n",
    "    ready_df,\n",
    "    final_dir=\"dataset/final\",\n",
    "    mkt_df=mkt,\n",
    "    rf_df=rf,\n",
    "    estimation_window=252,\n",
    "    min_estimation=60,\n",
    "    holding_days=30\n",
    ")\n",
    "\n",
    "# 3) Salva o dataset final pronto\n",
    "ready_df.to_csv(\"dataset/final/pead_event_dataset_2010_2019.csv\", index=False)\n",
    "print(\"OK â€” dataset de eventos (limpo e consistente) salvo em dataset/final/pead_event_dataset_2010_2019.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c05b13d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[A] Checando integridade do dataset final...\n",
      " - Colunas obrigatÃ³rias OK\n",
      " - Datas nulas: 2 | CAR_30D nulos: 0 | Duplicatas (Ticker,AnnounceDate): 1\n",
      " - Stats CAR_30D: count    9.000000\n",
      "mean     0.333776\n",
      "std      0.291750\n",
      "min     -0.035384\n",
      "1%      -0.031518\n",
      "5%      -0.016055\n",
      "50%      0.249886\n",
      "95%      0.689457\n",
      "99%      0.698490\n",
      "max      0.700748\n",
      "Name: CAR_30D, dtype: float64\n",
      "\n",
      "[B] Checando z-scores (STD_*)...\n",
      " - |mean(STD_*)| maiores (esperado ~0):\n",
      "STD_Beta                       1.011537e-15\n",
      "STD_EstimationLen              1.233581e-16\n",
      "STD_Divida_Bruta_Y_Change      2.467162e-17\n",
      "STD_Divida_Liquida_Q_Change    2.467162e-17\n",
      "STD_ROE_Q_Change               2.467162e-17\n",
      "dtype: float64\n",
      " - |std(STD_*) - 1| maiores (esperado ~0):\n",
      "STD_CAPEX                        1.0\n",
      "STD_DVA_Despesas_Fin             1.0\n",
      "STD_CRESC_RL_12M_Y_Change        1.0\n",
      "STD_CRESC_LL_12M_Y_Change        1.0\n",
      "STD_CRESC_EBITDA_12M_Y_Change    1.0\n",
      "dtype: float64\n",
      " - Magnitudes de STD_* dentro do esperado (<= 8).\n",
      "\n",
      "[C] Recontando CAR/Î² para amostra (confirmaÃ§Ã£o economÃ©trica)...\n",
      " - AZUL4 | 2018-09-03 | CAR_saved=0.660624  CAR_recalc=0.643745  Î”=-1.687874e-02  Î²=1.521\n",
      " - AZUL4 | 2018-09-08 | CAR_saved=0.700748  CAR_recalc=0.594078  Î”=-1.066706e-01  Î²=1.524\n",
      " - AZUL4 | 2019-07-11 | CAR_saved=0.084323  CAR_recalc=0.231536  Î”=1.472136e-01  Î²=1.744\n",
      " - AZUL4 | 2018-08-11 | CAR_saved=0.672520  CAR_recalc=0.482433  Î”=-1.900870e-01  Î²=1.520\n",
      " - AZUL4 | 2019-08-08 | CAR_saved=0.239220  CAR_recalc=0.239220  Î”=0.000000e+00  Î²=1.753\n",
      "\n",
      "[D] Conferindo que AnnounceDate âˆˆ Data_Publicacao (amostra por ticker)...\n",
      " - AZUL4: 78% das AnnounceDate batem com Data_Publicacao (faltando 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:66: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(np.linalg.lstsq(x, y, rcond=None)[0][0])\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:83: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:66: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(np.linalg.lstsq(x, y, rcond=None)[0][0])\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:83: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:66: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(np.linalg.lstsq(x, y, rcond=None)[0][0])\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:83: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:66: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(np.linalg.lstsq(x, y, rcond=None)[0][0])\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:83: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:66: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(np.linalg.lstsq(x, y, rcond=None)[0][0])\n",
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_25292\\3127715514.py:83: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n"
     ]
    }
   ],
   "source": [
    "# validate_pead_outputs.py\n",
    "import os, re, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Ajuste caminhos se precisar\n",
    "PEAD_DATASET = \"dataset/final/pead_event_dataset_2010_2019.csv\"\n",
    "FINAL_DIR    = \"dataset/final\"\n",
    "FUND_DIR     = \"dataset/fundamental\"\n",
    "PRICES_DIR   = \"dataset/prices\"\n",
    "IBOV_CSV     = \"dataset/prices/IBOV.SA.csv\"\n",
    "CDI_CSV      = \"dataset/prices/CDI.SA.csv\"\n",
    "\n",
    "# ---------- helpers leves (iguais aos do pipeline) ----------\n",
    "def to_float_smart(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nd\",\"nan\",\"none\"}: return np.nan\n",
    "    # tenta detectar formato com vÃ­rgula decimal\n",
    "    if \",\" in s and s.count(\",\") == 1 and s.count(\".\") >= 1:\n",
    "        s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    else:\n",
    "        s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n",
    "        if s.count(\".\") > 1:\n",
    "            parts = s.split(\".\")\n",
    "            s = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def load_ibov(path_ibov: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path_ibov, dtype=str)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "    for c in [\"FechAjust\",\"FechHist\"]:\n",
    "        if c in df: df[c] = df[c].apply(to_float_smart)\n",
    "    close = \"FechAjust\" if \"FechAjust\" in df and df[\"FechAjust\"].notna().any() else \"FechHist\"\n",
    "    out = df[[\"Date\", close]].rename(columns={close:\"Close\"}).dropna()\n",
    "    return out.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "def load_cdi(path_cdi: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path_cdi, dtype=str)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "    if \"Var\" in df:\n",
    "        df[\"Var\"] = df[\"Var\"].apply(to_float_smart)\n",
    "        if df[\"Var\"].notna().sum() > 3:\n",
    "            return df[[\"Date\"]].assign(rf_daily=df[\"Var\"]/100.0).dropna().sort_values(\"Date\").reset_index(drop=True)\n",
    "    for c in [\"FechAjust\",\"FechHist\"]:\n",
    "        if c in df:\n",
    "            df[c] = df[c].apply(to_float_smart)\n",
    "            if df[c].notna().sum() > 3:\n",
    "                r = df[c].pct_change()\n",
    "                return df[[\"Date\"]].assign(rf_daily=r).dropna().sort_values(\"Date\").reset_index(drop=True)\n",
    "    return pd.DataFrame(columns=[\"Date\",\"rf_daily\"])\n",
    "\n",
    "def detect_start_index(prices: pd.DataFrame, announce_date: pd.Timestamp) -> int:\n",
    "    # Sem horÃ¡rio (DUR/UNK): t1 = primeiro pregÃ£o >= announce_date\n",
    "    ts = prices['Date'].values\n",
    "    idx_ge = np.searchsorted(ts, np.array(announce_date, dtype='datetime64[ns]'))\n",
    "    return min(idx_ge, len(prices)-1)\n",
    "\n",
    "def estimate_beta(stock_df: pd.DataFrame, mkt_df: pd.DataFrame, rf_df: pd.DataFrame,\n",
    "                  event_idx: int, estimation_window: int = 252) -> float:\n",
    "    m = stock_df[['Date','Close']].merge(mkt_df[['Date','Close']], on='Date', suffixes=('_i','_m'))\n",
    "    m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
    "    m['ri'] = m['Close_i'].pct_change();  m['rm'] = m['Close_m'].pct_change()\n",
    "    if event_idx < 2: return np.nan\n",
    "    event_date = stock_df.iloc[event_idx]['Date']\n",
    "    eidx = m.index[m['Date'] == event_date]\n",
    "    if len(eidx) == 0: return np.nan\n",
    "    eidx = eidx[0]; start = max(m.index.min(), eidx - estimation_window); end = eidx - 1\n",
    "    if end - start < 30: return np.nan\n",
    "    win = m.loc[start:end].dropna()\n",
    "    if win.empty: return np.nan\n",
    "    x = (win['rm'] - win['rf_daily']).values.reshape(-1, 1)\n",
    "    y = (win['ri'] - win['rf_daily']).values.reshape(-1, 1)\n",
    "    return float(np.linalg.lstsq(x, y, rcond=None)[0][0])\n",
    "\n",
    "def compute_car(stock_df: pd.DataFrame, mkt_df: pd.DataFrame, rf_df: pd.DataFrame,\n",
    "                event_idx: int, beta: float, holding_days: int = 30) -> float:\n",
    "    m = stock_df[['Date','Close']].merge(mkt_df[['Date','Close']], on='Date', suffixes=('_i','_m'))\n",
    "    m = m.merge(rf_df[['Date','rf_daily']], on='Date', how='left').fillna(method='ffill').sort_values('Date')\n",
    "    m['ri'] = m['Close_i'].pct_change();  m['rm'] = m['Close_m'].pct_change()\n",
    "    start = event_idx; end = min(start + holding_days - 1, len(m) - 1)\n",
    "    seg = m.iloc[start:end+1].dropna()\n",
    "    if seg.empty: return np.nan\n",
    "    seg['E_ri'] = seg['rf_daily'] + beta * (seg['rm'] - seg['rf_daily'])\n",
    "    seg['AR'] = seg['ri'] - seg['E_ri']\n",
    "    return float(seg['AR'].sum())\n",
    "\n",
    "def find_fund_path_for_tkr(fund_dir: str, tkr: str) -> Path | None:\n",
    "    p = Path(fund_dir) / f\"{tkr}.SA.csv\"\n",
    "    if p.exists(): return p\n",
    "    t_base = re.sub(r\"(11|34|3|4|5|6)$\", \"\", tkr.upper())\n",
    "    cand = Path(fund_dir) / f\"{t_base}.SA.csv\"\n",
    "    if cand.exists(): return cand\n",
    "    matches = list(Path(fund_dir).glob(f\"{t_base}*.csv\"))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "# ---------- (A) INTEGRIDADE DO DATASET ----------\n",
    "print(\"\\n[A] Checando integridade do dataset final...\")\n",
    "df = pd.read_csv(PEAD_DATASET, dtype=str)\n",
    "# normaliza tipos\n",
    "date_col = 'AnnounceDate'\n",
    "df[date_col] = pd.to_datetime(df[date_col], dayfirst=True, errors='coerce')\n",
    "\n",
    "num_cols = [c for c in df.columns if c.startswith('CAR_') or c.startswith('STD_')]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c].apply(to_float_smart), errors='coerce')\n",
    "\n",
    "required = {'Ticker','AnnounceDate','CAR_30D','CAR_Sign'}\n",
    "missing = required - set(df.columns)\n",
    "print(f\" - Colunas obrigatÃ³rias faltando: {missing}\" if missing else \" - Colunas obrigatÃ³rias OK\")\n",
    "\n",
    "null_dates = df['AnnounceDate'].isna().sum()\n",
    "null_cars  = df['CAR_30D'].isna().sum()\n",
    "dups = df.duplicated(['Ticker','AnnounceDate']).sum()\n",
    "print(f\" - Datas nulas: {null_dates} | CAR_30D nulos: {null_cars} | Duplicatas (Ticker,AnnounceDate): {dups}\")\n",
    "\n",
    "print(\" - Stats CAR_30D:\", df['CAR_30D'].describe(percentiles=[.01,.05,.5,.95,.99]))\n",
    "\n",
    "# ---------- (B) SANIDADE DAS FEATURES (Z-SCORES) ----------\n",
    "print(\"\\n[B] Checando z-scores (STD_*)...\")\n",
    "zcols = [c for c in df.columns if c.startswith('STD_')]\n",
    "if not zcols:\n",
    "    print(\" - Nenhuma coluna STD_* encontrada.\")\n",
    "else:\n",
    "    means = df[zcols].mean(numeric_only=True)\n",
    "    stds  = df[zcols].std(ddof=0, numeric_only=True)\n",
    "    worst_mean = means.abs().sort_values(ascending=False).head(5)\n",
    "    worst_std  = (stds - 1).abs().sort_values(ascending=False).head(5)\n",
    "    print(\" - |mean(STD_*)| maiores (esperado ~0):\")\n",
    "    print(worst_mean)\n",
    "    print(\" - |std(STD_*) - 1| maiores (esperado ~0):\")\n",
    "    print(worst_std)\n",
    "\n",
    "    # Red flags de magnitude absurda (provÃ¡vel problema de leitura/locale)\n",
    "    big = (df[zcols].abs() > 8).sum().sort_values(ascending=False)\n",
    "    offenders = big[big > 0]\n",
    "    if len(offenders):\n",
    "        print(\"\\n !!! Red flags: valores |STD_*| > 8 detectados (checar locale/parse). Top problemÃ¡ticos:\")\n",
    "        print(offenders.head(10))\n",
    "    else:\n",
    "        print(\" - Magnitudes de STD_* dentro do esperado (<= 8).\")\n",
    "\n",
    "# ---------- (C) RECONTAGEM DE CAR/Î² PARA UMA AMOSTRA ----------\n",
    "print(\"\\n[C] Recontando CAR/Î² para amostra (confirmaÃ§Ã£o economÃ©trica)...\")\n",
    "mkt = load_ibov(IBOV_CSV)\n",
    "rf  = load_cdi(CDI_CSV)\n",
    "\n",
    "# Escolhe atÃ© 5 eventos aleatÃ³rios (com ticker presente no FINAL_DIR)\n",
    "events = df.dropna(subset=['AnnounceDate']).copy()\n",
    "events['exists_final'] = events['Ticker'].apply(lambda t: Path(FINAL_DIR, f\"{t}.final.csv\").exists())\n",
    "sample = events[events['exists_final']].sample(min(5, len(events[events['exists_final']])) , random_state=42) if len(events[events['exists_final']])>0 else pd.DataFrame()\n",
    "\n",
    "def reload_close_series(tkr: str) -> pd.DataFrame:\n",
    "    f = Path(FINAL_DIR, f\"{tkr}.final.csv\")\n",
    "    d = pd.read_csv(f, parse_dates=['Data'])\n",
    "    d = d[['Data','Close']].rename(columns={'Data':'Date'}).dropna().sort_values('Date')\n",
    "    return d\n",
    "\n",
    "if sample.empty:\n",
    "    print(\" - NÃ£o hÃ¡ amostra elegÃ­vel (confira se existem dataset/final/<TICKER>.final.csv).\")\n",
    "else:\n",
    "    for _, row in sample.iterrows():\n",
    "        tkr = row['Ticker']\n",
    "        ad  = row['AnnounceDate']\n",
    "        car_saved = row['CAR_30D']\n",
    "        try:\n",
    "            px = reload_close_series(tkr)\n",
    "            t1 = detect_start_index(px[['Date']], ad)\n",
    "            beta = estimate_beta(px, mkt, rf, t1, 252)\n",
    "            car  = compute_car(px, mkt, rf, t1, beta, 30)\n",
    "            diff = float(car) - float(car_saved)\n",
    "            print(f\" - {tkr} | {ad.date()} | CAR_saved={car_saved:.6f}  CAR_recalc={car:.6f}  Î”={diff:.6e}  Î²={beta:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ! Falha ao recontar {tkr} {ad.date()}: {e}\")\n",
    "\n",
    "# ---------- (D) COERÃŠNCIA EVENTO x FUNDAMENTO ----------\n",
    "print(\"\\n[D] Conferindo que AnnounceDate âˆˆ Data_Publicacao (amostra por ticker)...\")\n",
    "def publication_set(tkr: str):\n",
    "    p = find_fund_path_for_tkr(FUND_DIR, tkr)\n",
    "    if not p: return set()\n",
    "    fr = pd.read_csv(p, dtype=str)\n",
    "    if 'Data_Publicacao' not in fr: return set()\n",
    "    s = pd.to_datetime(fr['Data_Publicacao'], dayfirst=True, errors='coerce').dropna()\n",
    "    return set(s.dt.normalize().values)\n",
    "\n",
    "for tkr in df['Ticker'].unique()[:5]:  # checa primeiros 5 tickers\n",
    "    pub = publication_set(tkr)\n",
    "    if not pub:\n",
    "        print(f\" - {tkr}: sem Data_Publicacao no CSV de fundamentos.\")\n",
    "        continue\n",
    "    ann = pd.to_datetime(df.loc[df['Ticker']==tkr, 'AnnounceDate']).dt.normalize()\n",
    "    miss = ann[~ann.isin(pd.to_datetime(list(pub)).normalize())]\n",
    "    rate = 1 - (len(miss)/max(1,len(ann)))\n",
    "    print(f\" - {tkr}: {rate:.0%} das AnnounceDate batem com Data_Publicacao (faltando {len(miss)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544130b",
   "metadata": {},
   "source": [
    "#### Testes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ðŸ”§ CÃ©lula A â€” Helpers robustos (NOVA)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def coerce_date_any(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Aceita 'dd/mm/aaaa' e 'aaaa-mm-dd'. Retorna datetime64[ns].\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    d1 = pd.to_datetime(s, format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "    d2 = pd.to_datetime(s, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    d3 = pd.to_datetime(s, errors=\"coerce\")  # fallback\n",
    "    out = d1.fillna(d2).fillna(d3)\n",
    "    return out\n",
    "\n",
    "def coerce_num_br(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converte strings com milhares '.' e decimal ',' para float.\n",
    "    Se jÃ¡ for numÃ©rico, mantÃ©m.\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(x):\n",
    "        return x.astype(float)\n",
    "    s = x.astype(str).str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def read_events_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"LÃª o dataset de eventos com datas robustas.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    if \"AnnounceDate\" in df.columns:\n",
    "        df[\"AnnounceDate\"] = coerce_date_any(df[\"AnnounceDate\"])\n",
    "    if \"EventTradeDate\" in df.columns:\n",
    "        df[\"EventTradeDate\"] = coerce_date_any(df[\"EventTradeDate\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "73eb75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CÃ©lula 1 â€” Splits temporais + Purged K-Fold com embargo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "def year_splits(df: pd.DataFrame,\n",
    "                date_col: str = \"EventTradeDate\",\n",
    "                train_end: int = 2016,\n",
    "                val_end: int = 2018) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split temporal fixo: Treino=<=train_end, Val=(train_end, val_end], Test=>val_end.\n",
    "    Retorna arrays de Ã­ndices (posiÃ§Ãµes no df recebido).\n",
    "    \"\"\"\n",
    "    y = pd.to_datetime(df[date_col]).dt.year\n",
    "    i_train = np.where(y <= train_end)[0]\n",
    "    i_val   = np.where((y > train_end) & (y <= val_end))[0]\n",
    "    i_test  = np.where(y > val_end)[0]\n",
    "    return i_train, i_val, i_test\n",
    "\n",
    "def _intervals_from_events(dates: pd.Series, hold: int = 30) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Para cada amostra, define o intervalo [T1, T1+hold-1] em segundos UNIX.\n",
    "    Usado para 'purge' de sobreposiÃ§Ã£o.\n",
    "    \"\"\"\n",
    "    d = pd.to_datetime(dates).dt.floor(\"D\").astype(\"int64\") // 10**9\n",
    "    start = d.values\n",
    "    end   = d.values + 60*60*24*(hold-1)  # (hold-1) dias corridos (aprox conservadora)\n",
    "    return np.vstack([start, end]).T  # shape (n,2)\n",
    "\n",
    "def purged_kfold(df: pd.DataFrame,\n",
    "                 n_splits: int = 5,\n",
    "                 date_col: str = \"EventTradeDate\",\n",
    "                 holding_days: int = 30,\n",
    "                 embargo_days: int = 30) -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    K-fold temporal com 'purge' (remove do treino eventos que colidem com a janela de teste)\n",
    "    e 'embargo' (remove eventos imediatamente apÃ³s a janela de teste).\n",
    "    Retorna tuplas (idx_treino, idx_teste) como Ã­ndices do df original.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "    order = np.argsort(pd.to_datetime(df[date_col]).values)\n",
    "    n = len(df)\n",
    "    fold_edges = np.linspace(0, n, n_splits+1, dtype=int)\n",
    "\n",
    "    # janelas [start,end] em segundos\n",
    "    intervals = _intervals_from_events(df[date_col], hold=holding_days)\n",
    "\n",
    "    for k in range(n_splits):\n",
    "        lo, hi = fold_edges[k], fold_edges[k+1]\n",
    "        test_idx = order[lo:hi]\n",
    "        train_idx = np.setdiff1d(order, test_idx, assume_unique=False)\n",
    "\n",
    "        # janela agregada de teste\n",
    "        t_lo = intervals[test_idx, 0].min()\n",
    "        t_hi = intervals[test_idx, 1].max()\n",
    "\n",
    "        # embargo pÃ³s-teste\n",
    "        emb_hi = t_hi + 60*60*24*embargo_days\n",
    "\n",
    "        # mantÃ©m no treino apenas quem NÃƒO intersecta [t_lo, emb_hi]\n",
    "        keep = []\n",
    "        for i in train_idx:\n",
    "            s, e = intervals[i]\n",
    "            # Sem interseÃ§Ã£o se totalmente antes OU totalmente depois do embargo\n",
    "            if (e < t_lo) or (s > emb_hi):\n",
    "                keep.append(i)\n",
    "\n",
    "        train_idx_purged = np.array(keep, dtype=int)\n",
    "        yield np.sort(train_idx_purged), np.sort(test_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "269dc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ðŸ§  CÃ©lula 2 â€” prepare_xy / XGBoost / CV (SUBSTITUA A SUA)\n",
    "import os, json\n",
    "from typing import Dict, Tuple, List\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def prepare_xy(events_csv: str,\n",
    "               target_mode: str = \"cls\",\n",
    "               min_estimation: int = 60,\n",
    "               drop_na_target: bool = True) -> Tuple[pd.DataFrame, pd.Series, List[str]]:\n",
    "    df = read_events_csv(events_csv)  # << usa parser robusto\n",
    "    if \"EstimationLen\" in df.columns:\n",
    "        df = df[df[\"EstimationLen\"] >= min_estimation].copy()\n",
    "\n",
    "    # alvo\n",
    "    if target_mode == \"cls\":\n",
    "        y = (df[\"CAR_30D\"] > 0).astype(int)\n",
    "    else:\n",
    "        y = df[\"CAR_30D\"].astype(float)\n",
    "\n",
    "    # features = apenas STD_* (evita vazamento)\n",
    "    feats = [c for c in df.columns if c.startswith(\"STD_\")]\n",
    "    X = df[feats].copy().fillna(0.0)\n",
    "\n",
    "    if drop_na_target:\n",
    "        m = ~y.isna()\n",
    "        X, y = X.loc[m].reset_index(drop=True), y.loc[m].reset_index(drop=True)\n",
    "\n",
    "    # colunas auxiliares\n",
    "    X[\"EventTradeDate\"] = df.loc[X.index, \"EventTradeDate\"].values\n",
    "    X[\"Ticker\"] = df.loc[X.index, \"Ticker\"].values\n",
    "    return X, y, feats\n",
    "\n",
    "def xgb_default_params(mode: str) -> Dict:\n",
    "    if mode == \"cls\":\n",
    "        return dict(\n",
    "            n_estimators=800, learning_rate=0.03, max_depth=4,\n",
    "            subsample=0.8, colsample_bytree=0.8, min_child_weight=5,\n",
    "            reg_lambda=1.0, gamma=0.0, random_state=RANDOM_STATE,\n",
    "            objective=\"binary:logistic\", eval_metric=\"auc\"\n",
    "        )\n",
    "    else:\n",
    "        return dict(\n",
    "            n_estimators=800, learning_rate=0.03, max_depth=4,\n",
    "            subsample=0.8, colsample_bytree=0.8, min_child_weight=5,\n",
    "            reg_lambda=1.0, gamma=0.0, random_state=RANDOM_STATE,\n",
    "            objective=\"reg:squarederror\", eval_metric=\"rmse\"\n",
    "        )\n",
    "\n",
    "# === Purged K-Fold com embargo (mesma assinatura de antes) ===\n",
    "def year_splits(df: pd.DataFrame, date_col: str = \"EventTradeDate\", train_end: int = 2016, val_end: int = 2018):\n",
    "    y = pd.to_datetime(df[date_col]).dt.year\n",
    "    i_train = np.where(y <= train_end)[0]\n",
    "    i_val   = np.where((y > train_end) & (y <= val_end))[0]\n",
    "    i_test  = np.where(y > val_end)[0]\n",
    "    return i_train, i_val, i_test\n",
    "\n",
    "def _intervals_from_events(dates: pd.Series, hold: int = 30) -> np.ndarray:\n",
    "    d = coerce_date_any(dates).dt.floor(\"D\").astype(\"int64\") // 10**9\n",
    "    start = d.values\n",
    "    end   = d.values + 60*60*24*(hold-1)\n",
    "    return np.vstack([start, end]).T\n",
    "\n",
    "def purged_kfold(df: pd.DataFrame,\n",
    "                 n_splits: int = 5,\n",
    "                 date_col: str = \"EventTradeDate\",\n",
    "                 holding_days: int = 30,\n",
    "                 embargo_days: int = 30):\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "    order = np.argsort(coerce_date_any(df[date_col]).values)\n",
    "    n = len(df)\n",
    "    fold_edges = np.linspace(0, n, n_splits+1, dtype=int)\n",
    "    intervals = _intervals_from_events(df[date_col], hold=holding_days)\n",
    "    for k in range(n_splits):\n",
    "        lo, hi = fold_edges[k], fold_edges[k+1]\n",
    "        test_idx = order[lo:hi]\n",
    "        train_idx = np.setdiff1d(order, test_idx, assume_unique=False)\n",
    "        t_lo = intervals[test_idx, 0].min()\n",
    "        t_hi = intervals[test_idx, 1].max()\n",
    "        emb_hi = t_hi + 60*60*24*embargo_days\n",
    "        keep = []\n",
    "        for i in train_idx:\n",
    "            s, e = intervals[i]\n",
    "            if (e < t_lo) or (s > emb_hi):\n",
    "                keep.append(i)\n",
    "        train_idx_purged = np.array(keep, dtype=int)\n",
    "        yield np.sort(train_idx_purged), np.sort(test_idx)\n",
    "\n",
    "def cv_purged_scores(X: pd.DataFrame, y: pd.Series,\n",
    "                     mode: str = \"cls\",\n",
    "                     n_splits: int = 5,\n",
    "                     holding_days: int = 30,\n",
    "                     embargo_days: int = 30):\n",
    "    params = xgb_default_params(mode)\n",
    "    model = XGBClassifier(**params) if mode == \"cls\" else XGBRegressor(**params)\n",
    "    metrics = {}\n",
    "    for fold, (itr, ite) in enumerate(\n",
    "        purged_kfold(X.assign(EventTradeDate=X[\"EventTradeDate\"]),\n",
    "                     n_splits=n_splits,\n",
    "                     date_col=\"EventTradeDate\",\n",
    "                     holding_days=holding_days,\n",
    "                     embargo_days=embargo_days), 1\n",
    "    ):\n",
    "        Xtr = X.iloc[itr].drop(columns=[\"EventTradeDate\",\"Ticker\"])\n",
    "        Xte = X.iloc[ite].drop(columns=[\"EventTradeDate\",\"Ticker\"])\n",
    "        ytr, yte = y.iloc[itr], y.iloc[ite]\n",
    "        model.fit(Xtr, ytr)\n",
    "        if mode == \"cls\":\n",
    "            proba = model.predict_proba(Xte)[:,1]\n",
    "            auc = roc_auc_score(yte, proba)\n",
    "            metrics[fold] = float(auc)\n",
    "        else:\n",
    "            pred = model.predict(Xte)\n",
    "            rmse = float(np.sqrt(mean_squared_error(yte, pred)))\n",
    "            metrics[fold] = rmse\n",
    "    avg = float(np.mean(list(metrics.values()))) if metrics else float(\"nan\")\n",
    "    return avg, metrics\n",
    "\n",
    "def train_holdout_and_save(X: pd.DataFrame, y: pd.Series, feats: List[str],\n",
    "                           out_dir: str = \"models\",\n",
    "                           mode: str = \"cls\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    i_tr, i_va, i_te = year_splits(X, date_col=\"EventTradeDate\", train_end=2016, val_end=2018)\n",
    "    Xtr, Xva, Xte = X.iloc[i_tr], X.iloc[i_va], X.iloc[i_te]\n",
    "    ytr, yva, yte = y.iloc[i_tr], y.iloc[i_va], y.iloc[i_te]\n",
    "    params = xgb_default_params(mode)\n",
    "    model = XGBClassifier(**params) if mode == \"cls\" else XGBRegressor(**params)\n",
    "    Xtv = pd.concat([Xtr, Xva]).drop(columns=[\"EventTradeDate\",\"Ticker\"])\n",
    "    ytv = pd.concat([ytr, yva])\n",
    "    Xte2 = Xte.drop(columns=[\"EventTradeDate\",\"Ticker\"])\n",
    "    model.fit(Xtv, ytv)\n",
    "    if mode == \"cls\":\n",
    "        proba = model.predict_proba(Xte2)[:,1]\n",
    "        auc = roc_auc_score(y.iloc[i_te], proba)\n",
    "        acc = accuracy_score(y.iloc[i_te], (proba>=0.5).astype(int))\n",
    "        test_metrics = {\"AUC\": float(auc), \"ACC\": float(acc)}\n",
    "    else:\n",
    "        pred = model.predict(Xte2)\n",
    "        rmse = float(np.sqrt(mean_squared_error(y.iloc[i_te], pred)))\n",
    "        r2   = float(r2_score(y.iloc[i_te], pred))\n",
    "        test_metrics = {\"RMSE\": rmse, \"R2\": r2}\n",
    "    imp = pd.DataFrame({\"feature\": feats, \"gain\": model.feature_importances_}).sort_values(\"gain\", ascending=False)\n",
    "    model.save_model(os.path.join(out_dir, f\"xgb_{mode}.json\"))\n",
    "    imp.to_csv(os.path.join(out_dir, f\"feature_importance_{mode}.csv\"), index=False)\n",
    "    with open(os.path.join(out_dir, f\"test_metrics_{mode}.json\"), \"w\") as f:\n",
    "        json.dump(test_metrics, f, indent=2)\n",
    "    return test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "75b90b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ðŸ“ˆ CÃ©lula 3 â€” Event Study (SUBSTITUA A SUA)\n",
    "import os, math\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def load_market_rf(mkt_csv: str, rf_csv: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    mkt = pd.read_csv(mkt_csv)\n",
    "    mkt[\"Date\"]  = coerce_date_any(mkt[\"Data\"])\n",
    "    mkt[\"Close\"] = coerce_num_br(mkt[\"FechAjust\"])\n",
    "    mkt = mkt.dropna(subset=[\"Date\",\"Close\"])[[\"Date\",\"Close\"]].sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    rf = pd.read_csv(rf_csv)\n",
    "    rf[\"Date\"] = coerce_date_any(rf[\"Data\"])\n",
    "    if \"rf_daily\" not in rf.columns:\n",
    "        if \"Var\" in rf.columns:\n",
    "            rf[\"rf_daily\"] = coerce_num_br(rf[\"Var\"])  # CDI diÃ¡rio (decimal)\n",
    "        else:\n",
    "            # fallback neutro (0) se nÃ£o houver taxa diÃ¡ria; ajuste se tiver outra coluna apropriada\n",
    "            rf[\"rf_daily\"] = 0.0\n",
    "    rf = rf.dropna(subset=[\"Date\"])[[\"Date\",\"rf_daily\"]].sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # *garante* dtype datetime64\n",
    "    mkt[\"Date\"] = pd.to_datetime(mkt[\"Date\"], errors=\"coerce\")\n",
    "    rf[\"Date\"]  = pd.to_datetime(rf[\"Date\"],  errors=\"coerce\")\n",
    "    return mkt, rf\n",
    "\n",
    "def _merge_stock_mkt_rf(px: pd.DataFrame, mkt: pd.DataFrame, rf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # px: Data, Close_i\n",
    "    px = px.copy()\n",
    "    px[\"Date\"]   = pd.to_datetime(px[\"Date\"], errors=\"coerce\")\n",
    "    px[\"Close_i\"]= coerce_num_br(px[\"Close_i\"]) if not pd.api.types.is_numeric_dtype(px[\"Close_i\"]) else px[\"Close_i\"]\n",
    "    px = px.dropna(subset=[\"Date\",\"Close_i\"])\n",
    "\n",
    "    # garantir tipos\n",
    "    m = (px.merge(mkt, on=\"Date\", suffixes=(\"_i\",\"_m\"))\n",
    "            .merge(rf, on=\"Date\", how=\"left\")\n",
    "            .sort_values(\"Date\")\n",
    "            .reset_index(drop=True))\n",
    "    m[\"rf_daily\"] = m[\"rf_daily\"].fillna(method=\"ffill\").fillna(0.0)\n",
    "    m[\"ri\"] = m[\"Close_i\"].pct_change()\n",
    "    m[\"rm\"] = m[\"Close\"].pct_change()\n",
    "    return m\n",
    "\n",
    "def _estimate_beta_and_sigma(m: pd.DataFrame, event_idx: int, est_len: int) -> Tuple[float, float, pd.DataFrame]:\n",
    "    start = max(0, event_idx - est_len)\n",
    "    end   = event_idx - 1\n",
    "    win = m.iloc[start:end+1].dropna(subset=[\"ri\",\"rm\",\"rf_daily\"]).copy()\n",
    "    if len(win) < 30:\n",
    "        return np.nan, np.nan, win\n",
    "    x = (win[\"rm\"] - win[\"rf_daily\"]).values.reshape(-1,1)\n",
    "    y = (win[\"ri\"] - win[\"rf_daily\"]).values.reshape(-1,1)\n",
    "    beta = np.linalg.lstsq(x, y, rcond=None)[0].ravel()[0]\n",
    "    win[\"E_ri\"] = win[\"rf_daily\"] + beta * (win[\"rm\"] - win[\"rf_daily\"])\n",
    "    win[\"eps\"]  = (win[\"ri\"] - win[\"E_ri\"])\n",
    "    sigma = float(win[\"eps\"].std(ddof=1))\n",
    "    return float(beta), sigma, win\n",
    "\n",
    "def _event_window_ar(m: pd.DataFrame, event_idx: int, beta: float, L: int = 30) -> pd.Series:\n",
    "    seg = m.iloc[event_idx:event_idx+L].dropna(subset=[\"ri\",\"rm\",\"rf_daily\"]).copy()\n",
    "    if seg.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    seg[\"E_ri\"] = seg[\"rf_daily\"] + beta * (seg[\"rm\"] - seg[\"rf_daily\"])\n",
    "    seg[\"AR\"]   = seg[\"ri\"] - seg[\"E_ri\"]\n",
    "    seg[\"t\"]    = range(len(seg))\n",
    "    return seg.set_index(\"t\")[\"AR\"]\n",
    "\n",
    "def aar_caar_by_quintile(events_csv: str,\n",
    "                         final_dir: str,\n",
    "                         mkt_csv: str,\n",
    "                         rf_csv: str,\n",
    "                         signal_col: str = \"STD_EPS_EarningsSurprise\",\n",
    "                         holding_days: int = 30,\n",
    "                         estimation_window: int = 252,\n",
    "                         min_estimation: int = 60) -> Dict[str, pd.DataFrame]:\n",
    "    mkt, rf = load_market_rf(mkt_csv, rf_csv)\n",
    "    ev = read_events_csv(events_csv)  # robusto\n",
    "    ev = ev.dropna(subset=[\"EventTradeDate\"]).copy()\n",
    "\n",
    "    if signal_col not in ev.columns:\n",
    "        signal_col = \"STD_LPA\" if \"STD_LPA\" in ev.columns else ev.filter(like=\"EarningsSurprise\").columns[0]\n",
    "\n",
    "    ev[\"Year\"] = ev[\"EventTradeDate\"].dt.year\n",
    "    ev[\"Q\"] = ev.groupby(\"Year\")[signal_col].transform(lambda s: pd.qcut(s.rank(method=\"first\"), 5, labels=False)+1)\n",
    "\n",
    "    out = {}\n",
    "    for q in [1,2,3,4,5]:\n",
    "        sub = ev[ev[\"Q\"]==q].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        ars, sars = [], []\n",
    "        for _, r in sub.iterrows():\n",
    "            tkr = r[\"Ticker\"]\n",
    "            t1  = r[\"EventTradeDate\"]\n",
    "\n",
    "            p = os.path.join(final_dir, f\"{tkr}.final.csv\")\n",
    "            if not os.path.exists(p):\n",
    "                continue\n",
    "            px = pd.read_csv(p)\n",
    "            px[\"Date\"]    = coerce_date_any(px[\"Data\"])\n",
    "            px[\"Close_i\"] = coerce_num_br(px[\"Close\"])\n",
    "            px = px.dropna(subset=[\"Date\",\"Close_i\"])[[\"Date\",\"Close_i\"]].sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "            m = _merge_stock_mkt_rf(px, mkt, rf)\n",
    "            ds = m[\"Date\"].values\n",
    "            eidx = int(np.searchsorted(ds, np.array(pd.Timestamp(t1), dtype=\"datetime64[ns]\")))\n",
    "            est_len = min(estimation_window, eidx)\n",
    "            if est_len < min_estimation:\n",
    "                continue\n",
    "\n",
    "            beta, sigma, _ = _estimate_beta_and_sigma(m, eidx, est_len)\n",
    "            if not np.isfinite(beta) or not np.isfinite(sigma) or sigma==0:\n",
    "                continue\n",
    "            ar = _event_window_ar(m, eidx, beta, L=holding_days)\n",
    "            if ar.empty:\n",
    "                continue\n",
    "            ars.append(ar)\n",
    "            sars.append(ar / sigma)\n",
    "\n",
    "        if not ars:\n",
    "            continue\n",
    "\n",
    "        L = holding_days\n",
    "        M_ar   = pd.DataFrame({i: s.reindex(range(L)) for i, s in enumerate(ars)})\n",
    "        M_sar  = pd.DataFrame({i: s.reindex(range(L)) for i, s in enumerate(sars)})\n",
    "\n",
    "        AAR  = M_ar.mean(axis=1, skipna=True).rename(\"AAR\")\n",
    "        CAAR = AAR.cumsum().rename(\"CAAR\")\n",
    "\n",
    "        n_t = M_ar.count(axis=1).astype(float)\n",
    "\n",
    "        # Patell\n",
    "        Z_pat = (M_sar.mean(axis=1) * np.sqrt(n_t)).rename(\"PatellZ\")\n",
    "        P_pat = Z_pat.apply(lambda z: 2*(1-0.5*(1+math.erf(abs(z)/np.sqrt(2)))))  # two-sided\n",
    "\n",
    "        # BMP\n",
    "        std_cs = M_ar.std(axis=1, ddof=1)\n",
    "        T_bmp  = (AAR / (std_cs / np.sqrt(n_t))).replace([np.inf, -np.inf], np.nan).rename(\"BMP_t\")\n",
    "        P_bmp  = T_bmp.apply(lambda z: 2*(1-0.5*(1+math.erf(abs(z)/np.sqrt(2)))))  # approx normal\n",
    "\n",
    "        # Corrado (rank)\n",
    "        ranks = M_ar.rank(axis=1, method=\"average\")\n",
    "        Z_cor = ((ranks - (1 + ranks.shape[1])/2.0) / ranks.std(axis=1, ddof=1)).mean(axis=1)\n",
    "        Z_cor = (Z_cor * np.sqrt(n_t)).rename(\"CorradoZ\")\n",
    "        P_cor = Z_cor.apply(lambda z: 2*(1-0.5*(1+math.erf(abs(z)/np.sqrt(2)))))\n",
    "\n",
    "        out[f\"Q{q}\"] = pd.concat([AAR, CAAR, Z_pat, P_pat, T_bmp, P_bmp, Z_cor, P_cor], axis=1)\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "06eb8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CÃ©lula 4 â€” Backtest simples (equal-weight, custos, filtros)\n",
    "import os, math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class BTConfig:\n",
    "    events_csv: str = \"dataset/final/pead_event_dataset_2010_2019.csv\"\n",
    "    final_dir: str  = \"dataset/final\"\n",
    "    out_dir: str    = \"dataset/backtest\"\n",
    "    holding_days: int = 30\n",
    "    quantile: float = 0.2\n",
    "    long_short: bool = False\n",
    "    signal_col: str = \"STD_EPS_EarningsSurprise\"\n",
    "    slippage_bps: float = 5.0\n",
    "    commission_bps: float = 1.0\n",
    "    min_price: float = 1.0\n",
    "    vol_filter_quantile: float = 0.1\n",
    "    use_market_hedge: bool = False  # manter False por enquanto\n",
    "\n",
    "def _ensure_dir(d: str):\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _resolve_signal_col(df: pd.DataFrame, pref: str) -> str:\n",
    "    if pref in df.columns:\n",
    "        return pref\n",
    "    if \"STD_LPA\" in df.columns:\n",
    "        return \"STD_LPA\"\n",
    "    cand = [c for c in df.columns if c.startswith(\"STD_EPS\") or c.endswith(\"EarningsSurprise\")]\n",
    "    if cand: return cand[0]\n",
    "    raise ValueError(\"Nenhuma coluna de sinal encontrada (ex.: STD_EPS_EarningsSurprise ou STD_LPA).\")\n",
    "\n",
    "def _load_price_final(ticker: str, final_dir: str) -> pd.DataFrame:\n",
    "    p = Path(final_dir) / f\"{ticker}.final.csv\"\n",
    "    if not p.exists(): return pd.DataFrame()\n",
    "    px = pd.read_csv(p, parse_dates=[\"Data\"], dayfirst=True)\n",
    "    px = px.sort_values(\"Data\").dropna(subset=[\"Close\"]).reset_index(drop=True)\n",
    "    return px\n",
    "\n",
    "def _entry_exit_indices(px: pd.DataFrame, t1: pd.Timestamp, holding: int) -> Optional[Tuple[int, int]]:\n",
    "    if px.empty: return None\n",
    "    ds = px[\"Data\"].values\n",
    "    idx = np.searchsorted(ds, np.array(t1, dtype=\"datetime64[ns]\"))\n",
    "    if idx >= len(px): return None\n",
    "    idx_entry = int(idx)\n",
    "    idx_exit  = int(min(idx_entry + holding - 1, len(px) - 1))\n",
    "    return idx_entry, idx_exit\n",
    "\n",
    "def _apply_roundtrip_cost(ret_series: pd.Series, roundtrip_bps: float) -> pd.Series:\n",
    "    if ret_series.empty or roundtrip_bps <= 0: return ret_series\n",
    "    r = ret_series.copy()\n",
    "    r.iloc[0] = r.iloc[0] - (roundtrip_bps / 10000.0)\n",
    "    return r\n",
    "\n",
    "def _drawdown_stats(series: pd.Series) -> Tuple[float, float]:\n",
    "    if series.empty: return 0.0, 0.0\n",
    "    eq = (1.0 + series.fillna(0)).cumprod()\n",
    "    peak = eq.cummax()\n",
    "    dd = (eq / peak) - 1.0\n",
    "    maxdd = dd.min()\n",
    "    end = dd.idxmin()\n",
    "    start = (eq.loc[:end].idxmax() if end in eq.index else eq.idxmax())\n",
    "    duration = (pd.Timestamp(end) - pd.Timestamp(start)).days if (isinstance(end, pd.Timestamp) and isinstance(start, pd.Timestamp)) else 0\n",
    "    return float(maxdd), float(duration)\n",
    "\n",
    "def build_trades_and_pnl(cfg: BTConfig) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float]]:\n",
    "    _ensure_dir(cfg.out_dir)\n",
    "    ev = pd.read_csv(cfg.events_csv, parse_dates=[\"AnnounceDate\", \"EventTradeDate\"], dayfirst=True)\n",
    "    ev = ev.sort_values([\"EventTradeDate\", \"Ticker\"]).reset_index(drop=True)\n",
    "\n",
    "    sigcol = _resolve_signal_col(ev, cfg.signal_col)\n",
    "    ev[\"Year\"] = ev[\"EventTradeDate\"].dt.year\n",
    "\n",
    "    def _label_side(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        q_hi = g[sigcol].quantile(1 - cfg.quantile)\n",
    "        q_lo = g[sigcol].quantile(cfg.quantile)\n",
    "        if cfg.long_short:\n",
    "            g[\"Side\"] = np.where(g[sigcol] >= q_hi, 1, np.where(g[sigcol] <= q_lo, -1, 0))\n",
    "        else:\n",
    "            g[\"Side\"] = np.where(g[sigcol] >= q_hi, 1, 0)\n",
    "        return g\n",
    "\n",
    "    ev = ev.groupby(\"Year\", group_keys=False).apply(_label_side)\n",
    "    ev = ev[ev[\"Side\"] != 0].reset_index(drop=True)\n",
    "\n",
    "    roundtrip_bps = 2.0 * (cfg.slippage_bps + cfg.commission_bps)\n",
    "    trade_rows = []\n",
    "    pnl_rows: Dict[pd.Timestamp, List[float]] = {}\n",
    "\n",
    "    for _, row in ev.iterrows():\n",
    "        tkr = row[\"Ticker\"]\n",
    "        t1  = row[\"EventTradeDate\"]\n",
    "        side = int(row[\"Side\"])\n",
    "        beta = float(row[\"Beta\"]) if \"Beta\" in row and not pd.isna(row[\"Beta\"]) else np.nan\n",
    "\n",
    "        px = _load_price_final(tkr, cfg.final_dir)\n",
    "        if px.empty: continue\n",
    "\n",
    "        idxs = _entry_exit_indices(px, t1, cfg.holding_days)\n",
    "        if idxs is None: continue\n",
    "        i_entry, i_exit = idxs\n",
    "\n",
    "        px_entry = float(px.iloc[i_entry][\"Close\"])\n",
    "        if px_entry < cfg.min_price: continue\n",
    "        if \"Vol\" in px.columns:\n",
    "            v_hist = px[\"Vol\"].dropna()\n",
    "            v_thr = v_hist.quantile(cfg.vol_filter_quantile) if not v_hist.empty else 0.0\n",
    "            if float(px.iloc[i_entry].get(\"Vol\", v_thr)) < float(v_thr):\n",
    "                continue\n",
    "\n",
    "        seg = px.loc[i_entry:i_exit, [\"Data\", \"Close\"]].copy().reset_index(drop=True)\n",
    "        seg[\"ret\"] = seg[\"Close\"].pct_change()\n",
    "        seg = seg.dropna().reset_index(drop=True)  # comeÃ§a em T1+1\n",
    "        seg[\"ret\"] = _apply_roundtrip_cost(seg[\"ret\"], roundtrip_bps)\n",
    "        seg[\"ret\"] = side * seg[\"ret\"]\n",
    "\n",
    "        for _, r in seg.iterrows():\n",
    "            d = pd.Timestamp(r[\"Data\"])\n",
    "            pnl_rows.setdefault(d, []).append(float(r[\"ret\"]))\n",
    "\n",
    "        gross = float(seg[\"ret\"].sum() + (roundtrip_bps/10000.0 if side != 0 else 0.0))\n",
    "        net   = float(seg[\"ret\"].sum())\n",
    "        trade_rows.append({\n",
    "            \"Ticker\": tkr,\n",
    "            \"AnnounceDate\": row.get(\"AnnounceDate\"),\n",
    "            \"EventTradeDate\": t1,\n",
    "            \"EntryDate\": seg[\"Data\"].iloc[0] - pd.Timedelta(days=1),\n",
    "            \"ExitDate\": seg[\"Data\"].iloc[-1],\n",
    "            \"Side\": side,\n",
    "            \"Signal\": float(row[sigcol]),\n",
    "            \"EntryPrice\": px_entry,\n",
    "            \"GrossRet\": gross,\n",
    "            \"NetRet\": net,\n",
    "            \"Beta\": beta if not math.isnan(beta) else None\n",
    "        })\n",
    "\n",
    "    if not pnl_rows:\n",
    "        raise RuntimeError(\"Nenhum trade vÃ¡lido foi gerado. Verifique sinal, liquidez e cobertura de preÃ§os.\")\n",
    "\n",
    "    pnl_df = (pd.DataFrame([{\"Date\": d, \"PortRet\": np.mean(v)} for d, v in pnl_rows.items()]\n",
    "        ).sort_values(\"Date\").reset_index(drop=True))\n",
    "\n",
    "    daily = pnl_df[\"PortRet\"].values\n",
    "    ann_factor = 252.0\n",
    "    avg = np.nanmean(daily)\n",
    "    std = np.nanstd(daily, ddof=0)\n",
    "    cagr = float((1.0 + pnl_df[\"PortRet\"]).prod() ** (ann_factor / len(pnl_df)) - 1.0) if len(pnl_df) > 0 else 0.0\n",
    "    sharpe = float(np.sqrt(ann_factor) * avg / std) if std > 0 else 0.0\n",
    "    maxdd, dd_dur = _drawdown_stats(pnl_df.set_index(\"Date\")[\"PortRet\"])\n",
    "    hit = float((pnl_df[\"PortRet\"] > 0).mean())\n",
    "\n",
    "    metrics = {\"N_trades\": len(trade_rows), \"Daily_mean\": avg, \"Daily_std\": std,\n",
    "               \"Sharpe\": sharpe, \"CAGR\": cagr, \"MaxDD\": maxdd, \"MaxDD_days\": dd_dur, \"Hit_ratio\": hit}\n",
    "\n",
    "    _ensure_dir(cfg.out_dir)\n",
    "    trades_df = pd.DataFrame(trade_rows)\n",
    "    trades_df.to_csv(Path(cfg.out_dir) / \"pead_backtest_trades.csv\", index=False)\n",
    "    pnl_out = pnl_df.copy()\n",
    "    pnl_out[\"Equity\"] = (1.0 + pnl_out[\"PortRet\"]).cumprod()\n",
    "    pnl_out.to_csv(Path(cfg.out_dir) / \"pead_backtest_daily_pnl.csv\", index=False)\n",
    "\n",
    "    return trades_df, pnl_out, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a037479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos padrÃ£o\n",
    "EVENTS = \"dataset/final/pead_event_dataset_2010_2019.csv\"\n",
    "FINAL_DIR = \"dataset/final\"\n",
    "IBOV = \"dataset/prices/IBOV.SA.csv\"\n",
    "CDI  = \"dataset/prices/CDI.SA.csv\"\n",
    "\n",
    "# ===== 1) Modelagem com Purged K-Fold =====\n",
    "X, y, feats = prepare_xy(EVENTS, target_mode=\"cls\", min_estimation=60)\n",
    "avg_auc, auc_by_fold = cv_purged_scores(X, y, mode=\"cls\", n_splits=5, holding_days=30, embargo_days=30)\n",
    "print(\"[CV Purged] AUC mÃ©dio:\", avg_auc, \"| por fold:\", auc_by_fold)\n",
    "\n",
    "# ===== 2) Holdout temporal (Test=2019) =====\n",
    "metrics_test = train_holdout_and_save(X, y, feats, out_dir=\"models\", mode=\"cls\")\n",
    "print(\"[Holdout 2019] MÃ©tricas:\", metrics_test)\n",
    "\n",
    "# ===== 3) Event study por quintis (AAR/CAAR + Patell/BMP/Corrado) =====\n",
    "out = aar_caar_by_quintile(\n",
    "    events_csv=EVENTS,\n",
    "    final_dir=FINAL_DIR,\n",
    "    mkt_csv=IBOV,\n",
    "    rf_csv=CDI,\n",
    "    signal_col=\"STD_EPS_EarningsSurprise\",\n",
    "    holding_days=30,\n",
    "    estimation_window=252,\n",
    "    min_estimation=60\n",
    ")\n",
    "import os\n",
    "os.makedirs(\"dataset/event_study\", exist_ok=True)\n",
    "for k, dfq in out.items():\n",
    "    dfq.to_csv(f\"dataset/event_study/aar_caar_tests_{k}.csv\", index_label=\"t\")\n",
    "    print(f\"[EventStudy] {k} salvo com shape {dfq.shape}\")\n",
    "\n",
    "# ===== 4) Backtest com custos e filtros =====\n",
    "cfg = BTConfig(\n",
    "    events_csv=EVENTS,\n",
    "    final_dir=FINAL_DIR,\n",
    "    out_dir=\"dataset/backtest\",\n",
    "    holding_days=30,\n",
    "    quantile=0.2,\n",
    "    long_short=False,\n",
    "    signal_col=\"STD_EPS_EarningsSurprise\",\n",
    "    slippage_bps=5,\n",
    "    commission_bps=1,\n",
    "    min_price=1.0,\n",
    "    vol_filter_quantile=0.1,\n",
    "    use_market_hedge=False\n",
    ")\n",
    "trades, pnl, met = build_trades_and_pnl(cfg)\n",
    "print(\"[Backtest] MÃ©tricas:\", met)\n",
    "display(trades.head())\n",
    "display(pnl.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0df3f",
   "metadata": {},
   "source": [
    "## Teste 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959af6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SeÃ§Ã£o \"Teste\" (versÃ£o aprimorada)\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Warnings menos verbosos (mantÃ©m erros reais visÃ­veis)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Parsing dates in %Y-%m-%d\")\n",
    "\n",
    "# 1) Caminhos padrÃ£o\n",
    "EVENTS_RAW = \"dataset/final/pead_event_dataset_2010_2019.csv\"\n",
    "FINAL_DIR  = \"dataset/final\"\n",
    "IBOV       = \"dataset/prices/IBOV.SA.csv\"\n",
    "CDI        = \"dataset/prices/CDI.SA.csv\"\n",
    "\n",
    "# 2) Carrega e SANEIA eventos (datas e colunas essenciais)\n",
    "def load_and_sanitize_events(path: str) -> pd.DataFrame:\n",
    "    # ForÃ§a leitura com dayfirst=False (formato ISO YYYY-MM-DD)\n",
    "    ev = pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=[\"AnnounceDate\", \"EventTradeDate\"],\n",
    "        dayfirst=False\n",
    "    )\n",
    "    # Normaliza nomes esperados\n",
    "    required_cols = [\"Ticker\", \"EventTradeDate\"]\n",
    "    for c in required_cols:\n",
    "        if c not in ev.columns:\n",
    "            raise ValueError(f\"[Eventos] Coluna obrigatÃ³ria ausente: {c}\")\n",
    "\n",
    "    # Garante tipos corretos\n",
    "    ev[\"Ticker\"] = ev[\"Ticker\"].astype(str).str.strip()\n",
    "    ev = ev.dropna(subset=[\"Ticker\", \"EventTradeDate\"]).copy()\n",
    "\n",
    "    # Cria coluna Year (se nÃ£o existir)\n",
    "    if \"Year\" not in ev.columns:\n",
    "        ev[\"Year\"] = ev[\"EventTradeDate\"].dt.year\n",
    "\n",
    "    # Opcional: assert mÃ­nimo de janelas de estimaÃ§Ã£o, se existir a coluna\n",
    "    if \"EstimationLen\" in ev.columns:\n",
    "        ev = ev.loc[ev[\"EstimationLen\"].fillna(0) >= 60].copy()\n",
    "\n",
    "    return ev\n",
    "\n",
    "ev = load_and_sanitize_events(EVENTS_RAW)\n",
    "\n",
    "# 3) PrÃ©-filtro de ELEGIBILIDADE para o BACKTEST\n",
    "#    MantÃ©m apenas eventos cujo ticker tem preÃ§os no intervalo [T1, T2]\n",
    "#    usando os CSVs de preÃ§os por ticker em: dataset/prices_processed/<TICKER>.csv\n",
    "HOLDING_DAYS = 30\n",
    "PRICES_DIR   = \"dataset/prices_processed\"  # ajuste se seu repositÃ³rio for outro (ex.: dataset/prices)\n",
    "\n",
    "def load_prices_for_ticker(tkr: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Tenta abrir dataset/prices_processed/<TICKER>.csv.\n",
    "    Aceita 'Date' ou 'Data' como coluna de data e vÃ¡rias colunas de preÃ§o.\n",
    "    Retorna DF com ['Date','Adj Close','Close','WClose'] quando possÃ­vel.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(PRICES_DIR, f\"{tkr}.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Detecta coluna de data\n",
    "    date_col = \"Date\" if \"Date\" in df.columns else (\"Data\" if \"Data\" in df.columns else None)\n",
    "    if date_col is None:\n",
    "        return None\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[date_col]).sort_values(date_col)\n",
    "    df = df.rename(columns={date_col: \"Date\"})\n",
    "    return df\n",
    "\n",
    "def has_window_prices(tkr: str, t1: pd.Timestamp, t2: pd.Timestamp) -> bool:\n",
    "    px = load_prices_for_ticker(tkr)\n",
    "    if px is None:\n",
    "        return False\n",
    "    # Verifica se hÃ¡ ao menos 1 pregÃ£o no intervalo\n",
    "    mask = (px[\"Date\"] >= t1) & (px[\"Date\"] <= t2)\n",
    "    return bool(mask.any())\n",
    "\n",
    "def filter_events_with_prices(ev: pd.DataFrame, holding_days: int) -> pd.DataFrame:\n",
    "    kept = []\n",
    "    miss_cnt = 0\n",
    "    for _, row in ev.iterrows():\n",
    "        tkr = str(row[\"Ticker\"]).strip()\n",
    "        t0  = pd.to_datetime(row[\"EventTradeDate\"])\n",
    "        # ConvenÃ§Ã£o: entrar em T+1 e carregar por HOLDING_DAYS\n",
    "        t1  = t0 + pd.Timedelta(days=1)\n",
    "        t2  = t1 + pd.Timedelta(days=holding_days - 1)\n",
    "        if has_window_prices(tkr, t1, t2):\n",
    "            kept.append(row)\n",
    "        else:\n",
    "            miss_cnt += 1\n",
    "    kept = pd.DataFrame(kept).reset_index(drop=True)\n",
    "    print(f\"[Backtest Pre-filter] Eventos elegÃ­veis: {len(kept)} | ignorados por janela sem preÃ§o: {miss_cnt}\")\n",
    "    return kept\n",
    "\n",
    "ev_bt = filter_events_with_prices(ev, HOLDING_DAYS)\n",
    "\n",
    "# Salva uma cÃ³pia SANEADA e FILTRADA para o backtest\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "EVENTS_CLEAN = os.path.join(FINAL_DIR, \"pead_event_dataset_2010_2019_sanitized.csv\")\n",
    "EVENTS_BT    = os.path.join(FINAL_DIR, \"pead_event_dataset_2010_2019_sanitized_backtest.csv\")\n",
    "ev.to_csv(EVENTS_CLEAN, index=False)\n",
    "ev_bt.to_csv(EVENTS_BT, index=False)\n",
    "\n",
    "# 4) ===== 1) Modelagem com Purged K-Fold =====\n",
    "# Nota: usamos o EVENTS_CLEAN (datas saneadas), nÃ£o o RAW\n",
    "X, y, feats = prepare_xy(EVENTS_CLEAN, target_mode=\"cls\", min_estimation=60)\n",
    "print(f\"[Target balance] n={len(y)} | %positivos={100.0 * (y==1).mean():.1f}% | %negativos={100.0 * (y==0).mean():.1f}%\")\n",
    "\n",
    "avg_auc, auc_by_fold = cv_purged_scores(\n",
    "    X, y, mode=\"cls\", n_splits=5, holding_days=HOLDING_DAYS, embargo_days=30\n",
    ")\n",
    "print(\"[CV Purged] AUC mÃ©dio:\", avg_auc, \"| por fold:\", auc_by_fold)\n",
    "\n",
    "# 5) ===== 2) Holdout temporal (Test=2019) =====\n",
    "metrics_test = train_holdout_and_save(X, y, feats, out_dir=\"models\", mode=\"cls\")\n",
    "print(\"[Holdout 2019] MÃ©tricas:\", metrics_test)\n",
    "\n",
    "# 6) ===== 3) Event study por quintis (AAR/CAAR + Patell/BMP/Corrado) =====\n",
    "out = aar_caar_by_quintile(\n",
    "    events_csv=EVENTS_CLEAN,          # usa o saneado p/ evitar warning de dayfirst\n",
    "    final_dir=FINAL_DIR,\n",
    "    mkt_csv=IBOV,\n",
    "    rf_csv=CDI,\n",
    "    signal_col=\"STD_EPS_EarningsSurprise\",\n",
    "    holding_days=HOLDING_DAYS,\n",
    "    estimation_window=252,\n",
    "    min_estimation=60\n",
    ")\n",
    "os.makedirs(\"dataset/event_study\", exist_ok=True)\n",
    "for k, dfq in out.items():\n",
    "    dfq.to_csv(f\"dataset/event_study/aar_caar_tests_{k}.csv\", index_label=\"t\")\n",
    "    print(f\"[EventStudy] {k} salvo com shape {dfq.shape}\")\n",
    "\n",
    "# 7) ===== 4) Backtest com custos e filtros =====\n",
    "# Aqui usamos o EVENTS_BT (prÃ©-filtrado para garantir que cada evento tem preÃ§os na janela).\n",
    "cfg = BTConfig(\n",
    "    events_csv=EVENTS_BT,\n",
    "    final_dir=FINAL_DIR,\n",
    "    out_dir=\"dataset/backtest\",\n",
    "    holding_days=HOLDING_DAYS,\n",
    "    quantile=0.2,\n",
    "    long_short=False,\n",
    "    signal_col=\"STD_EPS_EarningsSurprise\",\n",
    "    slippage_bps=5,\n",
    "    commission_bps=1,\n",
    "    min_price=1.0,\n",
    "    vol_filter_quantile=0.1,\n",
    "    use_market_hedge=False\n",
    ")\n",
    "\n",
    "os.makedirs(\"dataset/backtest\", exist_ok=True)\n",
    "try:\n",
    "    trades, pnl, met = build_trades_and_pnl(cfg)\n",
    "except IndexError as e:\n",
    "    # Em caso de alguma ocorrÃªncia residual, relaxa o filtro de volume e re-tenta\n",
    "    print(f\"[Backtest] IndexError detectado ({e}). Re-tentando com filtros mais brandos...\")\n",
    "    cfg = BTConfig(\n",
    "        events_csv=EVENTS_BT,\n",
    "        final_dir=FINAL_DIR,\n",
    "        out_dir=\"dataset/backtest\",\n",
    "        holding_days=HOLDING_DAYS,\n",
    "        quantile=0.2,\n",
    "        long_short=False,\n",
    "        signal_col=\"STD_EPS_EarningsSurprise\",\n",
    "        slippage_bps=5,\n",
    "        commission_bps=1,\n",
    "        min_price=0.5,            # relaxa\n",
    "        vol_filter_quantile=0.0,  # desliga corte de liquidez\n",
    "        use_market_hedge=False\n",
    "    )\n",
    "    trades, pnl, met = build_trades_and_pnl(cfg)\n",
    "\n",
    "print(\"[Backtest] MÃ©tricas:\", met)\n",
    "display(trades.head())\n",
    "display(pnl.tail())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-ml)",
   "language": "python",
   "name": "fin-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
