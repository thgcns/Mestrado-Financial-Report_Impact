{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "# =====================\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97775160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils\n",
    "# -------------------------\n",
    "# Utils de I/O e diretórios\n",
    "# -------------------------\n",
    "\n",
    "def ensure_dirs(paths):\n",
    "    \"\"\"\n",
    "    Garante que todas as pastas em 'paths' existem.\n",
    "    paths pode ser lista de strings ou uma única string.\n",
    "    \"\"\"\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "def to_float_smart(x):\n",
    "    \"\"\"\n",
    "    Converte strings tipo '1.234,56' (BR) ou '1,234.56' (US) e variantes em float.\n",
    "    Trata negativos e milhares. Retorna NaN se não der.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if s in {\"\", \"-\", \"--\", \"nan\", \"NaN\", \"None\", \"NULL\"}:\n",
    "        return np.nan\n",
    "\n",
    "    # mantém apenas dígitos, sinais e separadores\n",
    "    s = re.sub(r\"[^0-9\\-\\.,]\", \"\", s)\n",
    "\n",
    "    has_dot   = \".\" in s\n",
    "    has_comma = \",\" in s\n",
    "\n",
    "    try:\n",
    "        if has_dot and has_comma:\n",
    "            # decide pelo separador mais à direita\n",
    "            if s.rfind(\",\") > s.rfind(\".\"):\n",
    "                # BR: 1.234,56 -> 1234.56\n",
    "                s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            else:\n",
    "                # US: 1,234.56 -> 1234.56\n",
    "                s = s.replace(\",\", \"\")\n",
    "            return float(s)\n",
    "\n",
    "        if has_comma and not has_dot:\n",
    "            # BR decimal: 1234,56 -> 1234.56\n",
    "            return float(s.replace(\",\", \".\"))\n",
    "\n",
    "        if has_dot and not has_comma:\n",
    "            # Pode ser decimal (um ponto) ou milhares (vários pontos)\n",
    "            if s.count(\".\") == 1:\n",
    "                return float(s)  # 1234.56\n",
    "            else:\n",
    "                # 109.641.290.194 -> 109641290194\n",
    "                return float(s.replace(\".\", \"\"))\n",
    "\n",
    "        # Só dígitos e talvez sinal\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "#def _parse_br_friendly_date_series(series: pd.Series) -> pd.Series:    \n",
    "#    \"\"\"\n",
    "#    Converte a coluna de datas de publicação para datetime,\n",
    "#    assumindo SEMPRE padrão brasileiro (DD/MM/AAAA) quando houver ambiguidade.\n",
    "#\n",
    "#    Regras:\n",
    "#    - Se estiver no padrão ISO 'YYYY-MM-DD', usamos isso direto (não é ambíguo).\n",
    "#    - Se estiver no padrão brasileiro 'DD/MM/YYYY', interpretamos como dia/mês/ano.\n",
    "#    - Se vier em qualquer outro formato, tentamos parse com dayfirst=True.\n",
    "#    - No final, retornamos datetime normalizado (sem hora).\n",
    "#    \"\"\"\n",
    "#\n",
    "#    s = series.astype(str).str.strip()\n",
    "#\n",
    "#    # 1) tenta ISO claro: 2024-03-31\n",
    "#    iso_mask = s.str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "#    out_iso = pd.to_datetime(\n",
    "#        s.where(iso_mask),\n",
    "#        format=\"%Y-%m-%d\",\n",
    "#        errors=\"coerce\"\n",
    "#    )\n",
    "#\n",
    "#    # 2) tenta BR claro: 31/03/2024\n",
    "#    br_mask = s.str.match(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n",
    "#    out_br = pd.to_datetime(\n",
    "#        s.where(br_mask),\n",
    "#        format=\"%d/%m/%Y\",\n",
    "#        dayfirst=True,\n",
    "#        errors=\"coerce\"\n",
    "#    )\n",
    "#\n",
    "#    # 3) começa com ISO e preenche lacunas com BR\n",
    "#    out = out_iso.fillna(out_br)\n",
    "#\n",
    "#    # 4) fallback genérico:\n",
    "#    #    qualquer coisa que sobrou a gente interpreta assumindo padrão brasileiro (dayfirst=True)\n",
    "#    still_nat = out.isna()\n",
    "#    if still_nat.any():\n",
    "#        out_fallback = pd.to_datetime(\n",
    "#            s[still_nat],\n",
    "#            errors=\"coerce\",\n",
    "#            dayfirst=True   # <- força semântica brasileira\n",
    "#        )\n",
    "#        out.loc[still_nat] = out_fallback\n",
    "#\n",
    "#    # 5) normaliza para \"apenas a data\" (zera hora)\n",
    "#    out = out.dt.normalize()\n",
    "#\n",
    "#    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2905cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataPrepFund\n",
    "\n",
    "class DataPrepFund:\n",
    "    def __init__(self,\n",
    "                 prices_dir: str = \"dataset/prices_processed\",\n",
    "                 fund_dir: str = \"dataset/fundamental\",\n",
    "                 out_fund_dir: str = \"dataset/fund_processed\",\n",
    "                 out_final_dir: str = \"dataset/final\"):\n",
    "        self.prices_dir = prices_dir\n",
    "        self.fund_dir = fund_dir\n",
    "        self.out_fund_dir = out_fund_dir\n",
    "        self.out_final_dir = out_final_dir\n",
    "        ensure_dirs([prices_dir, fund_dir, out_fund_dir, out_final_dir])\n",
    "\n",
    "    @staticmethod\n",
    "    def _ticker_from_price_filename(fname: str) -> str:\n",
    "        # \"AZUL4.SA.csv\" -> \"AZUL4\"\n",
    "        base = os.path.basename(fname)\n",
    "        if base.endswith(\".csv\"):\n",
    "            base = base[:-4]\n",
    "        return base.replace(\".SA\", \"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _fund_path_for_ticker(fund_dir: str, ticker: str) -> Optional[str]:\n",
    "        # procura \"<TICKER>_fUND.csv\" (case-insensitive)\n",
    "        for fn in os.listdir(fund_dir):\n",
    "            if fn.lower() == f\"{ticker.lower()}_fund.csv\" or fn.lower() == f\"{ticker.lower()}_fundamental.csv\" or fn.lower() == f\"{ticker.lower()}_fund.csv\":\n",
    "                return os.path.join(fund_dir, fn)\n",
    "            if fn.lower().startswith(ticker.lower()) and \"fund\" in fn.lower():\n",
    "                return os.path.join(fund_dir, fn)\n",
    "        return None\n",
    "\n",
    "    def process_one(self, price_csv_path: str,\n",
    "                    indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,\n",
    "                    attach_fundamentals_asof: bool = True,\n",
    "                    only_events: bool = False) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Processa um ticker:\n",
    "          - preço -> retornos/STD/indicadores\n",
    "          - evento -> flag 0/1 (Data ∈ Data_Publicacao)\n",
    "          - (opcional) merge_asof com fundamentos publicados\n",
    "          - salva CSV final\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- fundamentos (se houver)\n",
    "        fund_path = self._fund_path_for_pricefile(self.fund_dir, price_csv_path)\n",
    "        df_price_feat = pd.read_csv(self.prices_dir, sep=None, engine=\"python\", dtype=str)\n",
    "        \n",
    "        if fund_path is None:\n",
    "            # sem fundamentos -> apenas marca evento=0 e finaliza\n",
    "            df_final = df_price_feat.copy()\n",
    "            df_final[\"event\"] = 0\n",
    "        else:\n",
    "            dff_raw = pd.read_csv(fund_path, sep=None, engine=\"python\", dtype=str)\n",
    "            fund = FundamentalProcessing(dff_raw, tkr)\n",
    "            pub_dates = set(pd.to_datetime(fund.get_publication_dates(), errors=\"coerce\").dropna().values)\n",
    "            df_final = df_price_feat.copy()\n",
    "            # flag de evento\n",
    "            df_final[\"event\"] = df_final[\"Data\"].isin(pub_dates).astype(int)\n",
    "\n",
    "            # (opcional) merge_asof colando fundamentos até a próxima publicação\n",
    "            if attach_fundamentals_asof:\n",
    "                f_asof = fund.features_for_asof_merge()\n",
    "                if not f_asof.empty:\n",
    "                    df_final = df_final.sort_values(\"Data\")\n",
    "                    f_asof = f_asof.sort_values(\"Data_Publicacao\")\n",
    "                    df_final = pd.merge_asof(\n",
    "                        df_final,\n",
    "                        f_asof,\n",
    "                        left_on=\"Data\",\n",
    "                        right_on=\"Data_Publicacao\",\n",
    "                        direction=\"backward\"\n",
    "                    )\n",
    "                    # remove coluna-âncora para não poluir\n",
    "                    if \"Data_Publicacao\" in df_final.columns:\n",
    "                        df_final.drop(columns=[\"Data_Publicacao\"], inplace=True)\n",
    "\n",
    "            if only_events:\n",
    "                df_final = df_final[df_final[\"event\"] == 1].copy()\n",
    "\n",
    "        # salva final por ticker\n",
    "        out_final_path = os.path.join(self.out_final_dir, f\"{tkr}.final.csv\")\n",
    "        df_final.to_csv(out_final_path, index=False)\n",
    "        return df_final\n",
    "\n",
    "    def process_all(self,\n",
    "                    indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,\n",
    "                    attach_fundamentals_asof: bool = True,\n",
    "                    only_events: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Processa todos os arquivos em dataset/prices e devolve consolidado.\"\"\"\n",
    "        all_final = []\n",
    "        for fn in os.listdir(self.prices_dir):\n",
    "            if not fn.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            try:\n",
    "                path = os.path.join(self.prices_dir, fn)\n",
    "                df_final = self.process_one(\n",
    "                    path,\n",
    "                    indicator_factor=indicator_factor,\n",
    "                    save_intermediate_prices=save_intermediate_prices,\n",
    "                    attach_fundamentals_asof=attach_fundamentals_asof,\n",
    "                    only_events=only_events\n",
    "                )\n",
    "                if df_final is not None and not df_final.empty:\n",
    "                    all_final.append(df_final.assign(Ticker=self._ticker_from_price_filename(fn)))\n",
    "            except Exception as ex:\n",
    "                print(f\"Erro no ticker de {fn}: {ex}\")\n",
    "                continue\n",
    "\n",
    "        if not all_final:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df_all = pd.concat(all_final, ignore_index=True)\n",
    "        df_all = df_all.sort_values([\"Ticker\", \"Data\"]).reset_index(drop=True)\n",
    "        # salva consolidado\n",
    "        df_all.to_csv(os.path.join(self.out_final_dir, \"final_dataprep.csv\"), index=False)\n",
    "        return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6df98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FundamentalProcessing:\n",
    "    \"\"\"\n",
    "    Limpa e estrutura o DataFrame de fundamentos de um único ticker\n",
    "    e gera também métricas derivadas (QoQ, YoY, EPS surprise).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_fund: pd.DataFrame, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.df = df_fund.copy()\n",
    "\n",
    "        # 1. Padronizar datas principais\n",
    "        for col in [\"Data_Publicacao\", \"Data_Demonstracao\", \"Data_Analise\"]:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = pd.to_datetime(self.df[col], dayfirst=True, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "        #        self.df[col] = _parse_br_friendly_date_series(self.df[col])\n",
    "\n",
    "        # 2. Converter colunas numéricas relevantes\n",
    "        num_cols = [\n",
    "            \"RL\",\"LL\",\"EBITDA\",\"Preco_Abertura\",\"Preco_Fechamento\",\"LPA\",\"ROA\",\"ROE\",\"MEB\",\n",
    "            \"CRESC_RL_12M\",\"CRESC_LL_12M\",\"CRESC_EBITDA_12M\",\"CAPEX\",\"FCO\",\"FCF\",\n",
    "            \"Divida_Liquida\",\"PL\",\"Divida_Bruta\",\"AT\",\"DVA_Despesas_Fin\",\n",
    "            \"PC\",\"PNC\",\"Outros_PC\",\"LUB\"\n",
    "        ]\n",
    "        for c in num_cols:\n",
    "            if c in self.df.columns:\n",
    "                self.df[c] = self.df[c].apply(to_float_smart)\n",
    "\n",
    "        # 3. Resolver duplicadas (RL_dup*, EBITDA_dup etc.)\n",
    "        if set([\"RL\",\"RL_dup1\",\"RL_dup2\"]).issubset(self.df.columns):\n",
    "            self.df[\"RL\"] = (\n",
    "                self.df[[\"RL\",\"RL_dup1\",\"RL_dup2\"]]\n",
    "                .bfill(axis=1)\n",
    "                .iloc[:,0]\n",
    "            )\n",
    "            self.df.drop(columns=[\"RL_dup1\",\"RL_dup2\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        if set([\"EBITDA\",\"EBITDA_dup\"]).issubset(self.df.columns):\n",
    "            self.df[\"EBITDA\"] = (\n",
    "                self.df[[\"EBITDA\",\"EBITDA_dup\"]]\n",
    "                .bfill(axis=1)\n",
    "                .iloc[:,0]\n",
    "            )\n",
    "            self.df.drop(columns=[\"EBITDA_dup\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        # 4. Definir QuarterEnd (fim do trimestre contábil)\n",
    "        if \"Data_Demonstracao\" in self.df.columns:\n",
    "            self.df[\"QuarterEnd\"] = self.df[\"Data_Demonstracao\"]\n",
    "        else:\n",
    "            self.df[\"QuarterEnd\"] = pd.NaT\n",
    "\n",
    "        if self.df[\"QuarterEnd\"].isna().all() and \"Data_Analise\" in self.df.columns:\n",
    "            self.df[\"QuarterEnd\"] = self.df[\"Data_Analise\"]\n",
    "\n",
    "        # 5. Ordenar e deduplicar por QuarterEnd\n",
    "        sort_cols = []\n",
    "        if \"QuarterEnd\" in self.df.columns:\n",
    "            sort_cols.append(\"QuarterEnd\")\n",
    "        if \"Data_Publicacao\" in self.df.columns:\n",
    "            sort_cols.append(\"Data_Publicacao\")\n",
    "\n",
    "        if sort_cols:\n",
    "            self.df = (\n",
    "                self.df.sort_values(sort_cols)\n",
    "                       .drop_duplicates([\"QuarterEnd\"])\n",
    "                       .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "        # 6. Forçar coluna Ticker logo no início\n",
    "        self.df.insert(0, \"Ticker\", self.ticker)\n",
    "\n",
    "    def get_publication_dates(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Retorna as datas de publicação em string DD/MM/AAAA,\n",
    "        para ser usada na marcação de 'event' nos preços.\n",
    "        \"\"\"\n",
    "        if \"Data_Publicacao\" not in self.df.columns:\n",
    "            return pd.Series([], dtype=\"object\")\n",
    "\n",
    "        pub = self.df[\"Data_Publicacao\"].dropna()\n",
    "        pub = pd.to_datetime(pub, errors=\"coerce\", dayfirst=True).dropna()\n",
    "        pub_str = pub.dt.strftime(\"%d/%m/%Y\")\n",
    "        return pub_str.reset_index(drop=True)\n",
    "\n",
    "    def build_qoq_yoy_and_eps(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cria tabela enriquecida com:\n",
    "        - métricas contábeis numéricas\n",
    "        - variação QoQ e YoY\n",
    "        - EPS surprise\n",
    "        Retorna uma tabela alinhada em Data_Publicacao / QuarterEnd.\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "\n",
    "        if \"Data_Publicacao\" not in df.columns:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        out = df[[\"Ticker\", \"Data_Publicacao\", \"QuarterEnd\"]].copy()\n",
    "\n",
    "        # pega colunas numéricas\n",
    "        num_cols = [\n",
    "            c for c in df.columns\n",
    "            if pd.api.types.is_numeric_dtype(df[c])\n",
    "        ]\n",
    "\n",
    "        for c in num_cols:\n",
    "            out[c] = df[c].values\n",
    "            out[f\"{c}_Q_Change\"] = df[c].diff(1).values        # variação QoQ\n",
    "            out[f\"{c}_Y_Change\"] = (df[c] - df[c].shift(4)).values  # variação YoY (~mesmo tri ano passado)\n",
    "\n",
    "        # EPS Surprise\n",
    "        if \"EPS_Consensus\" in df.columns and \"LPA\" in df.columns:\n",
    "            eps = df[\"LPA\"] - df[\"EPS_Consensus\"]\n",
    "        elif \"LPA\" in df.columns:\n",
    "            eps = df[\"LPA\"].diff(1)\n",
    "        else:\n",
    "            eps = pd.Series([np.nan] * len(df))\n",
    "\n",
    "        out[\"EPS_EarningsSurprise\"] = eps\n",
    "        out[\"EPS_Earnings_Surprise_Backward_Diff\"] = eps - eps.shift(1)\n",
    "        out[\"EPS_Earnings_Surprise_Backward_Ave_Diff\"] = eps - eps.shift(3).rolling(3).mean()\n",
    "\n",
    "        # ordenar e limpar\n",
    "        out = (\n",
    "            out.dropna(subset=[\"Data_Publicacao\"])\n",
    "               .sort_values(\"Data_Publicacao\")\n",
    "               .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # renomear Data_Publicacao -> AnnounceDate (mais típico no seu pipeline)\n",
    "        out = out.rename(columns={\"Data_Publicacao\": \"AnnounceDate\"})\n",
    "\n",
    "        return out\n",
    "\n",
    "    def build_final_single_table(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Junta as colunas limpas (self.df) com as métricas derivadas\n",
    "        (QoQ/YoY/EPS surprise) em UMA tabela final por ticker.\n",
    "        A junção é feita por ['Ticker','QuarterEnd','AnnounceDate' (~Data_Publicacao)].\n",
    "        \"\"\"\n",
    "        base_df = self.df.copy()\n",
    "\n",
    "        # garantir que as chaves existam\n",
    "        if \"Data_Publicacao\" not in base_df.columns:\n",
    "            base_df[\"Data_Publicacao\"] = pd.NaT\n",
    "        if \"QuarterEnd\" not in base_df.columns:\n",
    "            base_df[\"QuarterEnd\"] = pd.NaT\n",
    "\n",
    "        # tabela derivada\n",
    "        metrics_df = self.build_qoq_yoy_and_eps()\n",
    "\n",
    "        if metrics_df.empty:\n",
    "            # se não conseguiu gerar métricas (ex falta Data_Publicacao),\n",
    "            # simplesmente retorna base_df como final\n",
    "            final_df = base_df.copy()\n",
    "            # manter sem poluir com colunas que não existem\n",
    "        else:\n",
    "            # vamos juntar o base_df com metrics_df\n",
    "            # base_df tem Data_Publicacao; metrics_df tem AnnounceDate.\n",
    "            tmp_base = base_df.copy()\n",
    "            tmp_base = tmp_base.rename(columns={\"Data_Publicacao\": \"AnnounceDate\"})\n",
    "\n",
    "            # chave de merge: Ticker + QuarterEnd + AnnounceDate\n",
    "            final_df = pd.merge(\n",
    "                tmp_base,\n",
    "                metrics_df,\n",
    "                on=[\"Ticker\", \"QuarterEnd\", \"AnnounceDate\"],\n",
    "                how=\"left\",\n",
    "                suffixes=(\"\", \"_MET\")\n",
    "            )\n",
    "\n",
    "        # reordena por QuarterEnd/AnnounceDate\n",
    "        sort_cols = []\n",
    "        if \"QuarterEnd\" in final_df.columns:\n",
    "            sort_cols.append(\"QuarterEnd\")\n",
    "        if \"AnnounceDate\" in final_df.columns:\n",
    "            sort_cols.append(\"AnnounceDate\")\n",
    "\n",
    "        if sort_cols:\n",
    "            final_df = (\n",
    "                final_df.sort_values(sort_cols)\n",
    "                        .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "        return final_df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# utilitário pra mapear arquivo -> ticker base\n",
    "# ------------------------\n",
    "\n",
    "def _ticker_from_fund_filename(fname: str) -> str:\n",
    "    \"\"\"\n",
    "    \"ABEV3.SA.csv\" -> \"ABEV3\"\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname)\n",
    "    if base.endswith(\".csv\"):\n",
    "        base = base[:-4]\n",
    "    return base.replace(\".SA\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a828f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrepFund:\n",
    "    \"\"\"\n",
    "    Lê todos os arquivos em dataset/fundamental/,\n",
    "    processa cada um,\n",
    "    gera UMA única tabela final por ticker (fundamentos limpos + métricas QoQ/YoY/EPS),\n",
    "    salva essa tabela única em dataset/fund_processed/<TICKER>.SA.csv,\n",
    "    e retorna um concat com todos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 fund_dir: str = \"dataset/fundamental\",\n",
    "                 out_fund_dir: str = \"dataset/fund_processed\"):\n",
    "        self.fund_dir = fund_dir\n",
    "        self.out_fund_dir = out_fund_dir\n",
    "        ensure_dirs([fund_dir, out_fund_dir])\n",
    "\n",
    "    def process_one(self, fund_csv_path: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Processa um único arquivo bruto de fundamentos e salva o resultado final único.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            raw = pd.read_csv(fund_csv_path, sep=None, engine=\"python\", dtype=str)\n",
    "\n",
    "            tkr = _ticker_from_fund_filename(fund_csv_path)\n",
    "\n",
    "            fp = FundamentalProcessing(raw, tkr)\n",
    "\n",
    "            final_df = fp.build_final_single_table()\n",
    "\n",
    "            # salva APENAS um arquivo final por ticker\n",
    "            out_path = os.path.join(self.out_fund_dir, f\"{tkr}.SA.csv\")\n",
    "            final_df.to_csv(out_path, index=False)\n",
    "\n",
    "            return final_df\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Erro ao processar {fund_csv_path}: {ex}\")\n",
    "            return None\n",
    "\n",
    "    def process_all(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Processa todos os CSVs em self.fund_dir e devolve concat completo.\n",
    "        \"\"\"\n",
    "        all_final = []\n",
    "\n",
    "        for fn in os.listdir(self.fund_dir):\n",
    "            if not fn.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            full_path = os.path.join(self.fund_dir, fn)\n",
    "\n",
    "            df_final = self.process_one(full_path)\n",
    "\n",
    "            if df_final is not None and not df_final.empty:\n",
    "                all_final.append(df_final)\n",
    "\n",
    "        if not all_final:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df_all = pd.concat(all_final, ignore_index=True)\n",
    "        return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb59f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fundamentals tratados salvos (1 arquivo por ticker) em dataset/fund_processed/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ensure_dirs([\n",
    "        \"dataset/fundamental\",\n",
    "        \"dataset/fund_processed\",\n",
    "    ])\n",
    "\n",
    "    fund_pipeline = DataPrepFund(\n",
    "        fund_dir=\"dataset/fundamental\",\n",
    "        out_fund_dir=\"dataset/fund_processed\"\n",
    "    )\n",
    "\n",
    "    df_all_fund = fund_pipeline.process_all()\n",
    "\n",
    "    print(\"Fundamentals tratados salvos (1 arquivo por ticker) em dataset/fund_processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-ml)",
   "language": "python",
   "name": "fin-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
