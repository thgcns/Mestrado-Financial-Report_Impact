{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "038fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "# =====================\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import enum\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas_ta as ta\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f356fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils\n",
    "# -------------------------\n",
    "# Utils de I/O e diret√≥rios\n",
    "# -------------------------\n",
    "\n",
    "DEFAULT_DIRS = [\"dataset\", \"dataset/prices\", \"dataset/prices_processed\",\n",
    "                \"dataset/fundamental\", \"dataset/final\"]\n",
    "\n",
    "def ensure_dirs(paths: List[str] = DEFAULT_DIRS) -> None:\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Utils de parsing num√©rico\n",
    "# -------------------------\n",
    "\n",
    "#def _parse_publication_dates(series: pd.Series) -> pd.Series:\n",
    "#    \"\"\"\n",
    "#    Converte a coluna de datas de publica√ß√£o para datetime,\n",
    "#    assumindo SEMPRE padr√£o brasileiro (DD/MM/AAAA) quando houver ambiguidade.\n",
    "#\n",
    "#    Regras:\n",
    "#    - Se estiver no padr√£o ISO 'YYYY-MM-DD', usamos isso direto (n√£o √© amb√≠guo).\n",
    "#    - Se estiver no padr√£o brasileiro 'DD/MM/YYYY', interpretamos como dia/m√™s/ano.\n",
    "#    - Se vier em qualquer outro formato, tentamos parse com dayfirst=True.\n",
    "#    - No final, retornamos datetime normalizado (sem hora).\n",
    "#    \"\"\"\n",
    "#\n",
    "#    s = series.astype(str).str.strip()\n",
    "#\n",
    "#    # 1) tenta ISO claro: 2024-03-31\n",
    "#    iso_mask = s.str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "#    out_iso = pd.to_datetime(\n",
    "#        s.where(iso_mask),\n",
    "#        format=\"%Y-%m-%d\",\n",
    "#        errors=\"coerce\"\n",
    "#    )\n",
    "#\n",
    "#    # 2) tenta BR claro: 31/03/2024\n",
    "#    br_mask = s.str.match(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n",
    "#    out_br = pd.to_datetime(\n",
    "#        s.where(br_mask),\n",
    "#        format=\"%d/%m/%Y\",\n",
    "#        dayfirst=True,\n",
    "#        errors=\"coerce\"\n",
    "#    )\n",
    "#\n",
    "#    # 3) come√ßa com ISO e preenche lacunas com BR\n",
    "#    out = out_iso.fillna(out_br)\n",
    "#\n",
    "#    # 4) fallback gen√©rico:\n",
    "#    #    qualquer coisa que sobrou a gente interpreta assumindo padr√£o brasileiro (dayfirst=True)\n",
    "#    still_nat = out.isna()\n",
    "#    if still_nat.any():\n",
    "#        out_fallback = pd.to_datetime(\n",
    "#            s[still_nat],\n",
    "#            errors=\"coerce\",\n",
    "#            dayfirst=True   # <- for√ßa sem√¢ntica brasileira\n",
    "#        )\n",
    "#        out.loc[still_nat] = out_fallback\n",
    "#\n",
    "#    # 5) normaliza para \"apenas a data\" (zera hora)\n",
    "#    out = out.dt.normalize()\n",
    "#\n",
    "#    return out\n",
    "\n",
    "def to_float_smart(x):\n",
    "    \"\"\"\n",
    "    Converte strings tipo '1.234,56' (BR) ou '1,234.56' (US) e variantes em float.\n",
    "    Trata negativos e milhares. Retorna NaN se n√£o der.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if s in {\"\", \"-\", \"--\", \"nan\", \"NaN\", \"None\", \"NULL\"}:\n",
    "        return np.nan\n",
    "\n",
    "    # mant√©m apenas d√≠gitos, sinais e separadores\n",
    "    s = re.sub(r\"[^0-9\\-\\.,]\", \"\", s)\n",
    "\n",
    "    has_dot   = \".\" in s\n",
    "    has_comma = \",\" in s\n",
    "\n",
    "    try:\n",
    "        if has_dot and has_comma:\n",
    "            # decide pelo separador mais √† direita\n",
    "            if s.rfind(\",\") > s.rfind(\".\"):\n",
    "                # BR: 1.234,56 -> 1234.56\n",
    "                s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            else:\n",
    "                # US: 1,234.56 -> 1234.56\n",
    "                s = s.replace(\",\", \"\")\n",
    "            return float(s)\n",
    "\n",
    "        if has_comma and not has_dot:\n",
    "            # BR decimal: 1234,56 -> 1234.56\n",
    "            return float(s.replace(\",\", \".\"))\n",
    "\n",
    "        if has_dot and not has_comma:\n",
    "            # Pode ser decimal (um ponto) ou milhares (v√°rios pontos)\n",
    "            if s.count(\".\") == 1:\n",
    "                return float(s)  # 1234.56\n",
    "            else:\n",
    "                # 109.641.290.194 -> 109641290194\n",
    "                return float(s.replace(\".\", \"\"))\n",
    "\n",
    "        # S√≥ d√≠gitos e talvez sinal\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "def to_int_smart(x) -> float:\n",
    "    \"\"\"Converte texto para inteiro removendo n√£o-d√≠gitos.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan\n",
    "    s = re.sub(r\"[^0-9\\-]\", \"\", str(x))\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    " \n",
    "def catalog_return(row, x, name_return):\n",
    "    val = row.get(name_return, np.nan)\n",
    "    std = row.get(f\"Rolling_std_{name_return}\", np.nan)\n",
    "    if pd.isna(val) or pd.isna(std) or std == 0: return 0\n",
    "    if val > x * std:   return 1\n",
    "    if val < -x * std:  return -1\n",
    "    return 0\n",
    "\n",
    "#def rsi_wilder(close: pd.Series, window: int = 14) -> pd.Series:\n",
    "#    close = close.astype(float)\n",
    "#    delta = close.diff()\n",
    "#    gain = delta.clip(lower=0)\n",
    "#    loss = -delta.clip(upper=0)\n",
    "#    # Wilder: alpha = 1/window\n",
    "#    avg_gain = gain.ewm(alpha=1/window, adjust=False).mean()\n",
    "#    avg_loss = loss.ewm(alpha=1/window, adjust=False).mean()\n",
    "#    rs = avg_gain / (avg_loss + 1e-12)\n",
    "#    rsi = 100 - (100 / (1 + rs))\n",
    "#    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f3e49c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#priceprocessing\n",
    "class PriceProcessing:\n",
    "    def __init__(self, df_prices: pd.DataFrame, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.df = df_prices.copy()\n",
    "\n",
    "        # Datas\n",
    "        self.df[\"Data\"] = pd.to_datetime(self.df[\"Data\"], dayfirst=True, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "        # üîß NOVO: remove linhas com Data = NaT (evita erro no merge_asof)\n",
    "        self.df = self.df[~self.df[\"Data\"].isna()].copy()\n",
    "\n",
    "        # N√∫meros\n",
    "        float_cols = [\"FechAjust\",\"FechHist\",\"AbertAjust\",\"MinAjust\",\"MedAjust\",\"MaxAjust\",\"Var\",\"Fator\"]\n",
    "        for c in float_cols:\n",
    "            if c in self.df.columns: self.df[c] = self.df[c].apply(to_float_smart)\n",
    "        for c in [\"Vol\",\"Neg\"]:\n",
    "            if c in self.df.columns: self.df[c] = self.df[c].apply(to_int_smart)\n",
    "\n",
    "        # Ordena/dedup\n",
    "        self.df = self.df.sort_values(\"Data\").drop_duplicates(\"Data\").reset_index(drop=True)\n",
    "\n",
    "        # Close\n",
    "        #self.df[\"Close\"] = self.df[\"FechAjust\"] if (\"FechAjust\" in self.df and self.df[\"FechAjust\"].notna().any()) else self.df.get(\"FechHist\", np.nan)\n",
    "        if \"FechAjust\" in self.df.columns and self.df[\"FechAjust\"].notna().any():\n",
    "            self.df[\"Close\"] = self.df[\"FechAjust\"]\n",
    "        else:\n",
    "            #if \"AbertAjust\" in self.df.columns:\n",
    "            #    self.df[\"Close\"] = self.df[\"AbertAjust\"].shift(-1)\n",
    "            #else:\n",
    "                self.df[\"Close\"] = np.nan   \n",
    "                \n",
    "        # Limpa linhas sem Close\n",
    "        \n",
    "        before = len(self.df)\n",
    "        close_num = self.df[\"Close\"]\n",
    "        mask = close_num.notna() & (close_num != 0)\n",
    "        dfp = self.df.loc[mask].copy()\n",
    "        after = len(dfp)\n",
    "        removed = before - after\n",
    "        pct = 0.0 if before == 0 else (removed / before) * 100\n",
    "        \n",
    "        print(f\"{ticker}: Linhas removidas (Close NaN/0): {removed} de {before} = {pct:.2f}%\")\n",
    "        self.df = dfp\n",
    "\n",
    "        # Fechamento ponderado por volume\n",
    "        if \"Vol\" in self.df.columns and self.df[\"Vol\"].notna().any():\n",
    "            vol_mean = self.df[\"Vol\"].replace(0, np.nan).mean()\n",
    "            if vol_mean and not np.isnan(vol_mean):\n",
    "                self.df[\"FechPonderado\"] = self.df[\"Close\"] * self.df[\"Vol\"] / vol_mean\n",
    "            else:\n",
    "                # fallback caso vol_mean vire NaN\n",
    "                self.df[\"FechPonderado\"] = self.df[\"Close\"]\n",
    "        else:\n",
    "            self.df[\"FechPonderado\"] = np.nan\n",
    "\n",
    "        self.df[\"Close\"] = self.df[\"Close\"].apply(to_float_smart)\n",
    "\n",
    "    def add_momentum_indicators(self):\n",
    "        \"\"\"\n",
    "        Adiciona indicadores de 'momento':\n",
    "        - M√©dias m√≥veis simples (5, 50, 200)\n",
    "        - RSI (9, 30)\n",
    "        - Raz√µes de MAs (5/50, 5/200, 50/200)\n",
    "        - Momentum (1M, 3M, 6M, 12M) em log-return acumulado\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "\n",
    "        # MAs\n",
    "        df[\"MA5\"]   = ta.sma(df[\"Close\"], length=5)\n",
    "        df[\"MA50\"]  = ta.sma(df[\"Close\"], length=50)\n",
    "        df[\"MA200\"] = ta.sma(df[\"Close\"], length=200)\n",
    "\n",
    "        # RSI\n",
    "        df[\"RSI9\"]  = ta.rsi(df[\"Close\"], lenght=9)\n",
    "        df[\"RSI14\"]  = ta.rsi(df[\"Close\"], lenght=14)\n",
    "        df[\"RSI30\"] = ta.rsi(df[\"Close\"], lenght=30)\n",
    "\n",
    "        # Raz√µes\n",
    "        df[\"MA5_50\"]   = df[\"MA5\"]   / (df[\"MA50\"].replace(0, np.nan))\n",
    "        df[\"MA5_200\"]  = df[\"MA5\"]   / (df[\"MA200\"].replace(0, np.nan))\n",
    "        df[\"MA50_200\"] = df[\"MA50\"]  / (df[\"MA200\"].replace(0, np.nan))\n",
    "\n",
    "        # Momentum cl√°ssico (opcional; comente se n√£o quiser)\n",
    "        # ~21, ~63, ~126, ~252 preg√µes\n",
    "        df[\"MOM_1M\"] = ta.mom(df[\"Close\"], length=21)\n",
    "        df[\"MOM_3M\"] = ta.mom(df[\"Close\"], length=63)\n",
    "        df[\"MOM_6M\"] = ta.mom(df[\"Close\"], length=126)\n",
    "        df[\"MOM_12M\"] = ta.mom(df[\"Close\"], length=252) \n",
    "\n",
    "        self.df = df            \n",
    "\n",
    "    def create_return_by_period(self, name_return: str, period: int, column_name: str = \"Close\", remove_nan=False):\n",
    "        self.df[name_return] = np.log(self.df[column_name] / self.df[column_name].shift(period))\n",
    "        if remove_nan:\n",
    "            self.df.dropna(subset=[name_return], inplace=True)\n",
    "        \n",
    "        if name_return == \"Daily_Return\":\n",
    "            #Proximo retorno\n",
    "            self.df[\"ret_t0_t1\"] = self.df[\"Daily_Return\"].shift(-1)\n",
    "            \n",
    "    \n",
    "    def create_return_by_period(self, name_return: str, period: int, pre_event: bool = False, column_name: str = \"Close\", remove_nan=False):\n",
    "        px = self.df[column_name].astype(float)\n",
    "        if pre_event:\n",
    "            num = px.shift(1)\n",
    "            den = px.shift(1 + period)\n",
    "        else:\n",
    "            num = px\n",
    "            den = px.shift(period)\n",
    "    \n",
    "        self.df[name_return] = np.log(num/den)\n",
    "        if remove_nan:\n",
    "            self.df.dropna(subset=[name_return], inplace=True)    \n",
    "\n",
    "   \n",
    "    def create_rolling_std(self, name_return: str, window: int = 22):\n",
    "        \"\"\"\n",
    "        Calcula o desvio padr√£o em uma janela m√≥vel.\n",
    "        Ex.: window=22 ‚Üí volatilidade de 22 per√≠odos.\n",
    "        \"\"\"\n",
    "        self.df[f\"Rolling_std_{name_return}\"] = (\n",
    "            self.df[name_return].rolling(window=window).std()\n",
    "    )    \n",
    "\n",
    "    def create_indicator(self, name_return: str, factor: float = 0.1):\n",
    "        self.df[f\"Indicator_{name_return}\"] = self.df.apply(lambda r: catalog_return(r, factor, name_return), axis=1)\n",
    "    \n",
    "    def set_event(self, fund_dir: str = \"dataset/fundamental\") -> None:\n",
    "        \"\"\"\n",
    "        Cria a coluna self.df[\"event\"] = 1 se houve publica√ß√£o de resultado naquela data,\n",
    "        sen√£o 0.\n",
    "\n",
    "        A data de publica√ß√£o vem de dataset/fundamental/<TICKER>.SA.csv\n",
    "        na coluna 'Data_Publicacao'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Monta o caminho para o arquivo fundamental correspondente\n",
    "        # Ex.: dataset/fundamental/ABEV3.SA.csv\n",
    "        fund_path = os.path.join(fund_dir, f\"{self.ticker}.SA.csv\")\n",
    "\n",
    "        # Se n√£o existir dado fundamental pra esse papel (empresa antiga, delistada etc.),\n",
    "        # n√£o falha: simplesmente marca tudo como 0.\n",
    "        if not os.path.exists(fund_path):\n",
    "            self.df[\"event\"] = 0\n",
    "            return\n",
    "\n",
    "        df_fund = pd.read_csv(fund_path)\n",
    "\n",
    "        # Se n√£o houver coluna de data de publica√ß√£o, tamb√©m fallback = 0\n",
    "        if \"Data_Publicacao\" not in df_fund.columns:\n",
    "            self.df[\"event\"] = 0\n",
    "            return\n",
    "\n",
    "        # Converte datas de publica√ß√£o para datetime normalizado (sem hora)\n",
    "        #df_fund[\"Data_Publicacao\"] = _parse_publication_dates(df_fund[\"Data_Publicacao\"])\n",
    "\n",
    "        #df_fund[\"Data_Publicacao\"] = pd.to_datetime(df_fund[\"Data_Publicacao\"], errors=\"coerce\", dayfirst=True, format=\"%Y-%m-%d\")\n",
    "        #pub_dates = set(\n",
    "        #    df_fund[\"Data_Publicacao\"]\n",
    "        #    .dropna()\n",
    "        #    .dt.normalize()\n",
    "        #    .values\n",
    "        #)\n",
    "#\n",
    "        ## Normaliza as datas de pre√ßo pra s√≥ comparar YYYY-MM-DD\n",
    "        #self.df[\"Data\"] = pd.to_datetime(self.df[\"Data\"], errors=\"coerce\", dayfirst=True, format=\"%Y-%m-%d\")\n",
    "        #data_normalized = self.df[\"Data\"].dt.normalize().values\n",
    "        ##data_normalized = self.df[\"Data\"].values\n",
    "#\n",
    "        ## Marca 1 se houve resultado publicado naquela data, sen√£o 0\n",
    "        #self.df[\"event\"] = np.array(\n",
    "        #    [1 if d in pub_dates else 0 for d in data_normalized],\n",
    "        #    dtype=int\n",
    "        #)\n",
    "        df_fund[\"Data_Publicacao\"] = pd.to_datetime(\n",
    "           df_fund[\"Data_Publicacao\"],\n",
    "            errors=\"coerce\",\n",
    "            dayfirst=True\n",
    "        )\n",
    "        \n",
    "        pub_dates = df_fund[\"Data_Publicacao\"].dropna().dt.normalize()\n",
    "        \n",
    "        self.df[\"Data\"] = pd.to_datetime(\n",
    "            self.df[\"Data\"],\n",
    "            errors=\"coerce\",\n",
    "            dayfirst=True\n",
    "        )\n",
    "        \n",
    "        self.df[\"Data_norm\"] = self.df[\"Data\"].dt.normalize()\n",
    "        self.df[\"event\"] = self.df[\"Data_norm\"].isin(pub_dates).astype(int)\n",
    "\n",
    "\n",
    "    def finalize(self) -> pd.DataFrame:\n",
    "        cols = [\"Data\",\"Close\",\"FechPonderado\",\"Vol\",\"Neg\",\"Var\"]\n",
    "        cols += [c for c in self.df.columns if c.startswith((\"Daily_\",\"ret_t0_t1\", \"Week_\",\"Month_\"))]\n",
    "        cols += [c for c in self.df.columns if c.startswith((\"Rolling_std_\",\"Indicator_\", \"event\"))]        \n",
    "        cols += [c for c in self.df.columns if c.startswith((\"MOM\",\"RSI\", \"MA\"))]\n",
    "        cols = [c for c in cols if c in self.df.columns]\n",
    "        out = self.df[cols].copy()\n",
    "        out.insert(0, \"Ticker\", self.ticker)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "43f5386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataPrepPrices\n",
    "class DataPrepPrices:\n",
    "    def __init__(self,\n",
    "                 prices_dir: str = \"dataset/prices\",\n",
    "                 fund_dir: str = \"dataset/fundamental\",\n",
    "                 out_prices_dir: str = \"dataset/prices_processed\",\n",
    "                 out_final_dir: str = \"dataset/final\"):\n",
    "        self.prices_dir = prices_dir\n",
    "        self.fund_dir = fund_dir\n",
    "        self.out_prices_dir = out_prices_dir\n",
    "        self.out_final_dir = out_final_dir\n",
    "        ensure_dirs([prices_dir, fund_dir, out_prices_dir, out_final_dir])\n",
    "\n",
    "    @staticmethod\n",
    "    def _ticker_from_price_filename(fname: str) -> str:\n",
    "        # \"AZUL4.SA.csv\" -> \"AZUL4\"\n",
    "        base = os.path.basename(fname)\n",
    "        if base.endswith(\".csv\"):\n",
    "            base = base[:-4]\n",
    "        return base.replace(\".SA\", \"\")\n",
    "\n",
    "\n",
    "    def process_one(self, \n",
    "                    price_csv_path: str,\n",
    "                    indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,                \n",
    "                    only_events: bool = False) -> Optional[pd.DataFrame]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Processa um √∫nico CSV de pre√ßo:\n",
    "        - calcula retornos (daily/week/month)\n",
    "        - calcula volatilidade rolling\n",
    "        - cria indicadores categ√≥ricos\n",
    "        - cria flag de evento (publica√ß√£o de resultado)\n",
    "        - salva vers√£o processada\n",
    "        - retorna dataframe final desse ticker\n",
    "        \"\"\"\n",
    "\n",
    "        base = os.path.basename(price_csv_path).upper()\n",
    "        if base in {\"IBOV.SA.CSV\", \"CDI.SA.CSV\"}:\n",
    "            return pd.DataFrame()  # proxies ‚Äî n√£o s√£o pap√©is\n",
    "\n",
    "        # Pre√ßo\n",
    "        dfp = pd.read_csv(price_csv_path, sep=None, engine=\"python\", dtype=str)      \n",
    "    \n",
    "        tkr = self._ticker_from_price_filename(price_csv_path)\n",
    "        price = PriceProcessing(dfp, tkr)\n",
    "\n",
    "        # Retornos e indicadores\n",
    "        price.create_return_by_period(\"Daily_Return\", 1)\n",
    "        price.create_return_by_period(\"Week_Return\", 5)\n",
    "        price.create_return_by_period(\"Month_Return\", 22)\n",
    "        \n",
    "        price.create_rolling_std(\"Daily_Return\", window=21)\n",
    "        price.create_rolling_std(\"Week_Return\", window=65)\n",
    "        price.create_rolling_std(\"Month_Return\", window=252)\n",
    "        \n",
    "        # indicadores discretos    \n",
    "        price.create_indicator(\"Daily_Return\", indicator_factor)\n",
    "        price.create_indicator(\"Week_Return\", indicator_factor)\n",
    "        price.create_indicator(\"Month_Return\", indicator_factor)   \n",
    "        \n",
    "        #Momento\n",
    "        price.add_momentum_indicators()\n",
    "        \n",
    "        #Eventos\n",
    "        price.set_event(fund_dir=self.fund_dir)\n",
    "\n",
    "\n",
    "        df_price_feat = price.finalize()\n",
    "        \n",
    "        if save_intermediate_prices:\n",
    "            df_price_feat.to_csv(os.path.join(self.out_prices_dir, os.path.basename(price_csv_path)), index=False)\n",
    "\n",
    "        # Garante Ticker correto (sem duplicar)\n",
    "        if \"Ticker\" in df_price_feat.columns:\n",
    "            df_price_feat[\"Ticker\"] = tkr\n",
    "        else:\n",
    "            df_price_feat.insert(0, \"Ticker\", tkr)\n",
    "        return df_price_feat\n",
    "    \n",
    "    def process_all(self,\n",
    "                indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,                \n",
    "                    only_events: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Processa todos os arquivos em dataset/prices e devolve consolidado.\"\"\"\n",
    "        all_final = []\n",
    "        for fn in os.listdir(self.prices_dir):\n",
    "            if not fn.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            try:\n",
    "                path = os.path.join(self.prices_dir, fn)\n",
    "                df_final = self.process_one(\n",
    "                    path,\n",
    "                    indicator_factor=indicator_factor,\n",
    "                    save_intermediate_prices=save_intermediate_prices,                \n",
    "                    only_events=only_events\n",
    "                )\n",
    "                if df_final is not None and not df_final.empty:\n",
    "                    all_final.append(df_final.assign(Ticker=self._ticker_from_price_filename(fn)))\n",
    "            except Exception as ex:\n",
    "                print(f\"Erro no ticker de {fn}: {ex}\")\n",
    "                continue\n",
    "        if not all_final:\n",
    "            return pd.DataFrame()\n",
    "        df_all = pd.concat(all_final, ignore_index=True)\n",
    "        # Ordena por Ticker e Data\n",
    "        df_all = df_all.sort_values([\"Ticker\", \"Data\"]).reset_index(drop=True)\n",
    "        # salva consolidado\n",
    "        df_all.to_csv(os.path.join(self.out_final_dir, \"final_price_process.csv\"), index=False)\n",
    "        return df_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31c9f27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABEV3: Linhas removidas (Close NaN/0): 998 de 2513 = 39.71%\n",
      "Step 1 OK ‚Äî final_price_process.csv salvo em dataset/final/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thgcn\\AppData\\Local\\Temp\\ipykernel_32904\\2174663360.py:175: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df_fund[\"Data_Publicacao\"] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "\n",
    "    # 0) Garante que todas as pastas base existem\n",
    "    ensure_dirs([\n",
    "        \"dataset/prices\",\n",
    "        \"dataset/fundamental\",\n",
    "        \"dataset/prices_processed\",\n",
    "        \"dataset/final\"\n",
    "    ])\n",
    "\n",
    "    # 1) Pr√©-processa todos os pap√©is\n",
    "    pipeline = DataPrepPrices(\n",
    "        prices_dir=\"dataset/prices\",\n",
    "        fund_dir=\"dataset/fundamental\",\n",
    "        out_prices_dir=\"dataset/prices_processed\",\n",
    "        out_final_dir=\"dataset/final\"\n",
    "    )\n",
    "\n",
    "    df_all_final = pipeline.process_all(\n",
    "        indicator_factor=0.1,\n",
    "        save_intermediate_prices=True,\n",
    "        only_events=False   # se quiser s√≥ linhas nas datas de evento, troque para True\n",
    "    )\n",
    "\n",
    "    print(\"Step 1 OK ‚Äî final_price_process.csv salvo em dataset/final/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "91163d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000001000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000010000000000000100000000000000000000000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000100000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/prices_processed/ABEV3.SA.csv\", dtype=str)\n",
    "df[\"event\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf959d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fund = pd.read_csv(\"dataset/fundamental/ABEV3.SA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1dacad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "if \"Data_Publicacao\" not in df_fund.columns:\n",
    "        df[\"event\"] = 0\n",
    "        print(\"nok\")\n",
    "else:\n",
    "    print(\"ok\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8d2f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fund[\"Data_Publicacao\"] = pd.to_datetime(df_fund[\"Data_Publicacao\"], errors=\"coerce\", dayfirst=True, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1852e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_dates = set(\n",
    "            df_fund[\"Data_Publicacao\"]\n",
    "            .dropna()\n",
    "            .dt.normalize()\n",
    "            .values\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba2f5ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{numpy.datetime64('2013-10-31T00:00:00.000000000'),\n",
       " numpy.datetime64('2014-03-24T00:00:00.000000000'),\n",
       " numpy.datetime64('2014-05-07T00:00:00.000000000'),\n",
       " numpy.datetime64('2014-07-31T00:00:00.000000000'),\n",
       " numpy.datetime64('2014-10-31T00:00:00.000000000'),\n",
       " numpy.datetime64('2015-02-26T00:00:00.000000000'),\n",
       " numpy.datetime64('2015-05-06T00:00:00.000000000'),\n",
       " numpy.datetime64('2015-07-30T00:00:00.000000000'),\n",
       " numpy.datetime64('2015-10-30T00:00:00.000000000'),\n",
       " numpy.datetime64('2016-02-25T00:00:00.000000000'),\n",
       " numpy.datetime64('2016-05-04T00:00:00.000000000'),\n",
       " numpy.datetime64('2016-07-29T00:00:00.000000000'),\n",
       " numpy.datetime64('2016-10-28T00:00:00.000000000'),\n",
       " numpy.datetime64('2017-05-04T00:00:00.000000000'),\n",
       " numpy.datetime64('2017-07-27T00:00:00.000000000'),\n",
       " numpy.datetime64('2017-10-26T00:00:00.000000000'),\n",
       " numpy.datetime64('2018-05-09T00:00:00.000000000'),\n",
       " numpy.datetime64('2018-07-05T00:00:00.000000000'),\n",
       " numpy.datetime64('2018-07-26T00:00:00.000000000'),\n",
       " numpy.datetime64('2018-10-25T00:00:00.000000000'),\n",
       " numpy.datetime64('2019-02-28T00:00:00.000000000'),\n",
       " numpy.datetime64('2019-05-07T00:00:00.000000000'),\n",
       " numpy.datetime64('2019-07-25T00:00:00.000000000'),\n",
       " numpy.datetime64('2019-10-25T00:00:00.000000000'),\n",
       " numpy.datetime64('2020-02-27T00:00:00.000000000')}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16dd6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Data\"] = pd.to_datetime(df[\"Data\"], errors=\"coerce\", dayfirst=True, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a41629e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = df[\"Data\"].dt.normalize().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e0f39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"event\"] = np.array(\n",
    "    [1 if d in pub_dates else 0 for d in data_normalized],\n",
    "    dtype=int\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d2c37b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatetimeArray>\n",
       "['2020-02-27 00:00:00', '2019-10-25 00:00:00', '2019-07-25 00:00:00',\n",
       " '2019-05-07 00:00:00', '2019-02-28 00:00:00', '2018-10-25 00:00:00',\n",
       " '2018-07-26 00:00:00', '2018-05-09 00:00:00', '2018-07-05 00:00:00',\n",
       " '2017-10-26 00:00:00', '2017-07-27 00:00:00', '2017-05-04 00:00:00',\n",
       " '2016-10-28 00:00:00', '2016-07-29 00:00:00', '2016-05-04 00:00:00',\n",
       " '2016-02-25 00:00:00', '2015-10-30 00:00:00', '2015-07-30 00:00:00',\n",
       " '2015-05-06 00:00:00', '2015-02-26 00:00:00', '2014-10-31 00:00:00',\n",
       " '2014-07-31 00:00:00', '2014-05-07 00:00:00', '2014-03-24 00:00:00',\n",
       " '2013-10-31 00:00:00']\n",
       "Length: 25, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "48b590b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2013-11-11T00:00:00.000000000', '2013-11-12T00:00:00.000000000',\n",
       "       '2013-11-13T00:00:00.000000000', ...,\n",
       "       '2019-12-27T00:00:00.000000000', '2019-12-30T00:00:00.000000000',\n",
       "       '2020-01-02T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9e3937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([df[\"event\"]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "287af634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨á Datas de publica√ß√£o (unique):\n",
      "[Timestamp('2013-10-31 00:00:00'), Timestamp('2014-03-24 00:00:00'), Timestamp('2014-05-07 00:00:00'), Timestamp('2014-07-31 00:00:00'), Timestamp('2014-10-31 00:00:00'), Timestamp('2015-02-26 00:00:00'), Timestamp('2015-05-06 00:00:00'), Timestamp('2015-07-30 00:00:00'), Timestamp('2015-10-30 00:00:00'), Timestamp('2016-02-25 00:00:00'), Timestamp('2016-05-04 00:00:00'), Timestamp('2016-07-29 00:00:00'), Timestamp('2016-10-28 00:00:00'), Timestamp('2017-05-04 00:00:00'), Timestamp('2017-07-27 00:00:00'), Timestamp('2017-10-26 00:00:00'), Timestamp('2018-05-09 00:00:00'), Timestamp('2018-07-05 00:00:00'), Timestamp('2018-07-26 00:00:00'), Timestamp('2018-10-25 00:00:00'), Timestamp('2019-02-28 00:00:00'), Timestamp('2019-05-07 00:00:00'), Timestamp('2019-07-25 00:00:00'), Timestamp('2019-10-25 00:00:00'), Timestamp('2020-02-27 00:00:00')]\n",
      "\n",
      "‚¨á Faixa de datas de pre√ßo:\n",
      "min: 2013-11-11 00:00:00 max: 2020-01-02 00:00:00\n",
      "\n",
      "‚¨á Interse√ß√£o entre publica√ß√£o e pre√ßos:\n",
      "[Timestamp('2014-03-24 00:00:00'), Timestamp('2014-05-07 00:00:00'), Timestamp('2014-07-31 00:00:00'), Timestamp('2014-10-31 00:00:00'), Timestamp('2015-02-26 00:00:00'), Timestamp('2015-05-06 00:00:00'), Timestamp('2015-07-30 00:00:00'), Timestamp('2015-10-30 00:00:00'), Timestamp('2016-02-25 00:00:00'), Timestamp('2016-05-04 00:00:00'), Timestamp('2016-07-29 00:00:00'), Timestamp('2016-10-28 00:00:00'), Timestamp('2017-05-04 00:00:00'), Timestamp('2017-07-27 00:00:00'), Timestamp('2017-10-26 00:00:00'), Timestamp('2018-05-09 00:00:00'), Timestamp('2018-07-05 00:00:00'), Timestamp('2018-07-26 00:00:00'), Timestamp('2018-10-25 00:00:00'), Timestamp('2019-02-28 00:00:00'), Timestamp('2019-05-07 00:00:00'), Timestamp('2019-07-25 00:00:00'), Timestamp('2019-10-25 00:00:00')]\n",
      "Qtd de datas em comum: 23\n",
      "\n",
      "Qtd de dias de preg√£o marcados como event=1: 23\n",
      "           Data  Data_norm\n",
      "87   2014-03-24 2014-03-24\n",
      "116  2014-05-07 2014-05-07\n",
      "174  2014-07-31 2014-07-31\n",
      "240  2014-10-31 2014-10-31\n",
      "317  2015-02-26 2015-02-26\n",
      "363  2015-05-06 2015-05-06\n",
      "422  2015-07-30 2015-07-30\n",
      "486  2015-10-30 2015-10-30\n",
      "561  2016-02-25 2016-02-25\n",
      "608  2016-05-04 2016-05-04\n",
      "669  2016-07-29 2016-07-29\n",
      "732  2016-10-28 2016-10-28\n",
      "857  2017-05-04 2017-05-04\n",
      "916  2017-07-27 2017-07-27\n",
      "979  2017-10-26 2017-10-26\n",
      "1107 2018-05-09 2018-05-09\n",
      "1147 2018-07-05 2018-07-05\n",
      "1161 2018-07-26 2018-07-26\n",
      "1224 2018-10-25 2018-10-25\n",
      "1306 2019-02-28 2019-02-28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Converte e normaliza as datas de publica√ß√£o\n",
    "df_fund[\"Data_Publicacao\"] = pd.to_datetime(\n",
    "    df_fund[\"Data_Publicacao\"],\n",
    "    errors=\"coerce\",\n",
    "    dayfirst=True\n",
    ")\n",
    "pub_dates = (\n",
    "    df_fund[\"Data_Publicacao\"]\n",
    "    .dropna()\n",
    "    .dt.normalize()\n",
    ")\n",
    "\n",
    "# 2) Converte e normaliza as datas de pre√ßo\n",
    "df[\"Data\"] = pd.to_datetime(\n",
    "    df[\"Data\"],\n",
    "    errors=\"coerce\",\n",
    "    dayfirst=True\n",
    ")\n",
    "df[\"Data_norm\"] = df[\"Data\"].dt.normalize()\n",
    "\n",
    "# 3) DEBUG: ver interse√ß√£o pra voc√™ n√£o ficar maluco\n",
    "pub_unique   = pub_dates.unique()\n",
    "price_unique = df[\"Data_norm\"].unique()\n",
    "\n",
    "intersection = set(pub_unique) & set(price_unique)\n",
    "\n",
    "print(\"‚¨á Datas de publica√ß√£o (unique):\")\n",
    "print(sorted(pub_unique))\n",
    "\n",
    "print(\"\\n‚¨á Faixa de datas de pre√ßo:\")\n",
    "print(\"min:\", df[\"Data_norm\"].min(), \"max:\", df[\"Data_norm\"].max())\n",
    "\n",
    "print(\"\\n‚¨á Interse√ß√£o entre publica√ß√£o e pre√ßos:\")\n",
    "print(sorted(intersection))\n",
    "print(\"Qtd de datas em comum:\", len(intersection))\n",
    "\n",
    "# 4) Finalmente, marca os eventos\n",
    "df[\"event\"] = df[\"Data_norm\"].isin(pub_unique).astype(int)\n",
    "\n",
    "print(\"\\nQtd de dias de preg√£o marcados como event=1:\", df[\"event\"].sum())\n",
    "print(df.loc[df[\"event\"] == 1, [\"Data\", \"Data_norm\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4573a04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"event\"].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-ml)",
   "language": "python",
   "name": "fin-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
