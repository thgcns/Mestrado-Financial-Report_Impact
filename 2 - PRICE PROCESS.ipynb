{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "# =====================\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import enum\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import ta\n",
    "from collections import OrderedDict\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f356fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils\n",
    "# -------------------------\n",
    "# Utils de I/O e diret√≥rios\n",
    "# -------------------------\n",
    "\n",
    "DEFAULT_DIRS = [\"dataset\", \"dataset/prices\", \"dataset/prices_processed\",\n",
    "                \"dataset/fundamental\", \"dataset/final\"]\n",
    "\n",
    "def ensure_dirs(paths: List[str] = DEFAULT_DIRS) -> None:\n",
    "    for p in paths:\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Utils de parsing num√©rico\n",
    "# -------------------------\n",
    "\n",
    "def _parse_publication_dates(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converte a coluna de datas de publica√ß√£o para datetime,\n",
    "    assumindo SEMPRE padr√£o brasileiro (DD/MM/AAAA) quando houver ambiguidade.\n",
    "\n",
    "    Regras:\n",
    "    - Se estiver no padr√£o ISO 'YYYY-MM-DD', usamos isso direto (n√£o √© amb√≠guo).\n",
    "    - Se estiver no padr√£o brasileiro 'DD/MM/YYYY', interpretamos como dia/m√™s/ano.\n",
    "    - Se vier em qualquer outro formato, tentamos parse com dayfirst=True.\n",
    "    - No final, retornamos datetime normalizado (sem hora).\n",
    "    \"\"\"\n",
    "\n",
    "    s = series.astype(str).str.strip()\n",
    "\n",
    "    # 1) tenta ISO claro: 2024-03-31\n",
    "    iso_mask = s.str.match(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "    out_iso = pd.to_datetime(\n",
    "        s.where(iso_mask),\n",
    "        format=\"%Y-%m-%d\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 2) tenta BR claro: 31/03/2024\n",
    "    br_mask = s.str.match(r\"^\\d{2}/\\d{2}/\\d{4}$\")\n",
    "    out_br = pd.to_datetime(\n",
    "        s.where(br_mask),\n",
    "        format=\"%d/%m/%Y\",\n",
    "        dayfirst=True,\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # 3) come√ßa com ISO e preenche lacunas com BR\n",
    "    out = out_iso.fillna(out_br)\n",
    "\n",
    "    # 4) fallback gen√©rico:\n",
    "    #    qualquer coisa que sobrou a gente interpreta assumindo padr√£o brasileiro (dayfirst=True)\n",
    "    still_nat = out.isna()\n",
    "    if still_nat.any():\n",
    "        out_fallback = pd.to_datetime(\n",
    "            s[still_nat],\n",
    "            errors=\"coerce\",\n",
    "            dayfirst=True   # <- for√ßa sem√¢ntica brasileira\n",
    "        )\n",
    "        out.loc[still_nat] = out_fallback\n",
    "\n",
    "    # 5) normaliza para \"apenas a data\" (zera hora)\n",
    "    out = out.dt.normalize()\n",
    "\n",
    "    return out\n",
    "\n",
    "def to_float_smart(x):\n",
    "    \"\"\"\n",
    "    Converte strings tipo '1.234,56' (BR) ou '1,234.56' (US) e variantes em float.\n",
    "    Trata negativos e milhares. Retorna NaN se n√£o der.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if s in {\"\", \"-\", \"--\", \"nan\", \"NaN\", \"None\", \"NULL\"}:\n",
    "        return np.nan\n",
    "\n",
    "    # mant√©m apenas d√≠gitos, sinais e separadores\n",
    "    s = re.sub(r\"[^0-9\\-\\.,]\", \"\", s)\n",
    "\n",
    "    has_dot   = \".\" in s\n",
    "    has_comma = \",\" in s\n",
    "\n",
    "    try:\n",
    "        if has_dot and has_comma:\n",
    "            # decide pelo separador mais √† direita\n",
    "            if s.rfind(\",\") > s.rfind(\".\"):\n",
    "                # BR: 1.234,56 -> 1234.56\n",
    "                s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            else:\n",
    "                # US: 1,234.56 -> 1234.56\n",
    "                s = s.replace(\",\", \"\")\n",
    "            return float(s)\n",
    "\n",
    "        if has_comma and not has_dot:\n",
    "            # BR decimal: 1234,56 -> 1234.56\n",
    "            return float(s.replace(\",\", \".\"))\n",
    "\n",
    "        if has_dot and not has_comma:\n",
    "            # Pode ser decimal (um ponto) ou milhares (v√°rios pontos)\n",
    "            if s.count(\".\") == 1:\n",
    "                return float(s)  # 1234.56\n",
    "            else:\n",
    "                # 109.641.290.194 -> 109641290194\n",
    "                return float(s.replace(\".\", \"\"))\n",
    "\n",
    "        # S√≥ d√≠gitos e talvez sinal\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "def to_int_smart(x) -> float:\n",
    "    \"\"\"Converte texto para inteiro removendo n√£o-d√≠gitos.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): return np.nan\n",
    "    s = re.sub(r\"[^0-9\\-]\", \"\", str(x))\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    " \n",
    "def catalog_return(row, x, name_return):\n",
    "    val = row.get(name_return, np.nan)\n",
    "    std = row.get(f\"Rolling_std_{name_return}\", np.nan)\n",
    "    if pd.isna(val) or pd.isna(std) or std == 0: return 0\n",
    "    if val > x * std:   return 1\n",
    "    if val < -x * std:  return -1\n",
    "    return 0\n",
    "\n",
    "def rsi_wilder(close: pd.Series, window: int = 14) -> pd.Series:\n",
    "    close = close.astype(float)\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    # Wilder: alpha = 1/window\n",
    "    avg_gain = gain.ewm(alpha=1/window, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(alpha=1/window, adjust=False).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-12)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e49c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#priceprocessing\n",
    "class PriceProcessing:\n",
    "    def __init__(self, df_prices: pd.DataFrame, ticker: str):\n",
    "        self.ticker = ticker\n",
    "        self.df = df_prices.copy()\n",
    "\n",
    "        # Datas\n",
    "        self.df[\"Data\"] = pd.to_datetime(self.df[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "        # üîß NOVO: remove linhas com Data = NaT (evita erro no merge_asof)\n",
    "        self.df = self.df[~self.df[\"Data\"].isna()].copy()\n",
    "\n",
    "        # N√∫meros\n",
    "        float_cols = [\"FechAjust\",\"FechHist\",\"AbertAjust\",\"MinAjust\",\"MedAjust\",\"MaxAjust\",\"Var\",\"Fator\"]\n",
    "        for c in float_cols:\n",
    "            if c in self.df.columns: self.df[c] = self.df[c].apply(to_float_smart)\n",
    "        for c in [\"Vol\",\"Neg\"]:\n",
    "            if c in self.df.columns: self.df[c] = self.df[c].apply(to_int_smart)\n",
    "\n",
    "        # Ordena/dedup\n",
    "        self.df = self.df.sort_values(\"Data\").drop_duplicates(\"Data\").reset_index(drop=True)\n",
    "\n",
    "        # Close\n",
    "        #self.df[\"Close\"] = self.df[\"FechAjust\"] if (\"FechAjust\" in self.df and self.df[\"FechAjust\"].notna().any()) else self.df.get(\"FechHist\", np.nan)\n",
    "        if \"FechAjust\" in self.df.columns and self.df[\"FechAjust\"].notna().any():\n",
    "            self.df[\"Close\"] = self.df[\"FechAjust\"]\n",
    "        else:\n",
    "            if \"AbertAjust\" in self.df.columns:\n",
    "                self.df[\"Close\"] = self.df[\"AbertAjust\"].shift(-1)\n",
    "            else:\n",
    "                self.df[\"Close\"] = np.nan   \n",
    "                \n",
    "        # Limpa linhas sem Close\n",
    "        \n",
    "        before = len(self.df)\n",
    "        close_num = self.df[\"Close\"]\n",
    "        mask = close_num.notna() & (close_num != 0)\n",
    "        dfp = self.df.loc[mask].copy()\n",
    "        after = len(dfp)\n",
    "        removed = before - after\n",
    "        pct = 0.0 if before == 0 else (removed / before) * 100\n",
    "        \n",
    "        print(f\"{ticker}: Linhas removidas (Close NaN/0): {removed} de {before} = {pct:.2f}%\")\n",
    "        self.df = dfp\n",
    "\n",
    "        # Fechamento ponderado por volume\n",
    "        if \"Vol\" in self.df.columns and self.df[\"Vol\"].notna().any():\n",
    "            vol_mean = self.df[\"Vol\"].replace(0, np.nan).mean()\n",
    "            if vol_mean and not np.isnan(vol_mean):\n",
    "                self.df[\"FechPonderado\"] = self.df[\"Close\"] * self.df[\"Vol\"] / vol_mean\n",
    "            else:\n",
    "                # fallback caso vol_mean vire NaN\n",
    "                self.df[\"FechPonderado\"] = self.df[\"Close\"]\n",
    "        else:\n",
    "            self.df[\"FechPonderado\"] = np.nan\n",
    "\n",
    "        self.df[\"Close\"] = self.df[\"Close\"].apply(to_float_smart)\n",
    "\n",
    "    def add_momentum_indicators(self):\n",
    "        \"\"\"\n",
    "        Adiciona indicadores de 'momento':\n",
    "        - M√©dias m√≥veis simples (5, 50, 200)\n",
    "        - RSI (9, 30)\n",
    "        - Raz√µes de MAs (5/50, 5/200, 50/200)\n",
    "        - Momentum (1M, 3M, 6M, 12M) em log-return acumulado\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "\n",
    "        # MAs\n",
    "        df[\"MA5\"]   = df[\"Close\"].rolling(5,   min_periods=5).mean()\n",
    "        df[\"MA50\"]  = df[\"Close\"].rolling(50,  min_periods=50).mean()\n",
    "        df[\"MA200\"] = df[\"Close\"].rolling(200, min_periods=200).mean()\n",
    "\n",
    "        # RSI\n",
    "        df[\"RSI9\"]  = rsi_wilder(df[\"Close\"], window=9)\n",
    "        df[\"RSI14\"]  = rsi_wilder(df[\"Close\"], window=14)\n",
    "        df[\"RSI30\"] = rsi_wilder(df[\"Close\"], window=30)\n",
    "\n",
    "        # Raz√µes\n",
    "        df[\"MA5_50\"]   = df[\"MA5\"]   / (df[\"MA50\"].replace(0, np.nan))\n",
    "        df[\"MA5_200\"]  = df[\"MA5\"]   / (df[\"MA200\"].replace(0, np.nan))\n",
    "        df[\"MA50_200\"] = df[\"MA50\"]  / (df[\"MA200\"].replace(0, np.nan))\n",
    "\n",
    "        # Momentum cl√°ssico (opcional; comente se n√£o quiser)\n",
    "        # ~21, ~63, ~126, ~252 preg√µes\n",
    "        df[\"MOM_1M\"]  = np.log(df[\"Close\"] / df[\"Close\"].shift(21))\n",
    "        df[\"MOM_3M\"]  = np.log(df[\"Close\"] / df[\"Close\"].shift(63))\n",
    "        df[\"MOM_6M\"]  = np.log(df[\"Close\"] / df[\"Close\"].shift(126))\n",
    "        df[\"MOM_12M\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(252))\n",
    "\n",
    "        self.df = df            \n",
    "\n",
    "    def create_return_by_period(self, name_return: str, period: int, column_name: str = \"Close\", remove_nan=False):\n",
    "        self.df[name_return] = np.log(self.df[column_name] / self.df[column_name].shift(period))\n",
    "        if remove_nan:\n",
    "            self.df.dropna(subset=[name_return], inplace=True)\n",
    "        \n",
    "        if name_return == \"Daily_Return\":\n",
    "            #Proximo retorno\n",
    "            self.df[\"ret_t0_t1\"] = self.df[\"Daily_Return\"].shift(-1)\n",
    "            \n",
    "    \n",
    "#    def create_return_by_period(self, name_return: str, period: int, pre_event: bool = False, column_name: str = \"Close\", remove_nan=False):\n",
    "#        px = self.df[column_name].astype(float)\n",
    "#        if pre_event:\n",
    "#            num = px.shift(1)\n",
    "#            den = px.shift(1 + period)\n",
    "#        else:\n",
    "#            num = px\n",
    "#            den = px.shift(period)\n",
    "#    \n",
    "#        self.df[name_return] = np.log(num/den)\n",
    "#        if remove_nan:\n",
    "#            self.df.dropna(subset=[name_return], inplace=True)    \n",
    "#\n",
    "   \n",
    "    def create_rolling_std(self, name_return: str, window: int = 22):\n",
    "        \"\"\"\n",
    "        Calcula o desvio padr√£o em uma janela m√≥vel.\n",
    "        Ex.: window=22 ‚Üí volatilidade de 22 per√≠odos.\n",
    "        \"\"\"\n",
    "        self.df[f\"Rolling_std_{name_return}\"] = (\n",
    "            self.df[name_return].rolling(window=window).std()\n",
    "    )    \n",
    "\n",
    "    def create_indicator(self, name_return: str, factor: float = 0.1):\n",
    "        self.df[f\"Indicator_{name_return}\"] = self.df.apply(lambda r: catalog_return(r, factor, name_return), axis=1)\n",
    "    \n",
    "    def set_event(self, fund_dir: str = \"dataset/fundamental\") -> None:\n",
    "        \"\"\"\n",
    "        Cria a coluna self.df[\"event\"] = 1 se houve publica√ß√£o de resultado naquela data,\n",
    "        sen√£o 0.\n",
    "\n",
    "        A data de publica√ß√£o vem de dataset/fundamental/<TICKER>.SA.csv\n",
    "        na coluna 'Data_Publicacao'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Monta o caminho para o arquivo fundamental correspondente\n",
    "        # Ex.: dataset/fundamental/ABEV3.SA.csv\n",
    "        fund_path = os.path.join(fund_dir, f\"{self.ticker}.SA.csv\")\n",
    "\n",
    "        # Se n√£o existir dado fundamental pra esse papel (empresa antiga, delistada etc.),\n",
    "        # n√£o falha: simplesmente marca tudo como 0.\n",
    "        if not os.path.exists(fund_path):\n",
    "            self.df[\"event\"] = 0\n",
    "            return\n",
    "\n",
    "        df_fund = pd.read_csv(fund_path)\n",
    "\n",
    "        # Se n√£o houver coluna de data de publica√ß√£o, tamb√©m fallback = 0\n",
    "        if \"Data_Publicacao\" not in df_fund.columns:\n",
    "            self.df[\"event\"] = 0\n",
    "            return\n",
    "\n",
    "        # Converte datas de publica√ß√£o para datetime normalizado (sem hora)\n",
    "        df_fund[\"Data_Publicacao\"] = _parse_publication_dates(df_fund[\"Data_Publicacao\"])\n",
    "\n",
    "        pub_dates = set(\n",
    "            df_fund[\"Data_Publicacao\"]\n",
    "            .dropna()\n",
    "            .dt.normalize()\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # Normaliza as datas de pre√ßo pra s√≥ comparar YYYY-MM-DD\n",
    "        self.df[\"Data\"] = pd.to_datetime(self.df[\"Data\"], errors=\"coerce\", dayfirst=True)\n",
    "        data_normalized = self.df[\"Data\"].dt.normalize().values\n",
    "\n",
    "        # Marca 1 se houve resultado publicado naquela data, sen√£o 0\n",
    "        self.df[\"event\"] = np.array(\n",
    "            [1 if d in pub_dates else 0 for d in data_normalized],\n",
    "            dtype=int\n",
    "        )\n",
    "   \n",
    "\n",
    "    def finalize(self) -> pd.DataFrame:\n",
    "        cols = [\"Data\",\"Close\",\"FechPonderado\",\"Vol\",\"Neg\",\"Var\"]\n",
    "        cols += [c for c in self.df.columns if c.startswith((\"Daily_\",\"ret_t0_t1\", \"Week_\",\"Month_\"))]\n",
    "        cols += [c for c in self.df.columns if c.startswith((\"Rolling_std_\",\"Indicator_\", \"event\"))]\n",
    "        cols += [c for c in self.df.columns if c.startswith((\"MOM\",\"RSI\", \"MA\"))]\n",
    "        cols = [c for c in cols if c in self.df.columns]\n",
    "        out = self.df[cols].copy()\n",
    "        out.insert(0, \"Ticker\", self.ticker)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataPrepPrices\n",
    "class DataPrepPrices:\n",
    "    def __init__(self,\n",
    "                 prices_dir: str = \"dataset/prices\",\n",
    "                 fund_dir: str = \"dataset/fundamental\",\n",
    "                 out_prices_dir: str = \"dataset/prices_processed\",\n",
    "                 out_final_dir: str = \"dataset/final\"):\n",
    "        self.prices_dir = prices_dir\n",
    "        self.fund_dir = fund_dir\n",
    "        self.out_prices_dir = out_prices_dir\n",
    "        self.out_final_dir = out_final_dir\n",
    "        ensure_dirs([prices_dir, fund_dir, out_prices_dir, out_final_dir])\n",
    "\n",
    "    @staticmethod\n",
    "    def _ticker_from_price_filename(fname: str) -> str:\n",
    "        # \"AZUL4.SA.csv\" -> \"AZUL4\"\n",
    "        base = os.path.basename(fname)\n",
    "        if base.endswith(\".csv\"):\n",
    "            base = base[:-4]\n",
    "        return base.replace(\".SA\", \"\")\n",
    "\n",
    "\n",
    "    def process_one(self, \n",
    "                    price_csv_path: str,\n",
    "                    indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,                \n",
    "                    only_events: bool = False) -> Optional[pd.DataFrame]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Processa um √∫nico CSV de pre√ßo:\n",
    "        - calcula retornos (daily/week/month)\n",
    "        - calcula volatilidade rolling\n",
    "        - cria indicadores categ√≥ricos\n",
    "        - cria flag de evento (publica√ß√£o de resultado)\n",
    "        - salva vers√£o processada\n",
    "        - retorna dataframe final desse ticker\n",
    "        \"\"\"\n",
    "\n",
    "        base = os.path.basename(price_csv_path).upper()\n",
    "        if base in {\"IBOV.SA.CSV\", \"CDI.SA.CSV\"}:\n",
    "            return pd.DataFrame()  # proxies ‚Äî n√£o s√£o pap√©is\n",
    "\n",
    "        # Pre√ßo\n",
    "        dfp = pd.read_csv(price_csv_path, sep=None, engine=\"python\", dtype=str)      \n",
    "    \n",
    "        tkr = self._ticker_from_price_filename(price_csv_path)\n",
    "        price = PriceProcessing(dfp, tkr)\n",
    "\n",
    "        # Retornos e indicadores\n",
    "        price.create_return_by_period(\"Daily_Return\", 1)\n",
    "        price.create_return_by_period(\"Week_Return\", 5)\n",
    "        price.create_return_by_period(\"Month_Return\", 22)\n",
    "        \n",
    "        price.create_rolling_std(\"Daily_Return\", window=21)\n",
    "        price.create_rolling_std(\"Week_Return\", window=65)\n",
    "        price.create_rolling_std(\"Month_Return\", window=252)\n",
    "        \n",
    "        # indicadores discretos    \n",
    "        price.create_indicator(\"Daily_Return\", indicator_factor)\n",
    "        price.create_indicator(\"Week_Return\", indicator_factor)\n",
    "        price.create_indicator(\"Month_Return\", indicator_factor)   \n",
    "        \n",
    "        #Momento\n",
    "        price.add_momentum_indicators()\n",
    "        \n",
    "        #Eventos\n",
    "        price.set_event(fund_dir=self.fund_dir)\n",
    "\n",
    "\n",
    "        df_price_feat = price.finalize()\n",
    "        \n",
    "        if save_intermediate_prices:\n",
    "            df_price_feat.to_csv(os.path.join(self.out_prices_dir, os.path.basename(price_csv_path)), index=False)\n",
    "\n",
    "        # Garante Ticker correto (sem duplicar)\n",
    "        if \"Ticker\" in df_price_feat.columns:\n",
    "            df_price_feat[\"Ticker\"] = tkr\n",
    "        else:\n",
    "            df_price_feat.insert(0, \"Ticker\", tkr)\n",
    "        return df_price_feat\n",
    "    \n",
    "    def process_all(self,\n",
    "                indicator_factor: float = 0.1,\n",
    "                    save_intermediate_prices: bool = True,                \n",
    "                    only_events: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Processa todos os arquivos em dataset/prices e devolve consolidado.\"\"\"\n",
    "        all_final = []\n",
    "        for fn in os.listdir(self.prices_dir):\n",
    "            if not fn.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            try:\n",
    "                path = os.path.join(self.prices_dir, fn)\n",
    "                df_final = self.process_one(\n",
    "                    path,\n",
    "                    indicator_factor=indicator_factor,\n",
    "                    save_intermediate_prices=save_intermediate_prices,                \n",
    "                    only_events=only_events\n",
    "                )\n",
    "                if df_final is not None and not df_final.empty:\n",
    "                    all_final.append(df_final.assign(Ticker=self._ticker_from_price_filename(fn)))\n",
    "            except Exception as ex:\n",
    "                print(f\"Erro no ticker de {fn}: {ex}\")\n",
    "                continue\n",
    "        if not all_final:\n",
    "            return pd.DataFrame()\n",
    "        df_all = pd.concat(all_final, ignore_index=True)\n",
    "        # Ordena por Ticker e Data\n",
    "        df_all = df_all.sort_values([\"Ticker\", \"Data\"]).reset_index(drop=True)\n",
    "        # salva consolidado\n",
    "        df_all.to_csv(os.path.join(self.out_final_dir, \"final_price_process.csv\"), index=False)\n",
    "        return df_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c9f27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABEV3: Linhas removidas (Close NaN/0): 998 de 2513 = 39.71%\n",
      "AEDU3: Linhas removidas (Close NaN/0): 1628 de 2513 = 64.78%\n",
      "ALLL11: Linhas removidas (Close NaN/0): 2313 de 2513 = 92.04%\n",
      "ALLL3: Linhas removidas (Close NaN/0): 1291 de 2513 = 51.37%\n",
      "ALPA4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ALSC3: Linhas removidas (Close NaN/0): 160 de 2513 = 6.37%\n",
      "ALUP11: Linhas removidas (Close NaN/0): 858 de 2513 = 34.14%\n",
      "AMBV3: Linhas removidas (Close NaN/0): 1555 de 2513 = 61.88%\n",
      "AMBV4: Linhas removidas (Close NaN/0): 1555 de 2513 = 61.88%\n",
      "AMIL3: Linhas removidas (Close NaN/0): 1685 de 2513 = 67.05%\n",
      "ANIM3: Linhas removidas (Close NaN/0): 988 de 2513 = 39.32%\n",
      "ARTR3: Linhas removidas (Close NaN/0): 916 de 2513 = 36.45%\n",
      "ARZZ3: Linhas removidas (Close NaN/0): 308 de 2513 = 12.26%\n",
      "AZUL4: Linhas removidas (Close NaN/0): 1841 de 2513 = 73.26%\n",
      "B3SA3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BBAS3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BBDC3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BBDC4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BBRK3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BBSE3: Linhas removidas (Close NaN/0): 861 de 2513 = 34.26%\n",
      "BEEF3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BIDI11: Linhas removidas (Close NaN/0): 2391 de 2513 = 95.15%\n",
      "BISA3: Linhas removidas (Close NaN/0): 1276 de 2513 = 50.78%\n",
      "BPAC11: Linhas removidas (Close NaN/0): 1827 de 2513 = 72.70%\n",
      "BPNM4: Linhas removidas (Close NaN/0): 1379 de 2513 = 54.87%\n",
      "BRAP4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BRDT3: Linhas removidas (Close NaN/0): 2010 de 2513 = 79.98%\n",
      "BRFS3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BRKM5: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BRML3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BRPR3: Linhas removidas (Close NaN/0): 82 de 2513 = 3.26%\n",
      "BRSR6: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BRTO4: Linhas removidas (Close NaN/0): 1951 de 2513 = 77.64%\n",
      "BTOW3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "BVMF3: Linhas removidas (Close NaN/0): 478 de 2513 = 19.02%\n",
      "CCRO3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CCXC3: Linhas removidas (Close NaN/0): 664 de 2513 = 26.42%\n",
      "CESP6: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CIEL3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CMIG3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CMIG4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CNFB4: Linhas removidas (Close NaN/0): 1927 de 2513 = 76.68%\n",
      "CPFE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CPLE6: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CRFB3: Linhas removidas (Close NaN/0): 1909 de 2513 = 75.96%\n",
      "CRUZ3: Linhas removidas (Close NaN/0): 1049 de 2513 = 41.74%\n",
      "CSAN3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CSMG3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CSNA3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "CTIP3: Linhas removidas (Close NaN/0): 720 de 2513 = 28.65%\n",
      "CVCB3: Linhas removidas (Close NaN/0): 1016 de 2513 = 40.43%\n",
      "CYRE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "DASA3: Linhas removidas (Close NaN/0): 577 de 2513 = 22.96%\n",
      "DTEX3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ECOD3: Linhas removidas (Close NaN/0): 2071 de 2513 = 82.41%\n",
      "ECOR3: Linhas removidas (Close NaN/0): 100 de 2513 = 3.98%\n",
      "EGIE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ELET3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ELET6: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ELPL4: Linhas removidas (Close NaN/0): 557 de 2513 = 22.16%\n",
      "ELPL6: Linhas removidas (Close NaN/0): 2273 de 2513 = 90.45%\n",
      "EMBR3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ENAT3: Linhas removidas (Close NaN/0): 313 de 2513 = 12.46%\n",
      "ENBR3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ENEV3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ENGI11: Linhas removidas (Close NaN/0): 750 de 2513 = 29.84%\n",
      "EQTL3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ESTC3: Linhas removidas (Close NaN/0): 154 de 2513 = 6.13%\n",
      "EVEN3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "EZTC3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "FFTL4: Linhas removidas (Close NaN/0): 2021 de 2513 = 80.42%\n",
      "FIBR3: Linhas removidas (Close NaN/0): 287 de 2513 = 11.42%\n",
      "FLRY3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "GETI4: Linhas removidas (Close NaN/0): 1029 de 2513 = 40.95%\n",
      "GFSA3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "GGBR3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "GGBR4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "GNDI3: Linhas removidas (Close NaN/0): 2094 de 2513 = 83.33%\n",
      "GOAU4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "GOLL4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "GRND3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "HAPV3: Linhas removidas (Close NaN/0): 2096 de 2513 = 83.41%\n",
      "HGTX3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "HRTP3: Linhas removidas (Close NaN/0): 1357 de 2513 = 54.00%\n",
      "HYPE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "IGTA3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "INPR3: Linhas removidas (Close NaN/0): 2183 de 2513 = 86.87%\n",
      "IRBR3: Linhas removidas (Close NaN/0): 1916 de 2513 = 76.24%\n",
      "ITSA4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ITUB3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "ITUB4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "JBSS3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "JHSF3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "KEPL3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "KLBN11: Linhas removidas (Close NaN/0): 1031 de 2513 = 41.03%\n",
      "KLBN4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "KROT3: Linhas removidas (Close NaN/0): 798 de 2513 = 31.75%\n",
      "LAME3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "LAME4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "LCAM3: Linhas removidas (Close NaN/0): 618 de 2513 = 24.59%\n",
      "LEVE3: Linhas removidas (Close NaN/0): 317 de 2513 = 12.61%\n",
      "LIGT3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "LINX3: Linhas removidas (Close NaN/0): 808 de 2513 = 32.15%\n",
      "LLXL3: Linhas removidas (Close NaN/0): 1459 de 2513 = 58.06%\n",
      "LREN3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "LUPA3: Linhas removidas (Close NaN/0): 55 de 2513 = 2.19%\n",
      "MAGG3: Linhas removidas (Close NaN/0): 274 de 2513 = 10.90%\n",
      "MDIA3: Linhas removidas (Close NaN/0): 41 de 2513 = 1.63%\n",
      "MGLU3: Linhas removidas (Close NaN/0): 367 de 2513 = 14.60%\n",
      "MILS3: Linhas removidas (Close NaN/0): 110 de 2513 = 4.38%\n",
      "MMXM3: Linhas removidas (Close NaN/0): 51 de 2513 = 2.03%\n",
      "MPLU3: Linhas removidas (Close NaN/0): 238 de 2513 = 9.47%\n",
      "MPXE3: Linhas removidas (Close NaN/0): 1569 de 2513 = 62.44%\n",
      "MRFG3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "MRVE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "MULT3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "MYPK3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "NATU3: Linhas removidas (Close NaN/0): 48 de 2513 = 1.91%\n",
      "NETC4: Linhas removidas (Close NaN/0): 1350 de 2513 = 53.72%\n",
      "ODPV3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "OGXP3: Linhas removidas (Close NaN/0): 282 de 2513 = 11.22%\n",
      "OIBR3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "OIBR4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "OSXB3: Linhas removidas (Close NaN/0): 272 de 2513 = 10.82%\n",
      "PCAR4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "PCAR5: Linhas removidas (Close NaN/0): 2205 de 2513 = 87.74%\n",
      "PDGR3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "PETR3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "PETR4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "PLAS3: Linhas removidas (Close NaN/0): 318 de 2513 = 12.65%\n",
      "PMAM3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "POMO4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "POSI3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "PRML3: Linhas removidas (Close NaN/0): 507 de 2513 = 20.18%\n",
      "PSSA3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "QGEP3: Linhas removidas (Close NaN/0): 486 de 2513 = 19.34%\n",
      "QUAL3: Linhas removidas (Close NaN/0): 408 de 2513 = 16.24%\n",
      "RADL3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "RAIL3: Linhas removidas (Close NaN/0): 598 de 2513 = 23.80%\n",
      "RAPT4: Linhas removidas (Close NaN/0): 41 de 2513 = 1.63%\n",
      "RDCD3: Linhas removidas (Close NaN/0): 1818 de 2513 = 72.34%\n",
      "RENT3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "RLOG3: Linhas removidas (Close NaN/0): 1219 de 2513 = 48.51%\n",
      "RPMG3: Linhas removidas (Close NaN/0): 518 de 2513 = 20.61%\n",
      "RSID3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "RUMO3: Linhas removidas (Close NaN/0): 2032 de 2513 = 80.86%\n",
      "SANB11: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "SAPR11: Linhas removidas (Close NaN/0): 1993 de 2513 = 79.31%\n",
      "SAPR4: Linhas removidas (Close NaN/0): 75 de 2513 = 2.98%\n",
      "SBSP3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "SEER3: Linhas removidas (Close NaN/0): 989 de 2513 = 39.36%\n",
      "SLCE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "SMLE3: Linhas removidas (Close NaN/0): 1400 de 2513 = 55.71%\n",
      "SMLS3: Linhas removidas (Close NaN/0): 1974 de 2513 = 78.55%\n",
      "SMTO3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "SULA11: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "SUZB3: Linhas removidas (Close NaN/0): 1987 de 2513 = 79.07%\n",
      "SUZB5: Linhas removidas (Close NaN/0): 566 de 2513 = 22.52%\n",
      "TAEE11: Linhas removidas (Close NaN/0): 257 de 2513 = 10.23%\n",
      "TAMM4: Linhas removidas (Close NaN/0): 1881 de 2513 = 74.85%\n",
      "TBLE3: Linhas removidas (Close NaN/0): 892 de 2513 = 35.50%\n",
      "TCSA3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "TCSL3: Linhas removidas (Close NaN/0): 2120 de 2513 = 84.36%\n",
      "TCSL4: Linhas removidas (Close NaN/0): 2120 de 2513 = 84.36%\n",
      "TEND3: Linhas removidas (Close NaN/0): 1830 de 2513 = 72.82%\n",
      "TERI3: Linhas removidas (Close NaN/0): 1008 de 2513 = 40.11%\n",
      "TIET11: Linhas removidas (Close NaN/0): 1269 de 2513 = 50.50%\n",
      "TIMP3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "TLPP4: Linhas removidas (Close NaN/0): 2075 de 2513 = 82.57%\n",
      "TMAR5: Linhas removidas (Close NaN/0): 1951 de 2513 = 77.64%\n",
      "TNLP3: Linhas removidas (Close NaN/0): 1951 de 2513 = 77.64%\n",
      "TNLP4: Linhas removidas (Close NaN/0): 1951 de 2513 = 77.64%\n",
      "TOTS3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "TRPL4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "TUPY3: Linhas removidas (Close NaN/0): 132 de 2513 = 5.25%\n",
      "UGPA3: Linhas removidas (Close NaN/0): 440 de 2513 = 17.51%\n",
      "UGPA4: Linhas removidas (Close NaN/0): 2110 de 2513 = 83.96%\n",
      "USIM3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "USIM5: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "VAGR3: Linhas removidas (Close NaN/0): 818 de 2513 = 32.55%\n",
      "VALE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "VALE5: Linhas removidas (Close NaN/0): 557 de 2513 = 22.16%\n",
      "VIVO4: Linhas removidas (Close NaN/0): 2159 de 2513 = 85.91%\n",
      "VIVT4: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "VLID3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "VVAR11: Linhas removidas (Close NaN/0): 1293 de 2513 = 51.45%\n",
      "VVAR3: Linhas removidas (Close NaN/0): 934 de 2513 = 37.17%\n",
      "WEGE3: Linhas removidas (Close NaN/0): 40 de 2513 = 1.59%\n",
      "WIZS3: Linhas removidas (Close NaN/0): 1382 de 2513 = 54.99%\n",
      "YDUQ3: Linhas removidas (Close NaN/0): 42 de 2513 = 1.67%\n",
      "Step 1 OK ‚Äî final_price_process.csv salvo em dataset/final/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "\n",
    "    # 0) Garante que todas as pastas base existem\n",
    "    ensure_dirs([\n",
    "        \"dataset/prices\",\n",
    "        \"dataset/fundamental\",\n",
    "        \"dataset/prices_processed\",\n",
    "        \"dataset/final\"\n",
    "    ])\n",
    "\n",
    "    # 1) Pr√©-processa todos os pap√©is\n",
    "    pipeline = DataPrepPrices(\n",
    "        prices_dir=\"dataset/prices\",\n",
    "        fund_dir=\"dataset/fundamental\",\n",
    "        out_prices_dir=\"dataset/prices_processed\",\n",
    "        out_final_dir=\"dataset/final\"\n",
    "    )\n",
    "\n",
    "    df_all_final = pipeline.process_all(\n",
    "        indicator_factor=0.1,\n",
    "        save_intermediate_prices=True,\n",
    "        only_events=False   # se quiser s√≥ linhas nas datas de evento, troque para True\n",
    "    )\n",
    "\n",
    "    print(\"Step 1 OK ‚Äî final_price_process.csv salvo em dataset/final/\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-ml)",
   "language": "python",
   "name": "fin-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
